{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "T5_DROP_training.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2a7ea77e513a4ee98f4f935473562d57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a9e7d88792e94333b4657e58dc7cb8f5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5d7282b5fcd44ce992b82c30a4f96654",
              "IPY_MODEL_840b2a8c7a064cce9e31de2e8921dda7"
            ]
          }
        },
        "a9e7d88792e94333b4657e58dc7cb8f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5d7282b5fcd44ce992b82c30a4f96654": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4a8518c321104a3d897882425d59f31d",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_43280ddcb8cf4f529295753fa13a9fbc"
          }
        },
        "840b2a8c7a064cce9e31de2e8921dda7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1f90e298d2f943888a2475d65b4148db",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1000/1000 [00:11&lt;00:00, 84.90ex/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0f43317567cc4ddabbbf217c6d9b8eb9"
          }
        },
        "4a8518c321104a3d897882425d59f31d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "43280ddcb8cf4f529295753fa13a9fbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1f90e298d2f943888a2475d65b4148db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0f43317567cc4ddabbbf217c6d9b8eb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "143645d284644e7486394aae9cfb028d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2d6860a9b9d344fb8f75ca6529cfd78f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c2e4e22bf5004f43812b76f88fa39f08",
              "IPY_MODEL_07d383bb04444df1b32ebc705ff31820"
            ]
          }
        },
        "2d6860a9b9d344fb8f75ca6529cfd78f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c2e4e22bf5004f43812b76f88fa39f08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c8e615c4414e4a1ba421bddc5a6d435e",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b6daff759ffc461782f41e6e73ae8ae6"
          }
        },
        "07d383bb04444df1b32ebc705ff31820": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2cdd4e3d16104157b28a16bdc3b854ca",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1000/1000 [00:06&lt;00:00, 156.05ex/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_01b172ec286e4e88afda35a3d2dd5b0d"
          }
        },
        "c8e615c4414e4a1ba421bddc5a6d435e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b6daff759ffc461782f41e6e73ae8ae6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2cdd4e3d16104157b28a16bdc3b854ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "01b172ec286e4e88afda35a3d2dd5b0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omarkapur-mids/w266-project/blob/phillip/DROP/T5_DROP_Custom_Learning_Rate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPTDq9hsSame"
      },
      "source": [
        "save_model = False\n",
        "load_model = False\n",
        "train_model = True\n",
        "use_learning_schedule = False\n",
        "save_preprocessing = True #add\n",
        "load_preprocessing = False #add\n",
        "save_results = False\n",
        "predict_train = False\n",
        "predict_dev = True\n",
        "\n",
        "run_toy = True\n",
        "toy_size = 100\n",
        "epochs = 3\n",
        "batch_size = 8\n",
        "dataset='squad' #acceptable values: drop, squad\n",
        "VERSION=\"drop_only-v1\"\n",
        "t5_model = 't5-small'\n",
        "\n",
        "warmup_steps = 10 #1e4\n",
        "encoder_max_len = 250\n",
        "decoder_max_len = 54\n",
        "buffer_size = 1000"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47S35TVwgIeG",
        "tags": []
      },
      "source": [
        "# Using T5 on DROP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUEAtP-egIeI"
      },
      "source": [
        "#### Package installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5W9FWSkzSk9r"
      },
      "source": [
        "!pip install --quiet transformers\n",
        "!pip install --quiet sentencepiece\n",
        "!pip install --quiet wget\n",
        "!pip install --quiet datasets\n",
        "#!pip install --quiet ipywidgets\n",
        "#!pip install --quiet tensorflow"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2EQCkymgIeK"
      },
      "source": [
        "#### Download drop_eval module and set directories\n",
        "\n",
        "https://github.com/allenai/allennlp-reading-comprehension/blob/master/allennlp_rc/eval/drop_eval.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dtV_UO9Sami",
        "outputId": "e753808d-e954-4425-bcae-1bdd0cf70c59"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/allenai/allennlp-reading-comprehension/master/allennlp_rc/eval/drop_eval.py -O drop_eval.py\n",
        "import os\n",
        "def create_dir(d,verbose=False):\n",
        "    if not os.path.exists(d):\n",
        "        !mkdir -p $d    \n",
        "        if verbose: print(f'created folder for {d}')\n",
        "    else:\n",
        "        if verbose: print(f'using existing folder for {d}\\nCAUTION -- this run may overwrite existing data!')\n",
        "    \n",
        "#set directories\n",
        "root_dir = './data'\n",
        "data_dir = f\"./data/{VERSION}/{t5_model}\"\n",
        "processed_dir = f\"{root_dir}/processed-data\"\n",
        "results_dir = f\"{data_dir}/results/\"\n",
        "log_dir = f\"{data_dir}/experiments/logs\"\n",
        "save_path = f\"{data_dir}/experiments/models\"\n",
        "\n",
        "\n",
        "create_dir(root_dir)\n",
        "create_dir(data_dir)\n",
        "create_dir(processed_dir)\n",
        "create_dir(results_dir)\n",
        "create_dir(log_dir)\n",
        "create_dir(save_path)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-18 17:28:10--  https://raw.githubusercontent.com/allenai/allennlp-reading-comprehension/master/allennlp_rc/eval/drop_eval.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11222 (11K) [text/plain]\n",
            "Saving to: ‘drop_eval.py’\n",
            "\n",
            "\rdrop_eval.py          0%[                    ]       0  --.-KB/s               \rdrop_eval.py        100%[===================>]  10.96K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-07-18 17:28:10 (39.0 MB/s) - ‘drop_eval.py’ saved [11222/11222]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-ADai1tgIeL"
      },
      "source": [
        "#### load packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4w1GhOTfgIeM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa242051-6a09-4634-9b65-e2f11e1b85bb"
      },
      "source": [
        "# import warnings\n",
        "# warnings.filterwarnings('ignore')\n",
        "# warnings.simplefilter('ignore')\n",
        "\n",
        "\n",
        "# import logging\n",
        "# logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
        "# logging.getLogger(\"tensorflow\").addHandler(logging.NullHandler(logging.ERROR))\n",
        "import os\n",
        "from transformers import T5Tokenizer, TFT5ForConditionalGeneration\n",
        "import tensorflow as tf\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import tensorflow.keras as keras\n",
        "import drop_eval\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from datasets import Dataset, load_dataset\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "import re\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm.notebook import tqdm,trange\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "assert len(tf.config.list_physical_devices(\"GPU\")) > 0, \"No GPU found by Tensorflow\"\n",
        "\n",
        "if(run_toy): print(f'Running on {toy_size:,} records for development run')\n",
        "    \n",
        "!nvcc -V"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on 100 records for development run\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Wed_Jul_22_19:09:09_PDT_2020\n",
            "Cuda compilation tools, release 11.0, V11.0.221\n",
            "Build cuda_11.0_bu.TC445_37.28845127_0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5goXuHR1gIeM"
      },
      "source": [
        "#### Define model class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdP6lcFegIeM",
        "tags": []
      },
      "source": [
        "\n",
        "class T5forDrop(TFT5ForConditionalGeneration):\n",
        "    def __init__(self, *args, log_dir=None, cache_dir= None, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.loss_tracker= tf.keras.metrics.Mean(name='loss')\n",
        "        self.F1_tracker= tf.keras.metrics.Mean(name='F1')\n",
        "        self.EM_tracker= tf.keras.metrics.Mean(name='EM')        \n",
        "\n",
        "    \n",
        "    @tf.function\n",
        "    def train_step(self, data):\n",
        "        x = data\n",
        "        y_true = x[\"labels\"]\n",
        "        with tf.GradientTape() as tape:\n",
        "            outputs = self(x, training=True)\n",
        "            logits = outputs['logits']\n",
        "            y_pred = tf.math.argmax(tf.nn.softmax(logits,axis=2), axis = 2, output_type=tf.int32)\n",
        "            loss = tf.reduce_mean(outputs['loss'])            \n",
        "            grads = tape.gradient(loss, self.trainable_variables)\n",
        "\n",
        "        # Calculate F1 and EM Metrics\n",
        "        # Create a word mask to not count the padding/sentence tokens\n",
        "        recall_word_mask = tf.math.logical_and(\n",
        "                    tf.math.not_equal(y_true,0)\n",
        "                    ,tf.math.not_equal(y_true,1))\n",
        "\n",
        "        precision_word_mask = tf.math.logical_and(\n",
        "                    tf.math.not_equal(y_pred,0)\n",
        "                    ,tf.math.not_equal(y_pred,1))\n",
        "\n",
        "        # match the tokens\n",
        "        match_token = tf.math.equal(y_true,y_pred)\n",
        "        recall_match = tf.math.logical_and(match_token,recall_word_mask) \n",
        "        precision_match = tf.math.logical_and(match_token,precision_word_mask) \n",
        "\n",
        "        # calculate score\n",
        "        precision_array = tf.math.reduce_sum(tf.cast(precision_match, tf.int32) ,axis=1)/tf.math.reduce_sum(tf.cast(precision_word_mask, tf.int32) ,axis=1)\n",
        "        recall_array = tf.math.reduce_sum(tf.cast(recall_match, tf.int32) ,axis=1)/tf.math.reduce_sum(tf.cast(recall_word_mask, tf.int32) ,axis=1)\n",
        "\n",
        "        P = tf.math.reduce_mean(precision_array)\n",
        "        R = tf.math.reduce_mean(recall_array)\n",
        "\n",
        "        EM = tf.math.reduce_mean(\n",
        "            tf.cast(tf.math.logical_and(\n",
        "                    tf.math.equal(precision_array,1)\n",
        "                    ,tf.math.equal(recall_array,1)), tf.int32))\n",
        "        F1 = 2*(P*R)/(P+R)\n",
        "        \n",
        "        '''\n",
        "        Note, since FP and FN are the same, \n",
        "        the F1 score is the same as Precision, Recall, \n",
        "        which is the average \"match\" score\n",
        "        '''\n",
        "\n",
        "        y_true = tf.reshape(y_true, [-1, 1])\n",
        "\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
        "        lr = self.optimizer._decayed_lr(tf.float32)\n",
        "\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        self.F1_tracker.update_state(F1)   \n",
        "        self.EM_tracker.update_state(EM)           \n",
        "        self.compiled_metrics.update_state(y_true, logits)\n",
        "        metrics = {m.name: m.result() for m in self.metrics}\n",
        "        metrics.update({'lr': lr})\n",
        "        \n",
        "        return metrics\n",
        "\n",
        "    def test_step(self, data):\n",
        "        x = data\n",
        "        y_true = x[\"labels\"]\n",
        "        outputs = self(x, training=True)\n",
        "        logits = outputs['logits']\n",
        "        y_pred = tf.math.argmax(tf.nn.softmax(logits,axis=2), axis = 2, output_type=tf.int32)\n",
        "        loss = tf.reduce_mean(outputs['loss'])      \n",
        "        \n",
        "        # Calculate F1 and EM Metrics\n",
        "        # Create a word mask to not count the padding/sentence tokens\n",
        "        recall_word_mask = tf.math.logical_and(\n",
        "                    tf.math.not_equal(y_true,0)\n",
        "                    ,tf.math.not_equal(y_true,1))\n",
        "\n",
        "        precision_word_mask = tf.math.logical_and(\n",
        "                    tf.math.not_equal(y_pred,0)\n",
        "                    ,tf.math.not_equal(y_pred,1))\n",
        "\n",
        "        # match the tokens\n",
        "        match_token = tf.math.equal(y_true,y_pred)\n",
        "        recall_match = tf.math.logical_and(match_token,recall_word_mask) \n",
        "        precision_match = tf.math.logical_and(match_token,precision_word_mask) \n",
        "\n",
        "        # calculate score\n",
        "        precision_array = tf.math.reduce_sum(tf.cast(precision_match, tf.int32) ,axis=1)/tf.math.reduce_sum(tf.cast(precision_word_mask, tf.int32) ,axis=1)\n",
        "        recall_array = tf.math.reduce_sum(tf.cast(recall_match, tf.int32) ,axis=1)/tf.math.reduce_sum(tf.cast(recall_word_mask, tf.int32) ,axis=1)\n",
        "\n",
        "        P = tf.math.reduce_mean(precision_array)\n",
        "        R = tf.math.reduce_mean(recall_array)\n",
        "\n",
        "        EM = tf.math.reduce_mean(\n",
        "            tf.cast(tf.math.logical_and(\n",
        "                    tf.math.equal(precision_array,1)\n",
        "                    ,tf.math.equal(recall_array,1)), tf.int32))\n",
        "        F1 = 2*(P*R)/(P+R)\n",
        "\n",
        "        y_true = tf.reshape(y_true, [-1, 1])\n",
        "\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        self.F1_tracker.update_state(F1)   \n",
        "        self.EM_tracker.update_state(EM)           \n",
        "        self.compiled_metrics.update_state(y_true, logits)\n",
        "        metrics = {m.name: m.result() for m in self.metrics}        \n",
        "        return metrics"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esxfDrXO66Bz"
      },
      "source": [
        "#### Import model and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r08M8aqE65_M",
        "outputId": "e05d3d5b-2129-46be-b2d6-c69ad3adaec7"
      },
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained(t5_model)\n",
        "#replace numbers with special tokens\n",
        "numbers = {'additional_special_tokens':['1','2','3','4','5','6','7','8','9','0','<ss>','<sv>']}\n",
        "num_tokens_added = tokenizer.add_special_tokens(numbers)\n",
        "\n",
        "\n",
        "model = T5forDrop.from_pretrained(t5_model)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing T5forDrop.\n",
            "\n",
            "All the layers of T5forDrop were initialized from the model checkpoint at t5-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5forDrop for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIt7bbI8gIeN"
      },
      "source": [
        "#### Import data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQD_dStuSamm"
      },
      "source": [
        "def make_toy(dataset,toy_size=1000):\n",
        "    df = dataset.to_pandas()\n",
        "    df = df.head(toy_size)\n",
        "    return Dataset.from_pandas(df)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaxF09BQgIeO",
        "outputId": "4e903313-74c3-40da-950a-b7e930a341c7"
      },
      "source": [
        "train_dataset_full = load_dataset(dataset, split='train')\n",
        "valid_dataset_full = load_dataset(dataset, split='validation')\n",
        "\n",
        "print('Dataset features: ',train_dataset_full.features)\n",
        "\n",
        "#reduce data to toy size if run_toy flag is set\n",
        "if(run_toy):\n",
        "    train_dataset = make_toy(train_dataset_full)\n",
        "    valid_dataset = make_toy(valid_dataset_full)\n",
        "\n",
        "else:\n",
        "    train_dataset = train_dataset_full\n",
        "    valid_dataset = valid_dataset_full\n",
        "    \n",
        "#check out one record\n",
        "data = next(iter(valid_dataset))\n",
        "print(\"\\n\\nExample data from the dataset: \\n\", data)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/6b6c4172d0119c74515f44ea0b8262efe4897f2ddb6613e5e915840fdc309c16)\n",
            "Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/6b6c4172d0119c74515f44ea0b8262efe4897f2ddb6613e5e915840fdc309c16)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Dataset features:  {'id': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'context': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}\n",
            "\n",
            "\n",
            "Example data from the dataset: \n",
            " {'id': '56be4db0acb8001400a502ec', 'title': 'Super_Bowl_50', 'context': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.', 'question': 'Which NFL team represented the AFC at Super Bowl 50?', 'answers': {'answer_start': [177, 177, 177], 'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos']}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jo9sZK0bgIeS"
      },
      "source": [
        "#### set parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_Zab82vSamn",
        "outputId": "5764b011-316c-4a98-f49e-7cce90e74802"
      },
      "source": [
        "steps = int(np.ceil(len(train_dataset)/batch_size))\n",
        "valid_steps = int(np.ceil(len(valid_dataset)/batch_size))\n",
        "print('Training datset size: {:,} records'.format(len(train_dataset)))\n",
        "print('Validation datset size: {:,} records'.format(len(valid_dataset)))\n",
        "print('Batch size: {}'.format(batch_size))\n",
        "print(\"Total Steps: {:,}\".format(steps))\n",
        "print(\"Total Validation Steps: {:,}\".format(valid_steps))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training datset size: 1,000 records\n",
            "Validation datset size: 1,000 records\n",
            "Batch size: 8\n",
            "Total Steps: 125\n",
            "Total Validation Steps: 125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZCcTPA0gIeS"
      },
      "source": [
        "#### Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnCSjtGwgIeT"
      },
      "source": [
        "def encode(example,\n",
        "           encoder_max_len=encoder_max_len, decoder_max_len=decoder_max_len):\n",
        "  \n",
        "    if dataset == 'drop':\n",
        "        context = example['passage']\n",
        "        question = example['question']\n",
        "\n",
        "        answer = example['answers_spans']['spans']\n",
        "        answer_type = example['answers_spans']['types']\n",
        "    elif dataset == 'squad':\n",
        "        context = example['context']\n",
        "        question = example['question']\n",
        "        \n",
        "        answer = example['answers']['text']\n",
        "        answer_type = 'text'\n",
        "    \n",
        "    \n",
        "    question_plus = f\"answer_me: {str(question)}\"\n",
        "    question_plus += f\" context: {str(context)}\"\n",
        "    \n",
        "    answer_plus = ', '.join([i for i in list(answer)])\n",
        "    answer_plus = f\"{answer_plus}\"\n",
        "    \n",
        "    encoder_inputs = tokenizer(question_plus, truncation=True, \n",
        "                               return_tensors='tf', max_length=encoder_max_len,\n",
        "                              pad_to_max_length=True)\n",
        "    \n",
        "    decoder_inputs = tokenizer(answer_plus, truncation=True, \n",
        "                               return_tensors='tf', max_length=decoder_max_len,\n",
        "                              pad_to_max_length=True)\n",
        "    \n",
        "    input_ids = encoder_inputs['input_ids'][0]\n",
        "    input_attention = encoder_inputs['attention_mask'][0]\n",
        "    target_ids = decoder_inputs['input_ids'][0]\n",
        "    target_attention = decoder_inputs['attention_mask'][0]\n",
        "    \n",
        "    outputs = {'input_ids':input_ids, 'attention_mask': input_attention, \n",
        "               'labels':target_ids, 'decoder_attention_mask':target_attention,\n",
        "                }\n",
        "    return outputs\n",
        "    \n",
        "def to_tf_dataset(dataset):\n",
        "    '''convert from arrow to TF dataset'''\n",
        "    \n",
        "    columns = ['input_ids', 'attention_mask', 'labels', 'decoder_attention_mask']\n",
        "    dataset.set_format(type='tensorflow', columns=columns)\n",
        "    return_types = {'input_ids':tf.int32, 'attention_mask':tf.int32, \n",
        "                'labels':tf.int32, 'decoder_attention_mask':tf.int32,}\n",
        "    return_shapes = {'input_ids': tf.TensorShape([None]), 'attention_mask': tf.TensorShape([None]), \n",
        "                  'labels': tf.TensorShape([None]), 'decoder_attention_mask':tf.TensorShape([None]),}\n",
        "    ds = tf.data.Dataset.from_generator(lambda : dataset, return_types, return_shapes)\n",
        "    return ds\n",
        "\n",
        "def create_dataset(dataset, cache_path=None, batch_size=batch_size, \n",
        "                   buffer_size= 1000, shuffling=True):\n",
        "    '''returns a padded_batch tf dataset'''\n",
        "    if cache_path is not None:\n",
        "        dataset = dataset.cache(cache_path)        \n",
        "    if shuffling:\n",
        "        dataset = dataset.shuffle(buffer_size)\n",
        "    dataset = dataset.padded_batch(batch_size)\n",
        "#     dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return dataset\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254,
          "referenced_widgets": [
            "2a7ea77e513a4ee98f4f935473562d57",
            "a9e7d88792e94333b4657e58dc7cb8f5",
            "5d7282b5fcd44ce992b82c30a4f96654",
            "840b2a8c7a064cce9e31de2e8921dda7",
            "4a8518c321104a3d897882425d59f31d",
            "43280ddcb8cf4f529295753fa13a9fbc",
            "1f90e298d2f943888a2475d65b4148db",
            "0f43317567cc4ddabbbf217c6d9b8eb9",
            "143645d284644e7486394aae9cfb028d",
            "2d6860a9b9d344fb8f75ca6529cfd78f",
            "c2e4e22bf5004f43812b76f88fa39f08",
            "07d383bb04444df1b32ebc705ff31820",
            "c8e615c4414e4a1ba421bddc5a6d435e",
            "b6daff759ffc461782f41e6e73ae8ae6",
            "2cdd4e3d16104157b28a16bdc3b854ca",
            "01b172ec286e4e88afda35a3d2dd5b0d"
          ]
        },
        "id": "2xiO1_gwgIeT",
        "outputId": "e811d2e5-863d-431a-c0f7-c6f7e9bda41f"
      },
      "source": [
        "#Preprocess data\n",
        "train_ds = train_dataset.map(encode)\n",
        "valid_ds = valid_dataset.map(encode)\n",
        "\n",
        "tf_train_ds = to_tf_dataset(train_ds)\n",
        "tf_train_ds = tf_train_ds.repeat(epochs)\n",
        "\n",
        "tf_valid_ds = to_tf_dataset(valid_ds)\n",
        "\n",
        "tf_train_ds= create_dataset(tf_train_ds, batch_size=batch_size, \n",
        "                         shuffling=True, cache_path = None)\n",
        "tf_valid_ds = create_dataset(tf_valid_ds, batch_size=batch_size, \n",
        "                         shuffling=False, cache_path = None)\n",
        "\n",
        "print('dataset schema:')\n",
        "tf_train_ds.element_spec"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a7ea77e513a4ee98f4f935473562d57",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "143645d284644e7486394aae9cfb028d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "dataset schema:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attention_mask': TensorSpec(shape=(None, None), dtype=tf.int32, name=None),\n",
              " 'decoder_attention_mask': TensorSpec(shape=(None, None), dtype=tf.int32, name=None),\n",
              " 'input_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name=None),\n",
              " 'labels': TensorSpec(shape=(None, None), dtype=tf.int32, name=None)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHAi1i2BgIeV"
      },
      "source": [
        "#### Callbacks and checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p-Da_UfbACy"
      },
      "source": [
        "# Learning Rate Schedule to input into CustomLearningRateScheduler()\n",
        "def lr_schedule(training_steps, lr):\n",
        "    \"\"\"Helper function to retrieve the scheduled learning rate based on epoch.\"\"\"\n",
        "    if training_steps < __TOTAL_WARM_UP_STEPS:\n",
        "        # print(\"\\nWARM UP: Using Increasing Linear Function at Training Step:{}\".format(training_steps))\n",
        "        lr = ((MAX_LR - START_LR)/__TOTAL_WARM_UP_STEPS) * training_steps + START_LR # y = (m)x + b\n",
        "    else:\n",
        "        __CURRENT_DECAY_STEP = training_steps - __TOTAL_WARM_UP_STEPS\n",
        "        if DECAY == \"ExponentialDecay\":\n",
        "            # print(\"\\nDECAY: Using ExponentialDecay at Training Step:{}\".format(training_steps))\n",
        "            __DECAY_FN = keras.optimizers.schedules.ExponentialDecay(\n",
        "                  initial_learning_rate=MAX_LR,\n",
        "                  decay_steps=__TOTAL_DECAY_STEPS,\n",
        "                  decay_rate=END_LR/MAX_LR)\n",
        "        elif DECAY == \"PiecewiseConstantDecay\":\n",
        "            # print(\"\\nDECAY: Using PiecewiseConstantDecay at Training Step:{}\".format(training_steps))\n",
        "            __CURRENT_DECAY_STEP = tf.Variable(__CURRENT_DECAY_STEP, trainable=False)\n",
        "            __DECAY_FN = keras.optimizers.schedules.PiecewiseConstantDecay(\n",
        "                  __BOUNDARIES, __VALUES)\n",
        "        elif DECAY == \"PolynomialDecay\":\n",
        "            # print(\"\\nDECAY: Using PolynomialDecay at Training Step:{}\".format(training_steps))\n",
        "            __DECAY_FN = tf.keras.optimizers.schedules.PolynomialDecay(\n",
        "                  MAX_LR,\n",
        "                  __TOTAL_DECAY_STEPS,\n",
        "                  END_LR,\n",
        "                  power=POWER)\n",
        "        elif DECAY == \"InverseTimeDecay\":\n",
        "            # print(\"DECAY: Using InverseTimeDecay at Training Step:{}\".format(training_steps))\n",
        "            __DECAY_FN = keras.optimizers.schedules.InverseTimeDecay(\n",
        "                  MAX_LR, __TIME_DECAY, 1)\n",
        "        else:\n",
        "            print(\"Please Select a Decay Function\")\n",
        "            exit\n",
        "        lr = __DECAY_FN(__CURRENT_DECAY_STEP)\n",
        "    return lr"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "dOhgt31tbFnR",
        "outputId": "e666508c-6455-4a5a-e66e-45292f42b2c9"
      },
      "source": [
        "# Learning Rate Schedule Parameters\n",
        "WARM_UP_FRACTION = 1/10 #The fraction of the training steps that will be ramping up linearly to the max LR\n",
        "START_LR = 0.0005 # must be float\n",
        "MAX_LR = 0.001 # must be float\n",
        "END_LR = 0.0001 # must be float\n",
        "\n",
        "EPOCHS = 100\n",
        "STEPS_PER_EPOCH = 1\n",
        "__TOTAL_STEPS = EPOCHS * STEPS_PER_EPOCH\n",
        "__TOTAL_WARM_UP_STEPS = __TOTAL_STEPS * WARM_UP_FRACTION\n",
        "__TOTAL_DECAY_STEPS = __TOTAL_STEPS - __TOTAL_WARM_UP_STEPS\n",
        "\n",
        "#--------------------------------------------------------------------------------------------\n",
        "\n",
        "# Decay Parameters\n",
        "# ExponentialDecay - None\n",
        "\n",
        "# PiecewiseConstantDecay\n",
        "NUM_BOUNDARIES = 10\n",
        "__BOUNDARY_STEP = __TOTAL_DECAY_STEPS/NUM_BOUNDARIES\n",
        "__BOUNDARIES = list(np.arange(0,__TOTAL_DECAY_STEPS+__BOUNDARY_STEP,__BOUNDARY_STEP))\n",
        "__PIECEWISE_STEP = (MAX_LR - END_LR)/(NUM_BOUNDARIES)\n",
        "__VALUES = list(np.arange(MAX_LR,END_LR-__PIECEWISE_STEP,-__PIECEWISE_STEP))\n",
        "\n",
        "# PolynomialDecay\n",
        "POWER = 7 # Must be > 1 to reach END_LR\n",
        "\n",
        "#InverseTimeDecay\n",
        "__TIME_DECAY = __TOTAL_DECAY_STEPS/(MAX_LR/END_LR-1)\n",
        "\n",
        "#--------------------------------------------------------------------------------------------\n",
        "# Initialize Graphing\n",
        "decay_array = [\"ExponentialDecay\", \"PiecewiseConstantDecay\", \"PolynomialDecay\", \"InverseTimeDecay\"]\n",
        "x = range(__TOTAL_STEPS)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15, 6),ncols=len(decay_array))\n",
        "fig.suptitle(f'Learing Rate Schedule', fontsize=14)\n",
        "ymax = max(START_LR,MAX_LR,END_LR)\n",
        "ymax *= 1.1\n",
        "\n",
        "\n",
        "ax[0].set_ylabel('Learning Rate')\n",
        "for i,decay_type in enumerate(decay_array):\n",
        "    DECAY = decay_type\n",
        "    lr_values = []\n",
        "    for step in x:\n",
        "      lr_values.append(lr_schedule(step,0))\n",
        "    ax[i].plot(x,lr_values)\n",
        "    ax[i].set_title(decay_type, fontsize=14, pad=11)\n",
        "    ax[i].set_xlabel('Steps')\n",
        "    ax[i].set_ylim([0,ymax])\n",
        "    if i > 0:\n",
        "        ax[i].tick_params(which='both', left=False, labelleft=False)\n",
        "plt.show()\n",
        "#--------------------------------------------------------------------------------------------\n",
        "\n",
        "# ACTION: Select Decay Type\n",
        "DECAY = \"PiecewiseConstantDecay\" #[\"ExponentialDecay\", \"PiecewiseConstantDecay\", \"PolynomialDecay\", \"InverseTimeDecay\"]"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11\n",
            "12\n",
            "10.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA40AAAGeCAYAAAAwieQlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeZxkVX338c+v9+7prp6Z7p6ehWUGGIHBDRwQARVFBeMaJQajTzTuSUyiPuqjz2MMMdHEJTGLMcEtbqgYl4BgXBFcWBRBUXYGBhiqevaq7ump6vU8f5xTPXdqqrprudVV3fN9v171mqm6t+49VX1P3fu755zfMeccIiIiIiIiIsW0NLoAIiIiIiIi0rwUNIqIiIiIiEhJChpFRERERESkJAWNIiIiIiIiUpKCRhERERERESlJQaOIiIiIiIiUpKBRRESagpltNDNnZlsbXZalyMyuM7OP1WG7W8PfZWMF7zk/vGcw7vKIiMjiU9AoIiKHMbPPmtnVDdj1I8A64Ff13pGZbQ9BjTOzrJndbWbvMDOrYjtvj6lMLzazG80sbWYHQpk+Fce2RUREaqGgUUREGs7MOpxzM865Eefc9CLt9n34IPVU4CPAB4A3LNK+D2NmFwD/BXwLOBs4HXgHUFEQKyIiUg8KGkVEpCJmtsXMrjGzMTPbZWZfNrO1keVnmtn3zGyPmY2a2U/N7CkF23Bm9qdm9g0zGwc+UNg9NdLF8QIzu9nMDprZLWZ2RsG2XmNmD4fl3zKzPzEzV8ZHGQtB6nbn3KeA24HnRLZ7opldaWYjZjZuZrea2fMjy68Djgc+nG+1jCw7x8yuD2V61Mz+3cwS85TlBcDNzrkPOOfuds7d55z7lnPutQWf9WwzuzaUJxP+vz6ySouZfSB897vM7CNm1hJ5f4eZfdDMdoSy/cLMLizYx0WhlTNnZj8BHlOw/NVmdqDgtQW7o1bxnYiISJNQ0CgiImUzs3XAj4HfAmcBzwJ6gSsjwUkf8AXgqWGdXwHfNrOBgs39FfBt4HHAv82z278D3gWcAewFLs93Iw3B6KfC+58IXAX8dYWfyczsfHyL41RkUS/wP8CzgScAXwe+YWanhOUvAXZwqMVyXdje44DvhbI8Iaz3ROAz8xRjBDjFzJ4wTzmfAPwIuB84F98ieQXQFlntFcA0cA7wZuAtwO9Hlv8n8HTgD4DHAp8DvpXfr5kdC/w38P1Q5n8FPjRPuctS5XciIiJNwpwr52asiIgcLczss8Cgc+75RZa9DzjXOXdB5LVVwD7gyc65nxd5jwFJ4B3OuS+G1xzwMefcn0XW2wg8CJzpnLslBHI/Ai5yzn03rHMu8FPgWOfcDjP7MrDKOXdRZDufAF7vnCvZtdPMtuODvCmgA2gHcsAFzrkb5nnfTcDVzrm/jWznY865j0TW+TwwFW0lNLMnArcBw865XUW2uwL4KvA7+ED0ZuAHwBedcwfCOpcDJzjnnlL4/rD8OqAzutzMvg885Jx7nZmdCNwHbHTOPRxZ57+BpHPuT8zsA8DFwMkuXCCY2XuAvwE2Oee2m9mrw2fujWzjfPzfasg5t6fI84q/ExERaR5qaRQRkUo8CXia+UQtB0I3xUfCshMBzGyNmV1mZveaWQYYA9YAxxVs65Yy93l75P/J8O+a8O8pQGGgenOZ2/1HfGvX0/EBzl9HA0YzW2FmHzKzO81sf/isWznycxR6EvDKgu/oZ2HZicXe4Jwbd849DzgJ31Kaxrew3mFmw2G104FrF9j37QXPkxz6rs7Aj5G8s6Bsz4uU61TgpnzAGNy4wD7LUfF3IiIizaNt4VVERETmtADXAMUyhu4M/34OGAbeCmwHJoAf4lv0osbL3Ge0y2g+mInjpude59z9wP1m9lLgPjO72Tn3o7D8I8BF+M96H3AQ+DxHfo5CLfgusx8tsuzR+d7onNsGbAM+ZWbvB+4F/hi4tKxPdPh3Bf77yn9XLeH5mUXWy5a5fYBZjkzQ077Ae6r+TkREpPEUNIqISCVuBV6G7/JYGHjknQf8uXPuGoDQUrauTuW5Gx8ERZ1V6Uacc/vNz3H4UTM7PbS0nQd83jn3dQAz68K3it0beesk0FqwuVuB00JAWovt+EA13w30NuCZNWzvNnywtzYSGBe6C3ipmVmktfHsgnV2Az1mlnDOjYbXnrjAvuP6TkREpAHUPVVERIpJmNkTCx4b8Qln+oErzOzJZnaCmT3LzD5hZn3hvffiuyJuMbMzga/gg6t6+BfgOebnWNxsZq8FfrfKbX0cOBn4vfD8XuB3zeyMkMjli0BXwXu2A081sw2RzKEfBM4ys/8ws9PN7CQze76ZXVZqx2Z2aegKe76ZbTKz0/FJYnrxyWMAPgycHr7rJ5jZyWb2OjNbqLssAM65e4HLgc+a2cXhb7fVzN5uZi8Jq/0HsBH4p7D9i4E3FWzqZnwr8d+Fz/ZS4E8W2H3F34mIiDQPBY0iIlLMU/EtU9HHR5xzSXzmzlngO8Ad+EByIjwAXoMPdn6JDxg/gw+uYuecuxF4PfDn+PF8L8YHKLkqtrULn/X10pAJ9m3ALuAn+CyqN4X/R70XOBbfpXR32M7twNPwwdf1wK/x4xN3Utr1wCZ81967gO+G97/QOffjsN1f4bPVnhLKcjNwCUd2NZ3PH+EzqH4I30p7dSjrQ2EfD+Mzm14Uyv1WfObaOc65ffgsrc8GfoOf2/Iv59tpld+JiIg0CWVPFRGRZcXMPgo8yzn3uEaXRUREZDnQmEYREVnSzOwd+HkFD+Bb4t4E/N+GFkpERGQZUUujiIgsaWZ2BXA+fqzlg8BlwD87neBERERioaBRRERERERESlIiHBERERERESlJQaOIiIiIiIiUpKBRRERERERESlLQKCIiIiIiIiUpaBQREREREZGSFDSKiIiIiIhISQoaZdGZ2avN7ECF77nOzD5WrzJJ86jm+FhsZubM7OJGl0NkuTCzjaFebW10WcphZtvN7O0VrH9++HyD9SyXyFKl67zmp6BxkZnZZ8OJo/BxU6PLVg8lLq6vAE6ocbvXRb67STNLmdl3zOyVZma1bFvqr6AeTJnZA2b2ETNbQQzHxyJYB3wrzg2Gi8qrzWyPmWXN7G4z+1cz2xjnfhYow2fN7OrF3HbB7+DBcCx8ycyeWo9ySP0sUK+XmzOBj1f75kgQ6cxs1sxGzex2M/tnM9sUYzllCavnb/JiCTeCi133Rh/nAy8B3r0I5bk0st9pM9tnZjeY2bvNrLfe+1/KFDQ2xg/wF53Rx+80tESLyDmXdc7timFT/4n/7k4AXgjcCFwGfNPMWmPYvtRXvh6cALwH+BPgIzEeH3XjnBtxzk3EtT0zeyPwQ2Av8HvAqcBr8b/R74lrP03s9fhjIf+5J4HrzewdDS2VVKNovW5oierAObfbOXcwhk2dBqwHzgD+Ovz7GzN7egzbFqmZmXXUuIkrOPx69wfAVwteu8E5t885N1bjvsp1T9jvscBTgc8BbwRuM7O1i1SGpcc5p8ciPoDPAleXWPZ0YAo4P/LaG4FR4ITw/DrgP4B/BvaHx4eBlsh7VuErwH4gi6+gp0WWvxo4AFwA/BYYB34EbCoozwuAXwI54EHg/UBHZPl2/EXBZaGMO4B3FCx3kcf26P4j650IXAmMhLLcCjy/oCzXAR8r9Tzy+oVhX38Uea0f+ASwCxgDrge2FrzvbODasP9M+P/6sOwi4Cfh+9wHfBc4NfLeawvLAiSAg8BLGn3MNeOjWD0APgmkCo+PMo/FDuADwEPABPAA8OeR5VuAa8LffxfwZWBtWHZKOGbyz3vCNr4Tef/rgPsjzx1wceT5eyP7HgE+H1lmwDuBbfj6+BvglZHlx4T3/UuJ72pl5P8vCe+fAB4B/h9g5dbJsM4bgXvDd7knHM9twKUcXl8d4bcI+Hv8STYb9vEhoCuyzUvxvyWXhM85Bvw3MBhZXmrbh32XkW1+AJgGTirn7xhZ51WR72gn8LnIsrcBt+Pr+aPAp/LfL7AifGcXF2zv2fjf5eFG15tmfzB/ve4E/in8TXLATcB5kfU2hmNha6gz9wNvL9jW5rDOGZFj5w3Af4W/6QPRuhXWeRz+HJjF/35/FugvLDPwf/B1NxOO95Zw3O4Kr/+fgu1uj5ZvvmMrLD8/lHew2PPIeq348802oDXyetW/gWGbnw7vywL34X+TWsLyp4VjvLAuvR+4vdHH1dH8iNapyLH6F+EY24+/ed4Tlr8h1K/Wgm18CbiqgmNpezj2PwOkgf8Kr1d9nisoz9XAZ4u8fh2HX+dtD/v8LP43/xHg94GVwFfw17H3Ac8p2M6854nw2X5bZP/r8DduP1fJ58Lf9Lk8vPcg8CvgGWHZvNe34fMVK8vPKHFN0NDjsdEFONoezBM0huUfCBVjFf5idhx4VWT5daEi/GtY/jL8Se5tkXWuBO7GnwgeB1wVttkdlr8af4L4AXAW8HjgNuC7kW1ciL+A+qNw0D8Df9H4kcg620MleTNwEvBn+JPgU8LyofD8dcBaYCiy/2jQ+ATgTaGsJ+EvhCeBUwo+94JBY1h2O4d+ZA34afgBOSts/2/CZ1sX2X8WH1g+Ed/a8UbguLD8peGxOXxXX8Vf0HSE5S/HX4x0RsrwRvyPVXujj7lmfBSrB8C/4IOYwuOjnGPxy/gA6aX4Fo5nAH8Ylq0L2/1g+Ns+Ht+19GYOXTSlgEvC/58F7MbXs7bw2heBT0X2NxfohH2OAs8DjsNf9L45su77Q3kvAjYBf4Cv188Ly98atrd+ge/sScAMvjXiMcAr8CfNP6ugTm7FB2KvAI4Px/5b8UFjL/6O8Pfx9XVt5Bj/S+Bc/IX97wAPA38T2e+loSzfDN/vU/AXF5eF5fNtu1TQOADMEi7My/w7vhF/IfQ24OTwnUVvZL0FeGb4HE/H/1Z8IbL8MuDbBeX4MvDNRteZpfBg/nr9z/h69rzw9/tkOGbyv8Mbw7GwNTx/N3BHwbb+DritoB7uAF4Zjve/w5878r/dK4Ak/gbG48Lf/F7g6wVlHsXfjD0F/3s+C3wnbO8x+POTA55UUNeiQeNCx9b5lBE0hmUvKfguav0NbAfeh+9SuxF/3ZAGXht5/93AOyPPW/DXDX/R6OPqaH5wZNCYCXXnVOA54e/47rB8Ff7376LI+3vx55uXVXAsbQ/rvDPUq83UeJ4r+EyVBI378L0VNgP/ED7ft4E/DGX7NP5aqyu8p5zzxKUUCdTCsn8J33FLOZ8L/xtzHz7Ie2r4Tl/CoaBx3utb/E3jaeCsSBlOxtf/JzT6+Dvi+2l0AY62R6j00/iTZfTxwbC8HfgF8A38HYkrCt5/Hf6kF21deA+wI/w/fyf2aZHl/aESvC48f3VY5+TIOq/A3z2y8PzHwF8W7PvFoaz5dbYDXy5Y5z7gPZHnR1wQUqQlqcj3dFPBdq6j/KDxK8Cd4f/PDGXuLljnV4QTJP4O0Y0V/A1X4C/ezwvPO/E/UpdE1rmZyI+wHkXrwdWR52eF7/CKwuNjoWMxcsxfVGJf7wN+WPDaqvCesyLHTD7A+Vvg38PxnQ+2HuHw1sFo0Pg2/EnliBsE4VjJAk8teP2fCMEJflxUpozv7HLg2oLXLiXU/fB83jqJP5llgL5y/i7zlOVNHN7yein+ZB5twfl/BesU3TYlgsawbAT4eAV/xx3A31dwHF6E/93LXyDkg+oNke1nKej5oEfJ77NUvf4v/IXSH0aWteLv3v9teL6RwwOltfibm2dH1n+Uwy9UHfB3kedt+Dv9rwzPX194vHMoWDspUuZHOLxV7xbg1wWfbTuHB4mHPS/j2Mrvt5ygMd/7IX+hX9NvYIny/T3wg8jztwN3RZ4/N5R/oNHH1dH84MigsfBY/WTB3/EbHH6z4pWhDuSDqnKv7b5VsE5N57mC1ysJGr8ced4bjvN/iby2kcN/N8o5T1xK6aAxf4NoTTmfC/8bM1asHs/zNy28vr0a+I/I8w8CtzT62Cv20JjGxvgxvkUr+vgwgHNuCn8n4/n4g/aNRd5/kwtHVnAjsMHMEvg7K7PhNcI2M/gm9S2R90w45+6JPE/iu7esCs+fBPw/MzuQf+C7OKzAn8zzbi8oWzKUu2xmtsLMPmRmd5rZ/rCvrfi7WdUwfKUH/zl6gN0Fn+Wx+DtCAKfju5iWKt+JITHHNjMbxXf/aMmXz/mxbV8AXhPWPw1/sfTpKst/tLgo/D1y+OP1x/iWsUILHYun44/5H5XYz5OApxW8/5GwLH8MXIe/iCP8+6P8a2Z2Ev5u4HUltv9fQBfwoJl92sx+z8w6w7ItYdl3Cvb/x5F9l5u46VT83cyon3Ko7ufNVye/j28BfNDMLjezV5lZ30I7NrOLzeynZjYSyv9RjqyfD4XfmmL7rVZhXS75dzSzNcAG/NjQUp/jmWb2fTPbYWZj+AusDsJvmnPuFvxv5avCW/4Af6f7f2r8HEeTYvX6X/E3ROeOX+fcTFi+pdhGnHMj+Iup1+S3C6zG3zyJuj3ynml8L4H8cXcqvntldJzUDfjfi+h+7wzlyduJ725NwWslj+eFjq0K5X8Tosd+Lb+BmNmbzOwWM9sd3v9WDq/DnwNOMLNzwvPXAP/tnNtbRfmlfgqP1cLf2S8CLzaznvD8FfiW9Vx4Xu613S0F+631PFetaP0+gL8p9JvI8p3h3/x3UM75fj7RulfO5zod/xuzp+jGyru+/SRwiZl1h3wc/4smvX5sa3QBjlIHnXP3z7P8bHxQshLfxTMd036jgeZ0iWUtkX//Gv9DUWh35P9TRbZT6c2Ij+AvCN6ObxU5CHwef8Ktxhb8eA5CWXbiuw0UGi1ze1fjWzDeiL/TPQ3cWVC+TwG3m9lx+JPtjc65uyov+lHlx/gxGFNAMtwwwY5MflvusVhKC757crH0+PkTznXAv4cAcWt43oMPGnYD25xzO4pt3Dn3iJmdjB8j/Cx8F5q/MrMnc6guvADfpTMqX3fuBRJmtt45lyzj8xQtRpHtRpe1hLKOmdkZ+K7rz8Z3AfyAmZ1Zat9mdja+Jfav8ReaaXziqcLkJnH8FkT3O4j//YvW5fn+jt0LbO/48P5P4seR7MUnHfkyR9blv8APFXgNfnzLDFKuI+q1mT1+nvXdPMs+BXzJzN6C/1t80zm3v2Cdao+7hepM2dut4NgqVz6gjR77Vf8Gmtnv41tH3o4PmkeBPwV+N7+Oc263mV0FvMbM7sHX8RdUUXapr4WOy2vw1ygvMrMf4s9JF0aWl3ssjR+2k9rPc9VaqG4Wu3Zd6Hw/ny34+rGXQ1nca/lc5VzfXhNefym+VXglPpBvOgoam4z5VNsfw/+gXwR80czODXdQ855sZhZpbTwbf3IeNbO78JXmKfiTN6EV4nH4AdPluhXf53q+4LYcU/huRfM5Dz+g+usAZtaFv4tzb6U7M7ML8a2I+QvaW4FhYNY590CJt92G78ZabHsD+K5Cf+Kc+1F47QwK6o5z7g4zuxnfVeGV+K55Mr+Fbp7kzXssmtmv8Mf8M/DjkIq9/2X4lrCiP/TOubvNbAT/d9vmnNtlZtcB/4ZPNnDdfAUMd3GvAa4xs7/Hd6s8F9+SMgEc75wr1Zr9NXxXsXcBf17k8610zqWBu8I2o87Dd08tO+Nc+C25FrjWzP4KPx7k+fgxvZMcWV/PBR51zv1NpEzHl7u/iGLbns//xree/Hd4vtDfcczMHsVf1Hy/yPKt+BP1W/NBoJk9v8h6lwMfNrM34y/8L6mgzFK8Xm/D//3PDf8n3FF/CvNfHH0HfwH3JvyFW6VZxu/CB0F9kTpyDv73Is6beuUeWwsK38tb8N/Tr8LLtf4Gngfc7Jz7WOQ9xVpdPon/PXoA/xv2g2o+gzSOc27CzP4L38I4iP87XhdZpepruxrPc4tlwfN9KWa2Dn+j+BvOuVkzu5OFP9dtwP8ys8ESrY0LXt8656bN7LP4G2OZsP9MkW01nILGxui0I1P6zuC7QX0BuN45d5mZfQ3fDP9X+EQUeeuBfzKzj+ODwXfgx2HhnLvPzK4ELjOzN+BbBd6PP/FWcufifcDVZvYQPvHLND4YO8s5984KtrMduMDMrsd3iS28Swy+8vxuKPcU/vN2lbHtnvA9tnFo2pJ34hMBfTGs8wN8l6grzeyd+MH+a/EB+Q+ccz/Bdw2+ycw+gQ8ScviWye/hWxj3AK83s0fw3d8+zJEtteBPuP8RPsMVZZRfyjPvseicu9fMvgp8ysz+An/SOAbY6Jz7Av5v+nrgCjP7IP5u6gn4E8v/jlxMXo8P+C8DcM5tN7Pd+HGAf1SqcGb2avwxeDN+XMjv44+B+0LL3keAj5hvQv0xflzG2fgbGZ8Id3DfCnzMzPrxN3cexNfzP8DXhdfj7+z+wswuxdflM/GB1f8t94sMF7InhnLsw19k9nHoAno78NxwR3kv/gR2L74L7CvwFwcX4pOFVOqIbUdO6itDXe4I5XsVPtHBO51z28I65fwd3w981Mx24i9ueoALnHP/gL/L2wK8xcy+gf8bvKWwkM65dLjo+gfgx865+6r4rBLhnBs3s38HPmhme/DH91vxN/RKznXonJsxs8/gE9I8yjxdj0u4HN+q8nkzey9++MVl+IuyWm+IRpV1bJWwxszyiagej/9eTgd+J9LCXetv4L3Aq83sufgkbpfgk/UUno+/j6+bf4UfGzxb6RchTeGL+LqyCT8mMPp3rOrartbzXMyfbz7lnu/bwjnH8N3ez8WfS/cR5oos83N9CX/D90ozexf+d+qxwFhoaCj3+vZT+AzOs/gER82pHgMl9Sj9wA9kdkUeO/CB4Qghy2hYP5/uPZ905Tp8YPIxfEC4H39xEx0YXdaUGwXlOp+CQfn4A/cn+GbzUXwf92gSgu0cmRL9Og4fyPwC/Al1itJTbhwfyjgevoe3UzBQush2r4t8d5Phe/sOvi+4FZSpD5+5b0dY9xF8d7sTI+uch/9ByIbv9Qccyur3TPz4llz490L8j+arC/bTgx8Q/ZlGH2fN/mD+qWeKHZ8LHYud+GkgHsXfGdxWsHwz/g56vk7cgx9nFU0znh8Af3FBOR1wTEF55tbDJxG4MRw34/hEVtGU2oYfq5m/a7kbf3H27IJtXoDPCrc3HGv5Mh4fWSc/5Ub+OC425UbJOhmO8x+FfWTD8RydnmYIf7NkjMOnxfi7UO4D+LFafwy4yPsupSCxQOHfcZ5tR38H8yngv0wkmVeFf8fXhu86/7vwmciyPw/HSBZ/UfWysN+NBft5Wnj9DwvLoEfV9To65cYE80y5UfC+48Pr7y2yzcPqa7E6gL+x+sPwN99PiSk3CrZxRKKOUN7CDJPR/cx7bFE6EU7+MYavj/9CmGKrYP9V/wbib8Z8Onz+dPj/ewnn5IL9vBd/4bqxcJkeja1TJY7VSznyt9c4NOXZ46s4lg47tsNrsZznwrqVJMIpLMdh1174AMwVlGXe8wSHTwE1E9a7ER809hX5Luf9XPgbNFeE7+YgvvXx/LBswevbyHauDfXWCpc1yyOfKUmWiNBl7rfOuTc3uixyODNbj+/3/nTnXGHCEhFZIsIYsMvw06DEMYG7VCmMmfoZPpAqHFckMQstwic5557d6LKIHE1Cd9jLnXPvb3RZSlH3VJEamVk7fk65D+DnEFPAKLIEmc84uBZ/x/mTChgbx3xmxiH8vLrfVMBYX6Fr/BZ8t/CXNbg4IkcNMxsCLsb3trissaWZn6bcEKndufhJq8/B96UXkaXpnfiuTPvwwYo0zsvx08MM4ueIk/q6Et+t9jPOuWsaXRiRo8gu/FjTN7oSU3c0C3VPFRERERERkZLU0igiIiIiIiIlKWgUERERERGRkhQ0ioiIiIiISEkKGkVERERERKQkBY0iIiIiIiJSkoJGERERERERKUlBo4iIiIiIiJTU1ugCNNLg4KDbuHFjo4shEqtf/vKXe5xzQ40uR57qmSxHzVbPQHVNlh/VM5HFUU5dO6qDxo0bN3LLLbc0uhgisTKzhxpdhijVM1mOmq2egeqaLD+qZyKLo5y6pu6pIiIiIiIiUpKCRhERERERESlJQaOIiIiIiIiUpKBRRERERERESlLQKCIiIiIiIiUpaBQREREREZGSFDSKiIiIiIhISQoaRUREREREpCQFjSIiIiIiIlKSgkYREREREREpSUGjiIiIiIiIlKSgUUREREREREpS0CgiIiIiIiIlKWgUERERERGRkhQ0ioiIiIiISEkKGkVERERERKQkBY0iIiIiIiJSkoJGERERERERKUlBo4iIiIiIiJSkoFFERERERERKUtAoIiIiIiIiJSloFBERERERkZIUNIqIiIiIiEhJChpFRERERESkpLoGjWZ2kZndY2b3m9m7iizvNLMrwvKbzWxjZNm7w+v3mNmFkdc/Y2a7zOy3BdtabWbfN7P7wr+r6vnZREREREREjgZ1CxrNrBX4N+C5wBbg5Wa2pWC11wL7nXMnAR8FPhjeuwW4BDgNuAj4eNgewGfDa4XeBfzQObcZ+GF4LiIiIiIiIjWoZ0vjWcD9zrkHnHOTwFeAFxWs8yLgc+H/XwMuMDMLr3/FOTfhnHsQuD9sD+fcj4F9RfYX3dbngBfH+WGqccP9exjJ5BpdDJFlbcf+g/xie7GfBBGJy8ys41u/TjI76xpdFJFl7Rfb97Fj/8FGF0PkCPUMGjcAj0Se7wivFV3HOTcNZICBMt9baNg5lwr/HwGGi61kZm8ws1vM7Jbdu3eX8zmqMjvreM3nfsE/fv+euu1DROCff3Aff3L5rY0uhsiydsO2PfzZl2/jpgf3NrooIsvaGz5/C//2o22NLobIEZZlIhznnAOK3g51zn3CObfVObd1aGiobmXYOz5JbmqWXz60v277EBHYsT/L/vFJfLUXkXrYsT8LwO6xiQaXRGT5Gp+YZv/BKdIHJxtdFJEj1DNofBQ4NvL8mPBa0XXMrA3oB/aW+d5CO81sXdjWOmBX1SWPQSrjT7Dbdo+r8ovUUSqTZXrWkZ2aaXRRRJatVNqf0/Ye0PlMpF7y146juakGl0TkSPUMGn8BbDazTWbWgU9sc1XBOlcBrwr/vxi4NrQSXgVcEiiiaYQAACAASURBVLKrbgI2Az9fYH/Rbb0KuDKGz1C1ZPrQWMbbHk43sCQiy5dzjlQYNzyanW5waUSWr2SoZ3vH1dIoUi/5a0edz6QZ1S1oDGMU3wx8F7gL+Kpz7g4ze5+ZvTCs9mlgwMzuB95GyHjqnLsD+CpwJ/Ad4E+dczMAZvZl4EbgZDPbYWavDdv6e+DZZnYf8KzwvGHyd4sAbn1YXVRF6mHf+CQT07OA7syK1FP+nLZvXC2NIvWilkZpZm313Lhz7tvAtwtee2/k/zng90q89/3A+4u8/vIS6+8FLqilvHFKZXJ0tLVw4lCvgkaROklFshOPZnWSFamXVGgBUfdUkfo51HNG5zNpPssyEU4zSKazrOvvYuvxq/jVw2lmlKZcJHbJ9KEWfd2ZFakP5xzJ0AKyVy2NInWTvzkzmptWcjdpOgoa62Qkk2NdfxdnHL+S8ckZ7hkZa3SRRJadkdFoS6PGgIjUQyY7RW7KdwNX91SR+snfnJmZdRycVHI3aS4KGusklcmxvr+bJx23GtC4RpF6iCacUkujSH3k69mavk72HlAiHJF6iQ65GMvpRqg0FwWNdTAz6xgZzbFuZRfHru5msLdT8zWK1EEqk2U40QloDIhIveSTczx2Qz+juWkmQ/IpEYmPc45UOsuavnBO041QaTIKGutg99gEM7OOdf3dmBlbj1/FLQ/ta3SxRJadVDrH8QMr6GxrYVR3ZUXqIj/dxmPXJwDYr7mHRWI3NjHN+OQMJ6/tA3QjVJqPgsY6yPdJX9ffBcCTjl/FI/uy7IqMvxKR2iUzWdb3d5HobtcJVqROUuksbS3GyWt90KgMqiLxyyfBOSUfNKqlUZqMgsY6yFf8df3dADxp4yoAdVEVidHsrGPnaI61/d30d7frBCtSJ6lMjuFEF4O9HYCS4YjUQ77B4THD+ZZG9Z6R5qKgsQ7y4z/Wr/QtjY9d309HW4uCRpEY7TkwwdSMY/3KLhJdbTrBitRJfgqpgV4/1mrvuJLhiMQt3+BwsloapUkpaKyDZDpHd3sr/d3tAHS0tfCEY/q5RUGjSGzy46zW9Xf77qk6wYrURSqTY93KbgZW+JZGdU8ViV8qk6XF4KQ1vYCyp0rzUdBYByOjWdat7MLM5l7bunE1dyQzZDXvjkgsRiJjhxNdGtMoUg/O+Wzg6/u76O9up7XF1D1VpA7y3cB7Otp8cjed06TJKGisg2Taz9EYdebGVUzNOH69I92gUoksL/m549av7CbR3absqSJ1sHd8ksnpWdb1d9HSYqzqaVf3VJE6SGWycwkU1XtGmpGCxjpIZbKsDRU/70nHrQbglu2aekMkDqlMls62Flb1tM+1NDrnGl0skWUlP85qbbgROrCiU91TReoglc7NJVDUOH1pRgoaYzY1M8uusQnWFwSN/T3tnDzcx8+3a1yjSBySmRzr+n038ER3O9OzjuyUun+LxClZkNht9YoOdU8ViZlzjqRaGqXJKWiM2c7RHM7BupXdRyzbunEVtz60n5lZtYaI1CqVzkbuyvqkU7ozKxKvVDo/dtjXtdW9HexV0CgSq/TBKXJTs3PXjhqnL81IQWPMRuYyOnYdsezMjas5MDHN3SOji10skWVnJJNj3cr8Xdk2QCnKReKWGs3R0doylzl1cEUHew5oTKNInOZa9MO1Y19Xm7KnStNR0Biz/DQA60u0NALcoi6qIjWZmXXsHJuYSzh1qKVRQaNInFLpHGtDEhyAgd5OxnLTTEyrK7hIXPJjh+daGtU9VZqQgsaYHerKc2RL44aV3azr7+LnDyoZjkgtdo3lmJl1kZbGEDTqJCsSq2hGR4DB3k5AczWKxClV0NLou6dOK7mbNBUFjTFLZXL0dbbRF1o+osyMszat5ufb9+mHQKQGc9NtRDLNgcY0isQtmc4d1nNmsNd3U1UXVZH4pDI52ltt7qZMoruNyZlZJqZnG1wykUMUNMYsmc7OtX4Uc9am1ewem+ChvQcXsVQiy0v+rqxaGkXqZ2bWsXM0d3hLY5+/qFXQKBKfVCbHcOJQN3ANuZBmpKAxZiOjubn5rIo5a6Ofr1FdVEWqN5dwKuHrWt9cS6NOsCJx2XtggulZd1jQONSbDxrVPVUkLsn04d3AdSNUmlFbowuw3CTTObasS5RcftKaXlav6ODn2/fxsjOPXcSSiSwfyXSOno7WuaypnW2tdLW38IWbHuLau3dVtK3HH7OSS194Wj2KKbKkJeeygR+6ETqg7qkisUtlcjzx2JVzz/NDLjIaciFNRC2NMZqYnmHPgYnDTrCFzIwzN65SS6NIDfLJOcxs7rXXnXcCjxnuY0VnW9mP3Qcm+NLND2uMsUgRc4ndIkMuejra6OloZc+YWhpF4jA76w6bQgoiLY3qPSNNRC2NMdqZ8Xde5xvTCH6+xu/esTNc+JYOMEWkuGQmd8S0Nm+/8OSKt/Px6+7nQ9+5h4npWbraW+MqnsiyMDeFVMF5arC3Uy2NIjHZOz7J5MzsYfVsVY9v0d9/UDdnpHmopTFGhyZnnT8QfPKmAUDjGkWqlSoY/1EtJRsQKS2VztLV3sLKnsOzgQ/2dihoFInJXGK3yDltVahz+w/q3CTNQ0FjjOaScyzQ0rhlfYK+zjZuekBBo0ilpmZm2b1AN/ByKdmASGmp0Rzr+7sP6wYOvqVR8zSKxCOVb9GP9J5JdLXTYpBWS6M0EQWNMUoWuVtUTGuLceam1dz84N7FKJbIsrJzNIdzsH6BmzPlULIBkdJSJaaQGlD3VJHYzI0djlw7trQY/d3t6p4qTUVBY4xS6Rz93e30dCw8VPTJm1bzwO5xdo3lFqFkIstH/q7sfFPblEstjSKlpTI51iaOrGdDvR3sOzjJ9IwmHhepVSqTo6OthdUrOg57fVVPh7qnSlNR0BijfEbHcjz5BD+u8WZ1URWpSDKdHzusMY0i9TI9M8vO0VzRFv3Bvk6cg31qBRGpWTKTOyIbOMDKnnZ1T5WmoqAxRsn0kRkdS3ns+gQrOlrVRVWkQqm5scNxtDT6XgGjOXVPFYnaNTbBrKPo2OHB3k4ATbshEoNSid1W9XSwf1w3NKV5KGiMUSUtjW2tLWzduFrJcEQqlEpn6etqo7ez9hmD1NIoUtxcRsdiLY35oFHjGkVqlsrkimbdX9nToZZGaSoKGmOSnZxh/8GpslsaAc4+YYD7dx1g95hOvCLlSpY4wVajq72VjrYWjWkUKZBMF5+jEWCg14+92juuc5dILWZmHSOjuaI3Z1b1tGtMozQVBY0xKTbPzkLOPmE1ADc9oC6qIuVKZbKsjWE8Y16iq51RZU8VOcx8U0ipe6pIPPYcmGBm1hXtBr5qRQfZqRlyUzMNKJnIkRQ0xmTuBFtBC8jjNvTT29nGjQoaRco2kimenKNaie42tTSKFEhmsqzoaKWvSDfwRFcbHa0t6p4qUqNkkek28lb2+OETabU2SpNQ0BiT5NzkrOVfzLa1tnDmxlXctE1Bo0g5JqZn2HNgsqKbMwvxLY06KYtEpdI51q3sPiKjI4CZMdTXqaEVIjVKzdPgsKrHdwPfN64WfWkOChpjkp+cdThRWQvIU04c4IE94+wc1XyNIgs51KIfZ0tju7KnihRYKLHbYF8nu9XSKFKTuSmkijQ4HGppVNAozUFBY0ySmRwDKzroam+t6H1POWEQ0LhGkXLMJeeIYbqNvP7udsbU0ihymIUSTg31qqVRpFapTI7u9lb6u9uPWLZ6hW9pVDIcaRYKGmOSymSLJgxYyJb1CRJdbdxwv4JGkYXkE07FmwhHYxpFoianZ9lzYGLec9qahIJGkVrlrx2LdQPPd0/dr5ZGaRIKGmOSSueqGmfV2mI8+YQBbnhgTx1KJbK85Md/xDXlBoTuqdlpnHOxbVNkKds5msO5+buBD/V2snd8kqmZ2UUsmcjykkznStYzdU+VZqOgMSbJTJb1VbZ+nHviAI/sy/LIvoMxl0pkeUmms6zsaae7o7Ju4PNJdLUzOTPLxLQufkVg/uQceWsSftqNvQd0QStSrZFM6QaHzrZWejpa1T1VmoaCxhgcmJhmLDfNuirHWZ1zkh/XeMM2tTaKzCc1zwm2WoluP6VARuMaRYBD3cDnywY+FOZqVBdVkepMz8yyayw3b4PDqp4OdU+VpqGgMQYjmdLz7JRj85pehvo6+ZnGNYrMK5WZ/wRbjUSX7wKkaTdEvHzCqflu0Az1+aBx15gyf4tUY+fYBLOOeRscVva0a55GaRoKGmNQzgl2PmbGOScOcMO2vRpXJTKPahNOzScRstYpGY6Il8pkSXS1saKzreQ6a8L0UmppFKlOfqq2+Roc1NIozURBYwxSNbY0Apxz4gB7Dkxw784DcRVLZFnJTs6QPjgVf/fULn9hPJrVXI0i4G+ELjStzWCvz+y4S0GjSFWSmYWnkFJLozQTBY0xSKZzmNU2DcC5YVzjz+7XuEaRYpIx3JwpRi2NIodLZbIL1rPOtlZW9rSrpVGkSuW2NO4bV0ujNAcFjTFIZbIM9XbS3lr913nMqh42DvQoaBQpIVVjN/BSNKZR5HCpTI61ZdSzoV7N1ShSrVQmR29nG33hHFTMQG8HmeyUpraRpqCgMQapTK7qzKlR5540yE0P7NWPg0gRyTIyOlajL989NafuqSK5qRn2jU+WlXBqqK9TiXBEqpRML9yiPxCyFO9Xa6M0gdKj3KVsyXSWzWv6at7OeScNcvnND/OrR9KcuXF1DCUTWT7yLY21dAMvpqu9lc62FnaO5thzoLJWk97ONrra45szUqTR5uZoLONG6Jq+Tn758P56F0lkWSqnwWEojB3efWBiLvmUSKMoaKyRc45UJsfTHjNU87bOOXEQM/jpfXsUNIoUGBnNMtjbQWdb/EHa6hUdfP7Gh/j8jQ9V9L4NK7v52bueGXt5RBplbo7GMlsad49N4JzDzOpdNJFlJZXJcdr6xLzr5Fsa9x5QS6M0noLGGo3mpjk4OcP6GMZZ9fe08/gN/fz0/j289dmPiaF0IstHMp2LfTxj3r++/HTuSo1W9J6f3LeH7925k9zUjFobZdmYGztcVktjF7mpWcYmpufGBovIwiamZ9hzYGLBc9pgCBor7QUjUg8KGms0N91GTOOszts8yH9c/wBjual5B0eLHG1SmSwbB1bUZdtbN65ma6Wt+2Z8786djOamFDTKslHJFFJDff6CdvfYhIJGkQrszPggcKFrx4HQPVUtjdIM6poIx8wuMrN7zOx+M3tXkeWdZnZFWH6zmW2MLHt3eP0eM7twoW2a2QVmdquZ/crMfmpmJ9Xzs+XFndHxqZuHmJl13LBtbyzbE1kuUulc7NNt1ELzO8pylMzkWNXTXtaNkHzQuGtUrSAilSh3Cqm+zjY62lrU0ihNoW5Bo5m1Av8GPBfYArzczLYUrPZaYL9z7iTgo8AHw3u3AJcApwEXAR83s9YFtvnvwCucc08EvgS8p16fLSruuePOOG4VPR2t/OS+3bFsT2Q5GMtNMTYxHUuW4rhofkdZjlLpbNk3QYcTIWhUBlWRihxq0Z+/rpkZgys62KOWRmkC9WxpPAu43zn3gHNuEvgK8KKCdV4EfC78/2vABeZH078I+IpzbsI59yBwf9jefNt0QH5EcT+QrNPnOkwqnaPFfBa5OHS0tfCUEwb4yX2ar1Ekby6jY1O1NGp+R1l+Uplc2dPa5LM5qqVRpDLJ0EutnLo22NeplkZpCvUMGjcAj0Se7wivFV3HOTcNZICBed473zZfB3zbzHYA/wv4+1g+xQKSmSzDiS7aWuP7Kp/2mCEe2nuQh/aOx7ZNkaUsmc7P0dg8LY393ZrfUZafZAUtjX2dbfR0tDIyqpZGkUqkMln6u9vp6Vg4tcjAig72jitolMar65jGRfZW4Hecc8cA/wn8Y7GVzOwNZnaLmd2ye3ftXUDrMc7qqZsHAdTaKBKopVGk/sYnphnNTZed2M3MGE50sVNBo0hFRjLlXzsO9nayZ0zdU6Xx6hk0PgocG3l+THit6Dpm1obvVrp3nvcWfd3MhoAnOOduDq9fAZxTrFDOuU8457Y657YODdU+t+LI6MKTs1Zq0+AKNqzs5vp7Na5RBHzQaAbDTTS5scY0ynKTvzlTyRRSa/o61T1VpELJdK7snjMDvZ3sHffzoYo0Uj2Dxl8Am81sk5l14BPbXFWwzlXAq8L/Lwaudb5WXAVcErKrbgI2Az+fZ5v7gX4zy09u+Gzgrjp+NgCccyTT2bImQa6EmfH0k4e4cdteJqdnY922yFKUSmdZ09dJe4zdwGvV2dZCR2uLsqfKslHJdBt5w4kudioRjkhFUplsBS2NHUzNOJ1rpOHqdgUWxii+GfguPoD7qnPuDjN7n5m9MKz2aWDAzO4H3ga8K7z3DuCrwJ3Ad4A/dc7NlNpmeP31wNfN7Nf4MY3vqNdny9t/cIqJ6VnW1mHC8ac/ZogDE9Pc+vD+2LctstSkMrm61LNamBmJ7ja1NMqyUc0UUsOJTnaO5tQKIlKm7OQM+w9OVdQ9FWCPxjVKgy08ArcGzrlvA98ueO29kf/ngN8r8d73A+8vZ5vh9W8C36yxyBWZS85Rh3FW55w4QFuLcf29uzn7hIHYty+ylCQzWU4e7mt0MY6Q6GrXmEZZNvJTSA33l58NfDjRRW5qltHcNP2hy7aIlFbudBt5c0Hj2AQnDvXWrVwiC2mevl5L0FxyjjpkdOzraudJx6/ixxrXKEc551xIONVcLY0Afd3typ4qy8ZIJsdgbyedba1lvyc/zljJcETKMzJ37Vheg8NAbwcAe8eVDEcaS0FjDfJ3i+rR0gjw9JOHuCM5qomT5aiWyU6RnZope+64xZToalNLoywbyQrmaMxT0ChSmWSFCafmWho1V6M0mILGGiTTOdpbba5Cx+3pj/HZXa+/R62NcvRKVjHOarEkuts1plGWjVS6/OQcecMJf/7bqQyqImVJhaFNa8usa6t62jGDPQfU0iiNpaCxBiOZLMOJLlparC7b37IuwZq+Tq5TF1U5io2MhvEfTdnS2K6MdrJspDKVdwNf06eWRpFKJDM5BlZ00NVeXjfwttYWVvd0qKVRGk5BYw2SmVxF81lVysw4/+QhfnLvbqZnNPWGHJ3yLY31rGvVUvZUWS5Gc1McmJiuuHtqd0cria42diloFClLKpOt+CbokOZDlSagoLEGqUy27O4F1XrGyWsYzU1z2yPpuu5HpFmlMllaW4yhvvp0A69FoqudyelZclMzjS6KSE3yyTmqmdpmONGl7qkiZRrJ5FibqKyeDSe6lN9CGk5BY5VmZx0jmVzdu8ydu3mQthbjR3fvqut+RJpVKp1juK+T1jp1A69FIkwxoNZGWepqmUJqONHFiFoaRcqSTGcrbtFfm+iau7Ej0igKGqu0Z3yCqRlX9y5ziTD1xo+UDEeOUslMti7T2sQh0eWnutW4RlnqaplCyrc06oJWZCHjE9OM5qYrHjs8nOhkz4EJDVWShlLQWKXUXEbH+ifneMYpa7grNTo3xYfI0cQn52i+JDiglkZZPlLpLC0Gw1V0A1/b38nusQlmZl0dSiayfMxN1VZhS+OaRBezTnM1SmMpaKzSoYpf/xaQC05ZA8CP7lZroxxdnHOkMrlFqWfVSHSFoFFzNcoSl8zkWNPXRVtr5ZcFa/u7mZ517FV2R5F5VTuFVH4+VHVRlUZS0Filua48i9ACctKaXo5d3c21d++s+75Emsm+8Ukmp2ebtqWxvzt0T82pe6osbdVkdMxbFy5oU7qgFZlXvsGh0nPa2oSmtpHGU9BYpVQmR2dbC6tXdNR9X2bGM09ew0/v36MsjXJUOXRzRi2NIvWUqmEKqXwWcQWNIvNLZXKYUXHm/eGE7za+c0yt+dI4ChqrlExnWdffhdniZHR85qnD5KZmuXHb3kXZn0gzmMvoWOcsxdXSmEZZDpxzpNLVjx3Ov29E4+5F5pVK5xjq7aS9wm7gA72dtBiaD1UaSkFjlVKZXN3naIx68qbV9HS08kN1UZWjSGpu7rjmDBo721roaG1R9lRZ0jLZKbJTM1XXs9UrOuhobSGlC1qReSUz2apuzuTnKtaYRmmktkYXYKlKpbOcfcLAou2vq72Vp24e5Id37eJvXuQWrYVTpJGSmSztrcbgisozOi4GMyPR3catD+/n8psfqui9x67q4WmPGapTyUTKl0/OUW3CKTNjbb/mkRNZSCqT46Sh3qreuzbRpe6p0lAKGqswM+vYOTZRddKAal1w6jDfvWMndyRHeeyG/kXdt0gjjIQW/ZaW5r1JsmlwBT9/cB8/f3BfRe9rbTHufN+FdLa11qlkIuWpNjlH1Nr+Lo1pFJmH7wae5ambB6t6/5pEF4/sOxhzqUTKp6CxCvn5qBY7OcczT1mDGfzwrl0KGuWo4MdZNWcSnLzLX3c26YOVzZ31tVt38KHv3MNodpqhPgWN0ljJTG0tjeADztseTsdVJJFlZzQ3zfjkTNUJp4YTnfxie2U3J0XipKCxCskqJ2et1WBvJ6cfu5If3LWTv3jW5kXdt0gjJDNZth6/qtHFmFdHWwtrEpX9FmwIF+ejuSmGqphMXSROqXSWthZjsLf6YzHfPdU5DZ8QKWauRb/Ka8fhvi7SB6fITc3Q1a6bjbL4lAinCqkqJ2eNw7O2DPObRzNzPz4iy9XsrGPnaI51NbR+NCtN1SHNJJXJMZzoorWGbuDrEl1Mzsyyb7yyVneRo0Wt147Dofv4bo1rlAZR0FiFfMBWbReDWjz71GHAd1EVWc72HJhgasaxvkkzp9Yi0e07eYzmlHVVGi+Vydbcc2ZtOB9qXKNIcam5buBVtjSGHi0jylIsDaKgsQqpTI6ejta5C7/FdNKaXjYO9PC9OzX1hixv+RNss49prIZaGqWZpDK1jx0+NFejLmhFikllsrS2GGv6apsPVTdmpFEUNFYhlcmytr+rIeM2zIznnLaWG7ftYUwTissylm/Rb9Y5GmuR6A5Bo+qwNJhzLgSNtdWzuQtatYKIFJVM51jT11l1N/B8HUumNTxJGkNBYxWS6VxDuqbmPWfLMFMzjuvu2d2wMojUW61zxzWzQy2N6p4qjbV3fJLJ6dmag8aB3k7aWowRjbcXKSqVydZUz/q62kl0tfHoftUxaQwFjVWoteLX6vTjVjGwokNdVGVZS2WydLa1sKqnvdFFiV1XewvtraaWRmm4ueQcNd6caW0xhhNdc9sTkcOlMrUndtuwqkctjdIwChorNDUzy+6xiYZmdGxtMZ516jDX3b2LyenZhpVDpJ6SmRzrV3Yvy/T9Zkaiq11jGqXhkjEmdlu/sotHdUErcgTfDTxbc2K3Dapj0kAKGiu0czTHrKPhGR0vfOwwYxPT3LBtT0PLIVIvqXRjW/TrLdHdruyp0nCpdG1zx0VtWNk9F4SKyCH7D06Rm5qtOeHU+pXdammUhlHQWKF8ZrhGzx13zomD9Ha28d07RhpaDpF6GYkho2MzS3S1qaVRGi41mqOjtYWBFR01b2v9ym5GMjlmZl0MJRNZPuamaqvx5sz6ld2M5qaVCFEaQkFjhZJz0wA0tgWkq72V808e4nt37NQJWpadmVnHzrGJhtezevItjTrxS2Ol0rnYsoGvX9nN1IxjzwFNPi4SlR/ru7bGG6EbQoNFUmOHpQEUNFZoritPE1zMXvTYtewdn+SW7fsaXRSRWO0a860VcXSZa1Ya0yjNIM7EbvkLWo25EjncXEtjjXVt/VzQqDomi09BY4VSmRx9nW30dTU+o+P5J6+ho62F//mtuqjK8jI33cZy7p7a3aYxjdJwyXQutmltdEErUlwyk6O91Rjs7axpO7oxI42koLFCyXS2aVo/ejvbeNrmIb57xwiz6qIqy0j+rmyz1LV6UEujNNrMrGPnaC62lsb8eC0FjSKHS6WzDCe6aGmprRv4mr5O2ltNQaM0hILGCqWaLDnH7zxuLalMjl/vSDe6KCKxmZs7ronqWtwS3e1MTM+Sm5ppdFHkKLXnwATTsy62xG59Xe30dbVpvJVIgWQmF0vPmZYWY21/l27MSEMoaKxQnOM/4nDBqcO0t5q6qMqyksxk6eloJdHV1uii1E3+s42pi6o0SP7Cc10ivnPahpXdagURKZDKZFkbV4t+v6bdkMZQ0FiBiekZ9hyYbKrWj/7uds47aZBv/yaFc+qiKstDKu27zMWR0bFZJbr9uGhlUJVGSc1NIRVf0Lh+ZTeP7tcFrUje7KzzU0jFVM82qI5JgyhorMDOjE8j3mzjrJ77uHXs2J/lt4+ONrooIrFIjcaXnKNZJUIyLY1rlEbJB41xJpxav7KLZEYXtCJ5e8cnmZpxsdWzY1Z1MzKaY3J6NpbtiZRLQWMFknMpk5vrYvY5W4ZpazGu+U2q0UURiUUq3VzdwOsh0e27pyqDqjRKKp2lq72FlT3xZQNfv7Kb9MEpxid0XItAJLFbTOe0Y1f3MOuUQVUW34JBo5k9xsx+aGa/Dc8fb2bvqX/Rmk+zZnRc2dPBuScNcs1vkuqiKkve5PQsuw9MNFU38HpQS6M0Wiok54izG3h+SoCUWhtFgMgUUjH1njl+YAUAD+87GMv2RMpVTkvjJ4F3A1MAzrnbgUvqWahmlZzL6NhcQSPA8x6/jkf2ZfnNo5lGF0WkJjtHczh3KH3/cqUxjdJoyUz8U0jlg8YdGnMlAsTf0nj8QA8AD+8dj2V7IuUqJ2jscc79vOC1o7LfSSqTpb+7nZ6O5svoeOGWtbS3Gtfcri6qsrTlx1mtPWpaGo/Kn1NpAql0jrWJeOvZMav8Ba2CRhEvlcnR0dbC6hUdsWxvqLeTzrYWHtqrlkZZXOUEjXvM7ETAAZjZxcBRGZnkMzo2o/4en0X16tuVRVWWttTc2OHmrGtx6Wpvob3V1NIoDTE9M8uusVzsLfr5yccVNIp4yTBGP65u4C0tZoouRAAAIABJREFUxnGre9Q9VRZdOUHjnwKXAaeY2aPAW4A31bVUTSqZae6Mjs9//HoeTWe57ZF0o4siUrW5buBNXNfiYGYkuto1plEaYufYBLOO2McOt7QYG1Z2s2O/LmhFwLc0xt3goKBRGqGcfpbOOfcsM1sBtDjnxsxsU70L1oxSmSxnHLey0cUo6dmnDdPxzRa+9eskZxy3qtHFEalKKpOlr6uN3s7m6wYet0R3O9+/c2fFrTLHrOrmb1/82GU9j6XUVypdv8Rux6zq4RG1NIoAvq6dfcJArNs8bqCHGx/Yi3NO5wFZNOW0NH4dwDk37pwbC699rX5Fak7ZyRnSB6eauqUx0dXOM04e4urbU8zMqouqLE35jI5Hg5eesYF1K7tJZ6fKfmzbfYDLb36YfeOTjS6+LGH1mKMx79jV3TyqlkYRZmYdO8cmYr85c/zqHg5OzrDngM4DsnhK3so3s1OA04B+M3tJZFECWN6DjYqIO/tVvbzwCRv47h07ufnBvZxz4mCjiyNSsVQdMjo2qzc/czNvfubmit7zzdt28NYrfs1obpqB3s46lUyWu3pOIXXMqh72HJgkOzlDd0dr7NsXWSp2j00wM+ti7wZ+XD6D6r5xhvp0HpDFMV9L48nA84GVwAsijzOA19e/aM3lUEbH5r6YfeYpa+jpaOVbv042uigiVWnmhFPNQPM7ShyS6RwrOlrpq0M38GNW5afdUGujHN2SdWpwOG61n6tRGVRlMZU8WzjnrgSuNLOnOOduXMQyNaVkOp/Rsbm7zXV3tPKcLcN8+zcjXPrC0+hs011eWTpyUzPsHZ+M/a7scqL5HSUOvkW/uy7joaLTbmwe7ot9+yJLRWpufu+4p7bpxgwlw5FFVc6YxtvM7E/N7ONm9pn8o+4lazJLpaUR4EVP3EAmO8X19+xudFFEKjKSyZ9gm7+eNYrmd5Q41COjY96xamkUASJTSMXcDbyrvZV1iS61NMqiKido/AKwFrgQuB44Bhib9x3LUCqTZWBFB13tzd9yd97mQVav6OBKdVGVJSY5d4JVS2MpiW7fQUQtjVKLZLp+CacGezvpaGtRBlU56iXTObrbW+kPPUTitGloBQ/sPhD7dkVKKSdoPMk595fAuHPuc8DzgCfXt1jNJ5nOLZnkHO2tLTzvcev4wZ07GdOFpSwhh7ryLI261gga0yi1mpieYc+B+DM65rW0GMes0lyNIvnEbvXoBn7CYC8P7B7HOWXLl8VRTtCYvzJJm9ljgX5gTf2K1JxGMrklNc7qxaevZ2J6lu/esbPRRREp28hofcZ/LCc9Ha20tphaGqVqu0YngPqO0T9mVU/F84+KLDf1nELqxKEVjE1Ms/vARF22L1KonKDxE2a2CngPcBVwJ/DBupaqCSUzWdYvodaPM45bxXGre/jmbTsaXRSRsiXTWVb1tCtN/zzMjERXm8Y0StXyid3q2XvmmFXdPKIkHXKUS2Wydes5c8JQLwDbdo3XZfsihRYMGp1zn3LO7XfO/dg5d4Jzbg3wP+Vs3MwuMrN7zOx+M3tXkeWdZnZFWH6zmW2MLHt3eP0eM7twoW2a934zu9fM7jKzPy+njOU4MDHNWG6atUuo9cPMePHpG7hh29655CIizS6VyS2petYoie52tTRK1VKLkHDquNU97D84peNUjlpTM7PsGpuoY9Dop914YI/GNcrimDdoNLOnmNnFZrYmPH+8mX0J+NlCGzazVuDfgOcCW4CXm9mWgtVeC+x3zp0EfJTQghnWuwQ4DbgI+LiZtS6wzVcDxwKnOOdOBb6yUBnLlUrXJ/tVvf3u6RtwDq781aONLopIWZLppdWi3yiJrnaNaZSqHZo7rn43aI5fHSYfV3ZHOUrtHM3hHKyrU2K39f3ddLW38MButTTK4igZNJrZh4HPAC8FrjGzvwW+B9wMbC5j22cB9zvnHnDOTeKDuBcVrPMi4HPh/18DLjA/WvhFwFeccxPOuQeB+8P25tvmHwPvc87NAjjndpVRxrIkM0tznNWmwRU88diVfPO2RzVQWpaEVGbpJJxqpER3G6M5dU+V6qTSORJdbazoLDlVc82OzQeN6qIqR6l6t+i3tBibBnuVQVUWzXwtjc8DTnfOvRx4DvAW4Gzn3D8758rp77gBeCTyfEd4reg6zrlpIAMMzPPe+bZ5IvD7ZnaLmf2PmRUNbM3sDWGdW3bvLm8ew3xL41LM6PjSMzZw98gYdyRHG10UkXkdnJwmk51acjdnGqG/Wy2NUr1UJlv3aW2OH1DQKEe3ZLr+U0idMLSCbWpplEUyX9CYyweHzrn9wH3Oue2LUqrqdOLLvBX4JL6V9AjOuU8457Y657YODQ2VteFkJocZrF2CQeMLnrCejtYWvn6rEuJIc0uG6TaWWjfwRkh0aUyjVC+ZztX9JmhfVzurV3Ro8nE5ai3G2OETB1ewY/9BJqZn6rYPkbz5gsYTzOyq/APYVPB8IY/ixxjmHRNeK7qOmbXhp/PYO89759vmDuAb4f/fBB5fRhnLkkpnGertpL21nGSzzWVlTwfP2rKGq36VZGpmttHFESlpZIl2A2+ERHe7sqdK1UZGc3UbZxV13OoeHt6nVhA5Oo1kcvR1ttEX5tathxPX9DLr0M0ZWRTzDWgoHH/4DxVu+xfAZjPbhA/sLgH+oGCdq4BXATcCFwPXOudcCEq/ZGb/CKzHj6H8OWDzbPO/gWcADwJPB+6tsLwlLdYJtl5eesYxfPs3I1x3z26e/f/bu+/4qsu7/+OvzxnJyTrZg0DC3iBbEURRWveoVq2rjjpq1dbeXbd22fqrve/WtraOauuo1mrdWtxWxYEKCspGIGxI2JAwkkCS6/dHDqPcBBLJOd+Tc97Px+M8SE7O+Z73OeTKOZ/vtQYUex1H5IB2L84Rzb3jEkU4FKB2VyM7G5pICXS8k1ninbpdjWzavjMmC06V56Xz2crNUX8ckXhUuaU26nP0e0a23ahYt40+xVlRfSyRFotG59y7h3Ng51yDmd0AvA74gYecc3PN7FZgmnNuIvAg8KiZVQCbaC4CidzuKZr3hGwArnfONQIc6JiRh/xf4DEz+y9gG3DV4eTfV+WW2g7dGI/tU0hBZirPTF+polHiVlVkeGpxdqrHSeJfOK35zHVN3S4KMvV6SetVxbBHv2t+Oi/PrmJXY1OHHKkjcjiqquui3s56FmZiBgvXbuXUwZ2i+lgi0Vs6DXDOvQK8st91P9/n6zrgvBbuextwW2uOGbl+C82L97Qr5xxV1XUc26d18x/jUdDv45zhnXlo8lI2bqsnXx8yJQ5VVddSkJlCasDvdZS4F44Md6qpVdEobRPLhd3K89JpbHKs3lxLt4KMqD+eSDypqq5lYGk4qo+RluKna146C9dujerjiMAh9mkUqKltYMfOxg4/ZO68EV1oaHI8/5n2bJT4VBmDs7KJIpzWfL5P225IW+3ZQipGcxpBK6hK8qlvaGTDtp0xeU/rW5LFgjUqGiX6VDQewp5NkDv4io69i7MYWpbDU9NWas9GiUtrqms75LY2Xti3p1GkLdZUx66nsWt+c+/ichWNkmTWVtcDsfns2Lc4i2Ubd1C3SyuoSnQdsmg0sxf3XTU1cnnUzG40s4T/hFe15w224/eAnD+yjIVrtzFrVbXXUUT+j6otdVHfOy5R7DunUaQtKqvryMtIIRSM/jDwoqxUUgM+VmzUCqqSXGK5sFufkiwamxyL12+L+mNJcmtNT+MSmheWuT9yqQG2An0i3ye0RNo77vQhnQgFfTw5baXXUUT+w9a6XWytb1BPYyvt7WnU8FRpm6otsevR9/mMrvnpLNN2AJJkqmI4Sq1vZKFGzWuUaGvNQjhjnHOj9vn+RTP7xDk3yszmtnivBFFVXYvfZxRldfwPs+FQkNMGlzJxRiU/ObU/GalRXQdJpNWqYjjPKhHsndOonkZpm6rqOrrkpsfs8brlZ7B0g3oaJbns6XCIQU9jt4IMgn5jwRr1NEp0taanMdPMynd/E/k6M/LtzqikiiNV1XUUZ6Xi95nXUdrFBUeWsa2+gZdnV3kdRWSPyi27h/J0/JMzsZAW9BPwmeY0SptVbqmN6ciZ7gUZLN+4g8YmzaWX5FFVXUtOepC0lOgPAw/6ffQszFRPo0Rda4rG7wOTzWySmb0DvA/8wMwygEeiGS4eVG2pS6jej5Fdc+lZmMETH6/wOorIHmvU09gmZkY4LaieRmmT7fUN1NQ1xHSOfveCDHY2Nu05MSSSDNbEeDXwPsVaQVWi75BFY2RfxN7Ad4Ebgb7OuZedc9udc3+MdkCvVVXXUpJAvR9mxgWjyvl0xRb9gZG4UVldh1nzwhnSOuFQgM07dlG3q7FNl4bGJq+ji0f2DAOP4Xva7v0Zl2kxHEkilVvqYtrO+nXKYvWWWqp36ESiRE9rJ7WNALpFbj/EzHDO/T1qqeKEc46q6jq+PKDY6yjt6qsjunD76wt4fOpyfnnWIK/jiFC1pZairFSCfu0C1Fo56Sm8PKuKl2e1bah5XkYKk//7eNJTNKc52VTFcLuN3bpHisalG7YzrndhzB5XxEtV1bUMK8+J2eMNLM0GYG5VNWN6FsTscSW5HPJTg5k9CvQEZgC7N4FxQMIXjZu276S+oSkhttvYV15GCicPKuG5z1Zz0yn9YzLmXuRgqmI8lCcR/PyMAUxZsrFN95lXWcNLs6pYV1NPtwIVjcmmas9q4LFra0VZqaSn+LUYjiSN2p2NbN6xK6btbGBpGGj+G6+iUaKlNZ8aRgIDXBLuCL97KE8ibLexv4uPKmfizEpenFXJ+SPLvI4jSa6yupZ+JVlex+hQhpfnMrw8t033eXPeWl6aVaW5kEmqsroWMygOx+49zczolp/BMhWNkiS86NEvyEylJBxizmrtwy3R05qxYHOAkmgHiUd7538kXg/Ikd3z6FWUyWNTtSCOeMs517zgVAK2s3gTTtP+jsmsaksdBZmppARiOwy8e4G23ZDk4dVnx4GlYeZW1sT0MSW5tOadowCYZ2avm9nE3ZdoB4sHsdycNdbMjIuPKmfmyi3MXqUzU+Kd6tpd1O5qjOlZ2WSl/R2TW2V1rSfb2nQvyGDl5lp2aREmSQJ7tpCK8WfHgZ2zWbx+G7U7Gw99Y5EvoDVF4y+ArwC/Bn6/zyXhVW6pI+g3CjISc0XHc4Z3IS3o5x9TlnsdRZLY7k2Q1dMYfeHQ7p5GFY3JqKq6zpPVwLsVZNDY5Fi1WdtuSOLb3dMYy2Hg0NzT2ORg/hr1Nkp0tGbLjXcPdIlFOK9VVddSHA7h85nXUaIiOy3IWUNL+dfM1VTrQ6R4ZE1N4vbox5s9w1PV05iUYr133G67V1Bdsn5bzB9bJNaqquvIz0ghFIztIoODOkdWUNUQVYmSFotGM5sc+XermdXsc9lqZknxG1m1pY7SBO/9uGR0V+p2NfH0tJVeR5EktbunMdHbWjzISPHjM81pTEY1dbvYVt/gycJuPQt3F42a1yiJr6q61pOToKXZIXLTg8zVYjgSJS0Wjc65YyL/Zjnnwvtcspxz4dhF9E6lRw0/lgZ1zmZ4eQ7/mLKcpqakWyBX4kBVdS0Bn1GYlZjDwOOJmRFOC6qnMQlVeTgMPCc9hYLMFBarp1GSgFcLu5kZgzpnM2Pllpg/tiSHVi2hZmZ+Mys1s/Ldl2gH81pTk2NtTXKs6HjZmG4s27iD9xat9zqKJKGqLXUUh0P4E3QYeLwJh4Ka05iEKqu9WZxjtx6FmVSsU9Eoic+rBacAhpXlsHDtVnbs1GgSaX+HLBrN7NvAWuDfwMuRy0tRzuW5Ddvr2dXoEnKPxv2dMqgTBZmp/P0jLYgjsVdZXauVU2MonBagpk4fKJKNlz2NAD0LM9XTKAlvW30DW+sa6JTjTTsbWp5Dk0Or4ktUtKan8Uagr3NuoHNucORyRLSDec3rN9hYSgn4uOiociYtWKcNmCXmqqrrPHuDTUbqaUxOVdW1+AyKPBoG3qsok807drFp+05PHl8kFqoi2214dSJ0SJccAA1RlahoTdG4Eki6UxZ79mhMkh6QS0aXE/AZD3+4zOsokkScc81FY5K0s3gQDmlOYzKq3FJHUVaIgL9Vs1La3e7FcDREVRJZZbW3HQ75mamU5aWpaJSoaM27xxLgHTO72cy+t/sS7WBe27t3XHJ8mC3KCnH6EaU8M30VW/WBUmJk4/ad7GxoSpp2Fg/CaQGtnpqEvFrRcbeehZkAGqIqCc3rnkaAoWW5KholKlpTNK6geT5jCpC1zyWhVVXXkhrwkZeR4nWUmLl8TDe21Tfw1LRVXkeRJJFMw8DjhXoak1NVtbdbSHXOSSM14GOxeholgVVW12EGJZ4WjTlUVdextqbOswySmAIH+6GZ+YE+zrmLY5QnblRGhsyZJc+KjkPKchjRNZeHP1zK5WO6aTVLiTqvV3RMRuG0IDt2NrKrsYmgR0MVJbacc1RuqWVCvyLPMvh8Rg8thiMJrmpLLYWZqZ7+bR1a1jyv8bMVmzl5UCfPckjiOehvtXOuEehqZsnT3RZx/fhe3Hb2YK9jxNxVx3Rn5aZa/j1vjddRJAkML8/l/ktH7hm6JtEXDjWfK9yqFVSThnPw4GWjOH9Umac5ehVlUqGiURLY1cf24HfnDfE0w6DOYVICPj5ZttnTHJJ4DtrTGLEE+MDMJgJ7ltZ0zv0haqniwIDSsNcRPHHiwBLK8tJ44P2lOkMlUVeYlcqXBxR7HSOphNOCANTU7kqq4ffJzOczjuld4HUMehVm8tKsSmp3NpKW4vc6jki761OcRZ9ib2dwpQb8DC3LYdqyTZ7mkMTTmv7zxTTvy+gjieY0Jiu/z7hiTHemLd/MZyt0lkok0YRDkaJR8xolxvoUZ+KcVlAVibZR3XKZU1nD9nqNKJH2c8ii0Tn3ywNdYhFOvHH+qDLCoQAPvL/U6ygi0s729jTqw4TEVu9ID8zCtVs9TiKS2EZ1y6OxyfHZCq2iKu3nkEWjmRWa2e1m9oqZvb37Eotw4o3M1AAXj+7Kq3OqWLFxh9dxRKQdhdOaZyWop1FirVt+Oil+n4pGkSgb0TUXn8EnGqIq7ag1w1MfAz4HugO/BJYBn0Qxk8SB3aunPjB5iddRRKQd7RmeWquiUWIr4PfRozBDRaNIlGWFgvQrCatolHbVmqIx3zn3ILDLOfeuc+4bwAlRziUeKw6H+MrQzjw1bSUbt9V7HUdE2sme4anqaRQP9CnOYuFazWkUibYju+fx2Yot7Gxo8jqKJIjWFI27P1lUmdlpZjYMyItiJokT3zyuB3W7mnjko+VeRxGRdpKR4sdnmtMo3uhbksXqLbVs0wIdIlE1ukc+tbsamblK8xqlfbSmaPyVmWUD3wd+ADwA/FdUU0lc6FWUxYkDinnkw2VagUskQZgZ4bSgehrFE72LmvdkXaQhqiJRdXSPfMzgg4oNXkeRBNGa1VNfcs5VO+fmOOeOd86NcM5NjEU48d6143tSXbuLf368wusoItJOwqGg5jSKJ3bvYbdIQ1RFoio7Pcig0mw+XLzR6yiSIAKHuoGZ9QHuBYqdc4PM7AjgTOfcr6KeTjw3vDyX0T3yuP/9JXz96K6kBrQhs0hHF04LULF+GxNnVrbpfvkZKYzt5f0m8dJxleWlEwr6WKCeRpGoG9Mrn4cmL2XHzgbSUw75kV/koFrzG3Q/8EPgLwDOuVlm9jigojFJXH98L77+4Mc8O301Fx1V7nUcETlMZbnpvDpnDd/552dtvu+HN51AaU5aFFJJMvD7jN5FWSxYo6JRJNrG9izgL+8u4ZNlmzmuT6HXcaSDa03RmO6c+9jM9r1OE9ySyDG9ChjSJZv73l3M+SO7EPC3ZiqsiMSrO742lO+fWNum+3y0ZCM/e2EOG7ftVNEoh6V/pyzemr8O5xz7fbYQkXY0slsuQb/xYcUGFY1y2FpTNG4ws56AAzCzc4GqqKaSuGJmXH98L655dDoTZ1ZyzvAuXkcSkcMQCvrpFVmQpLV2b72jBXTkcPXvFOapaatYv7WeonDI6zgiCSs9JcCIrrm8t2gDN3sdRjq81nQZXU/z0NR+ZrYa+C5wbVRTSdz5Uv9i+pVkcfekChqbnNdxRCTG9uzvqAV05DD17xQGYF5VjcdJRBLf+L5FzK+qYW1NnddRpINrzeqpS5xzXwIKgX7OuWOAs6OeTOKKz2fcOKE3S9Zv56VZbVs8Q0Q6vj1Fo3oa5TD1L2kuGudXaV6jSLTtHpb67oL1HieRjq7Vk9Occ9udc7v/wn8vSnkkjp00sIS+xVnc9bZ6G0WSTTjUPJuhplZT2uXwZKcHKc0OMV89jSJR168ki5JwiEkL1nkdRTq4L7qiiWauJyGfz/j2hF5UrNvGy7M1rVUkmWSkBPCZehqlffTvFFbRKBIDZsZxfQqZvGgDuxqbvI4jHdgXLRrVzZSkTh3Uib7FWfzpzYXqbRRJIj6fkRUKak6jtIv+ncIs2bCdul2NXkcRSXjj+xaytb6BT5dv9jqKdGAtFo1mttXMag5w2QqUxjCjxBGfz7jxS71ZvH47L7ZxY3AR6djCaQFq6jQ8VQ5fv05ZNDY5KtZt8zqKSMI7pncBQb/x5vy1XkeRDqzFotE5l+WcCx/gkuWca81WHZKgTh5YQr+SLP745kIaNNRBJGmE1dMo7WRAZAXVuZXVHicRSXxZoSBjehbwxry1OKdRYvLFaJd2aTOfz/j+iX1ZtnEHz366yus4IhIj4VBQcxqlXXTLzyAzNcCc1ZrXKBILJw4sZvnGHSxS7758QSoa5Qv5Uv8ihpTlcOdbFdQ3aE6KSDIIpwW0eqq0C5/PGFAaZvZq9TSKxMKX+hcD8MbcNR4nkY5KRaN8IWbGD0/sy+ottTw2ZYXXcUQkBtTTKO1pcOds5lfVaJqDSAwUh0MMLcvh3/M0r1G+GBWN8oWN7ZXPmJ753DOpgm316n0QSXThNM1plPYzuHM29Q1NVKzXcDmRWDhxYDEzV1WzavMOr6NIB6SiUb4wM+NHJ/dj4/adPPD+Eq/jiEiUhUNBtu9sVM+QtItBnZsXw9G8RpHYOH1w8+YHr2ivbfkColo0mtnJZrbAzCrM7KYD/DzVzJ6M/HyqmXXb52c3R65fYGYnteGYd5qZTlvGyNCyHE4ZVML97y1hw7Z6r+OISBSF05oXzt6qbTekHXQvyCQ9xc8czWsUiYny/HSO6JLNy7NUNErbRa1oNDM/cA9wCjAAuNDMBux3syuBzc65XsAdwG8i9x0AXAAMBE4G/mxm/kMd08xGArnRek5yYD84qS91DU3c9dYir6OISBSFQ0EAzWuUduH3GQM6hVU0isTQaYM7MXNVNSs2aoiqtE00exqPBCqcc0ucczuBJ4Cz9rvNWcAjka+fASaYmUWuf8I5V++cWwpURI7X4jEjBeXtwI+i+JzkAHoWZnLhkWU8NnUFSzds9zqOiERJOC1SNGoFVWkngzpnM7dSi+GIxMppR3QC4KXZlR4nkY4mmkVjZ2DlPt+vilx3wNs45xqAaiD/IPc92DFvACY65w7a525m15jZNDObtn79+jY9IWnZjRP6kBLw8dvXPvc6iohESTjUPDxVPY3SXoaW5VC7q1F7x4nESJfcdIaV5zBxhopGaZuEWAjHzEqB84C7DnVb59xfnXMjnXMjCwsLox8uSRRmpfLNY3vy6pw1fLJsk9dxRCQK9vY0qmiU9jGkLAeAmSu3eJxEJHmcM6wzn6/ZyrxKLUIlrRfNonE1ULbP910i1x3wNmYWALKBjQe5b0vXDwN6ARVmtgxIN7OK9noi0jpXH9udknCIX700j6Ym53UcEWlne4pG9TRKO+mWn052WpAZKhpFYub0I0oJ+o3nPl3ldRTpQKJZNH4C9Daz7maWQvPCNhP3u81E4LLI1+cCbzvnXOT6CyKrq3YHegMft3RM59zLzrkS51w351w3YEdkcR2JofSUAD84qS8zV1UzcaaGPYgkmj3DUzWnUdqJmTGkLEdFo0gM5WakcEK/Il6YUan5xNJqUSsaI3MUbwBeB+YDTznn5prZrWZ2ZuRmDwL5kV7B7wE3Re47F3gKmAe8BlzvnGts6ZjReg7SducM68zgztn876ufs2OnPliKJJKMlAA+U0+jtK+hZTksXLuV7fV6zxCJlbOHdWHDtnreX7TB6yjSQUR1TqNz7hXnXB/nXE/n3G2R637unJsY+brOOXeec66Xc+5I59ySfe57W+R+fZ1zrx7smAd43MxoPi9pmc9n/PyMAaypqeO+d5cc+g4i0mH4fEZWKKg5jdKuhpZl0+RgtrbeEImZE/oVkZ+RwhOfrPA6inQQCbEQjsSXUd3yOP2ITvzl3cWs2qx9gEQSSTgtQE2deoSk/QzposVwRGItJeDj3BFdeGv+OtbV1HkdRzoAFY0SFTef2h8zuO3l+V5HEZF2FFZPo7Sz/MxUyvPS+WyFikaRWPraqDIamhxPT9eCOHJoAa8DSGLqnJPG9eN78ft/L+SDig2M7VXgdSQRaQfhUPNKl9c//mmb7peTFuSWMwaSEtC5Svm/RnTN5f1FG3DOYWZexxFJCj0KMxndI48nP1nJt47ric+ntict07u3RM3Vx/agPC+dn/9rDjsbtDqXSCI4cWAxOelBPq+qafXl0+WbeWzqChau3ep1fIlTI7rmsmFbPSs2aUqDSCxddFRXVmzawbsL13sdReKceholakJBP784cwDfeHgaD32wlGuP6+l1JBE5TFeM7c4VY7u36T4fLd7IhfdP0aqr0qKR3XIBmLZsM13zMzxOI5I8ThlUQlFWKg9/uIzj+xV5HUfimHoaJapO6FfMlwcU86c3F1G5pdbrOCLigXCa9neUg+tTlEVWKMC05Zu8jiKSVIJ+Hxcf1ZV3F65nyfptXseROKaiUaLu56cPwOH4xURtqSmSjMKhIKD9HaU3VP4yAAAgAElEQVRlPp8xvDyXacs2ex1FJOlceFQZQb/x94+Wex1F4piKRom6srx0bpzQhzfmreXf89Z6HUdEYiycFikateqqHMTIrrksWreNLTt2eh1FJKkUZYU4Y0gpT01bSfUO/Z2WA1PRKDFx1bju9C3O4pZ/zWF7vYaoiSSTrNQAZmh/Rzmokd3yAJi+XL2NIrF2zbE92LGzkX9MVW+jHJiKRomJoN/Hr88ZRFVNHb97Y4HXcUQkhnw+IzM1oJ5GOaihZTmk+H1MXap5jSKx1q8kzHF9CvnbB8uo29XodRyJQyoaJWZGdM3jkqO68vCHy5ixUps4iySTcCioOY1yUGkpfoaW5TBlyUavo4gkpW8e24MN2+p5Zvoqr6NIHFLRKDH1o5P7UpwV4qZnZ2nvRpEkEk4LavVUOaTRPfKYs7paJxhEPHB0z3yGledw7zuL2dWoz2jyn1Q0SkxlhYL86iuD+HzNVu59Z7HXcUQkRsKhgAoBOaTRPfJpcjBtmYaoisSamfGdE3qzekstz3+62us4EmdUNErMfWlAMWcOKeXuSYv4fE2N13FEJAay04Ka0yiHNLxrLil+H1OWqGgU8cL4voUM7pzNPe9UqLdR/oOKRvHEL84cSDgU5IdPz9IfJZEkEE4LslWrp8ohhIJ+hpZrXqOIV8yMGyf0ZvnGHTyruY2yDxWN4om8jBR+9ZVBzF5dzX0apiqS8MIh9TRK64zukc+c1dXaL07EIxP6FzGsPIc/vbVIK6nKHioaxTOnDO7EGUNKufPtRcyr1DBVkUQWTguwtb6BxibndRSJc+N6F9Dk4KMlG7yOIpKUzIwfntSXquo6/jFF+zZKMxWN4qlbzxxITnoK33tqBvUNOpslkqjCoSAAW7UYjhzC0LIcMlMDvLdIRaOIV8b0LGBc7wLunlShXn8BVDSKx3IzUvjtV4/g8zVb+cMbC72OIyJREk5rLhq17YYcStDvY3SPfCaraBTx1M2n9Ke6dhd3T1rkdRSJAyoaxXPH9yviwiPL+ev7S/hosRY/EElE4VAAQNtuSKuM613Aik07WL5xu9dRRJLWgNIw543owsMfLlNbFBWNEh9+dnp/uuVn8P2nZlCtxTJEEs7enka1bzm0cb0LAHhfvY0invr+iX0J+n38v5fmeR1FPKaiUeJCekqAP35tKOu21vOT52fjnBbLEEkku+c0qqdRWqN7QQadc9J4b+F6r6OIJLXicIjvTOjNm/PX8fbna72OIx5S0ShxY0hZDv/15T68NKuKp6dpbyCRRBJOiwxP1ZxGaQUzY3zfQiZXbNAiaSIe+8bY7vQozOCXL87TFhxJTEWjxJVrj+vJmJ753DJxLhXrtnodR0TayZ7hqepplFaa0L+IHTsb+XjpJq+jiCS1lICPW88cxPKNO7hnUoXXccQjKholrvh9xh1fG0paip8bHv9MZ7REEkRmSgAzzWmU1ju6RwGpAR9vzV/ndRSRpHdM7wLOGdaZe99ZzII1OqmfjFQ0StwpDof4w/lD+HzNVn754lyv44hIO/D5jKzUADV1Gp4qrZOW4mdsrwLe/nyd5rmLxIGfnNafrFCA/352Fo1NapPJRkWjxKXxfYv41vie/PPjlTz/meY3iiSCcFpQPY3SJsf3K2LFph0sXq/l/kW8lp+Zyi/OHMiMlVu4//0lXseRGFPRKHHr+1/uw5Hd8/jxc3NYuFZDIUQ6unAoqDmN0iYT+hUB8OZ8rdooEg/OHFLKyQNL+MMbC/XZLMmoaJS4FfD7uPvCYWSkBrj2H9PZqg+bIh1aOC2g1VOlTUpz0jiiSzavzVnjdRQRoXll41+dPYisUIAbn5ih1Y2TSMDrACIHUxQOcfdFw7j4gan88OlZ3HvJcMzM61gi8gXkpKXw2tw19PnJq226X1YowCs3jqM4HIpSMolnJw0s4fbXF1BVXUun7DSv44gkvYLMVH577hFc+cg0fvf6An5y2gCvI0kMqGiUuDe6Rz43ndyP216Zz5/fWcz1x/fyOpKIfAHXH9+L7oUZbbpP1ZZaXphRyZL121U0JqmTBzUXjW/MXctlY7p5HUdEgAn9i7lkdDn3v7+Usb0KGN+3yOtIEmUqGqVDuGpcd2avruZ3byxgQKcwx/fTHyeRjmZwl2wGd8lu033mrK7mhRmVmguZxHoWZtK7KJPX5qxR0SgSR3562gCmLdvM956aycvfOUYjARKc5jRKh2Bm/OarR9C/JMx3nviMxeu3eR1JRGIgOy0IaH/HZHfyoBKmLt3I+q31XkcRkYhQ0M89Fw+nblcjNzz+GTsbmryOJFGkolE6jLQUP3+9dAQpfh9XPTKN6h36ECmS6MKhSNGo/R2T2hlDSmly8MrsKq+jiMg+ehZm8ttzj2D68s38v5fmeR1HokhFo3QoXXLTue/rI1i1eQfXP/4puxp1VkskkWWGmmdRqKcxufUpzqJfSRYTZ1Z6HUVE9nP6EaVcc2wPHp2ynCc/WeF1HIkSFY3S4YzqlsdtZw9mcsUGbpk4F+ec15FEJEr8PiMrNaA5jcIZQ0qZvnwzqzbv8DqKiOznRyf1ZVzvAn76whymLtnodRyJAhWN0iGdP7KMa4/ryeNTV/Dg5KVexxGRKAqnBbW/o3DmkFIAXpypIaoi8Sbg93H3RcMpy0vn2n9MZ+mG7V5HknamolE6rB+d1JdTBpVw2yvzeVXzXEQSVlZIPY0CZXnpDC/P4blPV2mEiUgcyk4L8tBlozAzLv/bx2zYpoWrEomKRumwfD7jjq8NZWhZDt99cgbTlm3yOpKIREFzT6OKRoHzRpaxaN02Zq6q9jqKiBxAt4IMHrhsJGtr6rjy4U/YVq9RIolCRaN0aKGgnwcuHUlpThpXPjKNinVbvY4kIu0sHApq9VQB4PQjOhEK+nh62kqvo4hIC4aX53L3hcOZU1nDNx+dRn1Do9eRpB2oaJQOLz8zlUeuOJKg38elD35MVXWt15FEpB2F0wLqaRQAskJBThnUiYkzK6nbpQ+iIvHqSwOKuf3cI/igYiM3PP6ZVrtPACoaJSGU56fz8BWjqKlr4NIHP2bz9p1eRxKRdtLc06iiUZqdN6ILW+saeHWO5rKLxLNzhnfh1rMG8u95a/nuEzNoUOHYoalolIQxqHM29186kuWbdnC5xtGLJIxwWpBt9Q00NWnxE4HRPfLpXpDBP6ZoPziReHfp0d346Wn9eXl2Fd95Qj2OHZmKRkkoR/fM556LhjNndTVXPzJNw5dEEkA4FMA52KoTQULzImgXH1XO9OWbmVupBXFE4t1V43rw09P688rsNVz32Kf6bNZBqWiUhPPlAcX8/rwhTFm6kWv/MV0TsEU6uHBaEEDzGmWP80aUEQr61Nso0kFcNa7HnqGq33j4E7brJGCHo6JREtJXhnXm12cP5p0F6zUBW6SDC4ciRaPmNUpEdnqQM4eU8sJnq6neod8LkY7g0qO78fvzhjB16SYu+OsU1m/VPo4diYpGSVgXHlnOL89sPqv1nX+qcBTpqMJpAQBqanVmWva6fEx3anc18tjHy72OIiKt9NURXfjr10ewaN1Wvnrvhyxev83rSNJKKholoV02phs/O30Ar85Zo8JRpINST6McyIDSMON6F/DwB8s0DUGkA5nQv5jHrx7N9voGzvnzh0xZstHrSNIKKhol4V15TPc9heP1j32qDxciHUy25jRKC64e14N1W+uZOKPS6ygi0gbDy3N5/rqxFGSmcMkDU3l8quYnxzsVjZIUrjymO784YwBvzFvLtY9O18pdIh3I3p5GDU+V/zSudwH9SrK4793FNGpLFpEOpTw/neeuG8vYXgX8+PnZ/OT52TqxH8eiWjSa2clmtsDMKszspgP8PNXMnoz8fKqZddvnZzdHrl9gZicd6phm9ljk+jlm9pCZBaP53KTjuXxs9+bFcRau54q/aR9HkY4iM7R7TqN6GuU/mRk3nNCLxeu38/LsKq/jiEgbZacFeejyUXzzuB48NnUFF/x1CpVbar2OJQcQtaLRzPzAPcApwADgQjMbsN/NrgQ2O+d6AXcAv4ncdwBwATAQOBn4s5n5D3HMx4B+wGAgDbgqWs9NOq6LjirnjvOH8vGyTVzywFS27NjpdSQROQS/z8hKDWhOoxzQqYM60bsok7veWkSTehtFOhy/z7j5lP7ce/FwFq3dxql3vs/bn6/1OpbsJ5o9jUcCFc65Jc65ncATwFn73eYs4JHI188AE8zMItc/4Zyrd84tBSoix2vxmM65V1wE8DHQJYrPTTqwrwzrzL0XD2deVQ3n3fcRVdU6oyUS78JpQa2eKgfk8zX3Ni5at029jSId2CmDO/Hit4+hNDuNbzw8jV++OFfTieJINIvGzsDKfb5fFbnugLdxzjUA1UD+Qe57yGNGhqV+HXjtQKHM7Bozm2Zm09avX9/GpySJ4sSBJTxyxZFUVddx7r0fUbFuq9eRROQgskLqaZSWnX5EKf1KsvjdGwvY2aBVskU6qu4FGTx33RguH9ONv32wjK/c8wHzq2q8jiUk5kI4fwbec869f6AfOuf+6pwb6ZwbWVhYGONoEk+O7pnPE9eMpr6hia/e+xGfLNvkdSQRaUFzT6OKRjkwv8/475P7sXzjDv75sVZhFOnIQkE/vzhzIH+7fBQbtu3krLs/4J5JFTRo2zRPBaJ47NVA2T7fd4lcd6DbrDKzAJANbDzEfVs8ppndAhQC32yH/JIEBnXO5vnrxnDZQx9z8QNT+cP5Qzj9iFKvY4nIfsKhIAvXbmXSgnVtul9GSoBR3XJpnvkgiWx830JG98jjzrcW8ZVhnfds1SIiHdPx/Yp4/bvj+Pm/5nL76wt4bc4afvPVIxhQGvY6WlKKZtH4CdDbzLrTXNhdAFy0320mApcBHwHnAm8755yZTQQeN7M/AKVAb5rnKVpLxzSzq4CTgAnOOZ2KkFYry0vn2W+N4ZpHp3HD45+xfOMOrhvfUx8yReJIaU6IN+ev5Yq/fdLm+75w/ViGluVEIZXEEzPjJ6cO4Mx7JvPHNxdyyxkDvY4kIocpPzOVey4ezqmzqrhl4hzOvHsyVx/bg++c0Ju0FL/X8ZJK1IpG51yDmd0AvA74gYecc3PN7FZgmnNuIvAg8KiZVQCbaC4CidzuKWAe0ABc75xrBDjQMSMPeR+wHPgo8mH/OefcrdF6fpJYcjNSePTKo/jRM7O4/fUFLN2wndvOHkRqQH+QROLBj0/tz9nD9p8Wf3BLN2zne0/NZF1NXZRSSbwZ3CWbC48s5+8fLeeCUeX0LcnyOpKItIPTjujE2F753PbyfO59ZzEvzqzk56cP4MsDinWSP0ai2dOIc+4V4JX9rvv5Pl/XAee1cN/bgNtac8zI9VF9LpL4QkE/f7pgKN0LMvjTW4tYtmE79319BAWZqV5HE0l6oaCfYeW5bbpPfkZz262p06qryeSHJ/blldlV/PSF2Tx5zdH4fPpAKZIIctJTuP28IZw7ogs/+9ccrnl0Osf1KeRnpw+gV1Gm1/ESXiIuhCPyhZkZ//XlPtx90TDmVFZz5l2Tmb2q2utYIvIFhNOazyVqAZ3kkpuRwo9P7c8nyzbz2NTlXscRkXZ2VI98Xv7OOH56Wn8+Xb6Zk//4Hr+YOJdN27X3djSpaBQ5gNOPKOWZa8dgZpx734c8O32V15FEpI0yUyNFo7bqSDrnjejCuN4F/O+rn7Nq8w6v44hIOwv6fVw1rgeTfjie80eV8fePlnHc7ZP48zsV1O7U3o7RoKJRpAWDOmfzrxvGMrw8l+8/PZOfvTCH+gb9IRLpKAJ+H5mpAWpqNTw12ZgZvz57MGbG956cSWOT8zqSiERBQWYqvz57MK9/91iO6p7Hb19bwHG3T+LvHy3TZ7Z2pqJR5CAKMlN59MojuebYHjw6ZTnn/2WKzlqLdCDhUEA9jUmqLC+dW88ayMfLNvHnSRVexxGRKOpdnMUDl43i6WuPplt+Bj//11yOv/0d/jFluYrHdqKiUeQQAn4fPz61P/ddMpwl67Zx2p2T+fe8tV7HEpFWCKcFNacxiZ09rDNnDS3ljjcX8mHFBq/jiEiUjeqWx5PfHM3fv3EkJdkhfvrCHI777Ts8NHkpO3Zq1MnhUNEo0konD+rEi98+hrK8NK7++zR+MXGuzl6JxLlwKKiexiRmZtx29mB6FGZywz8/o3JLrdeRRCTKzIxj+xTy7LfG8OiVR1Ken86tL81j7P++zR3/XsjGbfVeR+yQVDSKtEG3ggye/dYYrhjbjYc/XMbZ93zIorVbvY4lIi0Ip2lOY7LLTA3wl6+PYGdDE1f/fRrb6/X7IJIMzIxxvQt56ptH8+y3jmZE1zz+9NYixvzv29z07CwW6vNbm6hoFGmj1ICfW84YyIOXjWRNTR2n3zWZRz5chnNaaEEk3qinUQB6FmZy10XD+HzNVm54/FMaGpu8jiQiMTSiax4PXDaSN793HOcM78Lzn63mxDve46L7p/DanDX6m9AKKhpFvqAJ/Yt57bvjOLpnPrdMnMulD31MVbWGPonEE81plN2O71vE/ztrEJMWrOdHz8yiSSuqiiSdXkWZ/M85g/no5gn86OS+LNuwnWv/MZ1jfjOJP/x7oYawH4SKRpHDUJQV4m+Xj+JXXxnEtGWbOfGO93hm+ir1OorEiXAowNb6BhUIAsBFR5Xz/S/34bnPVvOTF2br90IkSeVlpHDd+F6896Pj+evXR9CnJIu73l7E2N+8zeV/+5hXZldp3Yr9BLwOINLRmRmXjO7KuN4F/PDpWfzg6Zm8NKuSX589mNKcNK/jiSS1cFoQ52DbzgbCoaDXcSQOfHtCb+obmrh7UgX1u5r47blHEPDrHLpIMgr4fZw4sIQTB5awctMOnp62kqemreK6xz4lJz3ImUNKOXtYZ4aW5WBmXsf1lIpGkXbSNT+DJ64ZzSMfLeO3ry3gy394l/8+pR8XH9UVvy+5/9CIeGV3oVhTu0tFo+zx/RP7EAr6+N0bC6mu3cWdFw4jI1UfiUSSWVleOt87sS83fqkPkys28PS0lTz5yUr+/tFyuuWnc+bQzpw5pJReRZleR/WE/kKKtCOfz7hibHe+1L+YHz8/m5//ay7Pfbqa/zlnMP07hb2OJ5J0wmnNb3M1tQ2Q63EYiRtmxg0n9CY7PYVb/jWHc+/7iPsvHUGX3HSvo4mIx/w+47g+hRzXp5Caul28NnsNL8xYzV1vL+LOtxbRrySL04/oxCmDO9GzMHkKSI3HEImCsrx0/v6NI7nja0NYsWkHp981mV+9NI9tWupdJKb29DRqBVU5gK+P7spDl49i1aYdnHbnZN7+fK3XkUQkjoRDQc4fVcbjV49m6s0TuOWMAWSkBvjdGwuZ8Pt3OfGOd/n9GwuYs7o64dezUE+jSJSYGWcP68LxfYv4zWsLeGDyUibOrOTHp/bnrKGlST82XiQWwml7h6eKHMj4vkW8+O1juO6xT/nGw9O4ZHQ5Pz61P+kp+ogkInsVhUNcMbY7V4ztzprqOl6dU8Xrc9dwz6QK7nq7gtLsEBP6F3NC/yKO7pFPKOj3OnK70l9EkSjLSU/hf84ZzPkju3DLxLl898kZ/GPKcm45YyCDu2R7HU8koe3taVQvv7SsW0EGz103ht+9voAHP1jKOwvWc+tZAzmhX7HX0UQkDpVk7y0gN23fyZvz1/LmvLU8M30Vj05ZTlrQz5ie+YzvW8hxfYooz+/4Q99VNIrEyLDyXF64bixPT1/J7a8v4Mx7JnPOsC784KQ+dMrWKqsi0bB3TqN6GuXgQkE/Pz19ACcOLOEnz8/mGw9PY3zfQm46pR/9SjQnXUQOLC8jhfNHlnH+yDLqdjUyZclG3v58He8sWM9bn68D5tItP51xvQs5pncBo3vkk53W8RZmU9EoEkM+n/G1UeWcOrgT90xazEOTl/Ly7EquOqYH3zyuB1la3VGkXWVGVsTUnEZprSO75/Hyd8bx8IdLufvtCk750/ucMqiE68b3YlBnjQ4RkZaFgn7G9y1ifN8inHMs3bCddxeu5/1FG/b0QvoMBnfJYUzPfI7ukc+IrrkdYvXm+E8okoCyQkFuOqUfFx9Vzu/eWMDdkyp4bOpyrj++F5eM7ppw4+BFvBLw+8hMDTSvnirSSikBH9cc25PzR5bxwPtLefjDZbwyew1Hdc/jktFdOXFgMakB/Z0WkZaZGT0KM+lRmMkVY7uzs6GJT1ds5sOKDXyweCP3v7eEe99ZTMBnDO6SzVHd8zmyey4juubFZU+kikYRD5XlpfOnC4Zx1TE9+O3rn/Orl+fz4OSlfPuE3pw3sgtBbTgtctjCoYB6GuULyUlP4Qcn9eXqY3vw5CcreOTD5Xz7n5+RnRbklEElnDq4E6N75JMS0N9qETm4lICP0T3yGd0jn+8B2+sbmLZ8M1OXbGTq0k08OHkJ973rMIM+RVmM6JbLyK65DC/PpWt+uucLKKpoFIkDg7tk8+iVR/FhxQZuf2MBP35+Nve+W8H143vx1REqHkUORzgtqDmNcliy04Jcc2xPrjqmBx8sbh5m9uLMSp74ZCWZqQHG9MxnbK8CjuyeR9/iLHw+rY4tIgeXkRrYsx8kQO3ORmas3MInyzYxbflmXpxRyeNTVwDN8yaHluUwrCyHIWU5DOmSQ3Z6bHsjVTSKxJExvQp4rmc+7yxYzx1vLuSm52Zz19sVXDu+J+eN6KJhqyJfQDgUVE+jtAufzxjXu5BxvQup29XIBxUbeHP+Ot5ftJ435jXv8ZiZGmBQ5zADS7PpV5JFr6Lm4WnxONxMROJHWoqfo3vmc3TPfAAamxyL1m3lsxVb+HT5Zmas3MLbn6/bc/uu+ekc0SWHwZ3DDOqczcDS7Kj+nVHRKBJnzIzj+xUxvm8h7yxYz51vL+JnL8zhzrcWcdUx3bl4dNc9i3uIyKGF0wJUbqnzOoYkmFDQz4T+xUzoX4xzjlWba/l46SZmrNzCrFVbeGzqcup2Ne25fU56kC65aZRmp1GSHaIoK5WCzFRyM1LITU8hOy1IOC1AZmqA9JQAfvVWiiQ1v8/oVxKmX0mYC48sB5oXdZu9qpqZq7Ywa2U1ny7fzIszK/fcpzwvnYGlYQaWhhlQGqZ/pzAl4VC7DG3VJ0+ROLVv8fjh4o38+Z0K/ufVz+lbksX4vkVexxPpMMKhIFM2beLm52a1+j4ju+bx1RFdophKEomZUZaXTlle+p7fm8Ymx/KN26lYt41lG7ezfOMOVm2uZemG7UxZsvGQe4eGgj7Sgn7Sgn5Sg35S/D5SAs2XgM8I+n34fUbAZ/h8ht8Mv88wA5/957+GRXKC7fn3Pz9E7vuZMlZTp4J+H7eeNSg2DyaSAMKhIGN7FTC2V8Ge6zZuq2f26mrmVtYwt7L531fnrNnz85z0IHd8bSjHH+ZnRxWNInHOzPb8gZhXWUP/TlleRxLpUI7umc8Hizfw1vx1h75xREaK3h7l8Ph9e1dOPJC6XY1s3L6Tzdt3snnHTmpqG6ip28XWul3s2NnI9voG6nY1UburkbpdjexsaGJnYxMNjY6djU3s2NlAY5OjocnR2ORocs3/OqBp97/O0bS3sxPnmq937j+zNF+7+zbt/lK0SFMuRA5ffmbqnm0+dttat4vP12xlflUN86tqKMs9/P3A9a4o0oEMKNUG0yJtdd7IMs4bWeZ1DJH/EAr66ZyTRuecw/8wJyKyr6xQkFHd8hjVLa/djqklGUVERERERKRFKhpFRERERESkRSoaRUREREREpEUqGkVERERERKRFKhpFRERERESkRSoaRUREREREpEUqGkVERERERKRFKhpFRERERESkRSoaRUREREREpEUqGkVERERERKRFKhpFRERERESkRSoaRUREREREpEUqGkVERERERKRFKhpFRERERESkRSoaRUREREREpEUqGkVERERERKRFKhpFRERERESkRSoaRUREREREpEUqGkVERERERKRFKhpFRERERESkRSoaRUREREREpEUqGkVERERERKRFKhpFRERERESkRSoaRUREREREpEUqGkVERERERKRFUS0azexkM1tgZhVmdtMBfp5qZk9Gfj7VzLrt87ObI9cvMLOTDnVMM+seOUZF5Jgp0XxuIiIiIiIiySBqRaOZ+YF7gFOAAcCFZjZgv5tdCWx2zvUC7gB+E7nvAOACYCBwMvBnM/Mf4pi/Ae6IHGtz5NgiIiIiIiJyGKLZ03gkUOGcW+Kc2wk8AZy1323OAh6JfP0MMMHMLHL9E865eufcUqAicrwDHjNynxMixyByzK9E8bmJiIiIiIgkhUAUj90ZWLnP96uAo1q6jXOuwcyqgfzI9VP2u2/nyNcHOmY+sMU513CA2/8HM7sGuCby7TYzW3CQ51AAbDjIz2MlHnIow17xkONgGbrGMsihTJ8+fYOZLT/ITeLh9YT4yBEPGSA+csRDBmg5R1y1M+gwbS0eMkB85FCGvdTO2l885FCGveIhx2F9doxm0RiXnHN/Bf7amtua2TTn3MgoR+oQOZQhvnLEQ4bWcs4VHuzn8fJc4iFHPGSIlxzxkCGecrRGR2hr8ZAhXnIoQ/zlaI2O0M7iJYcyxFeOw80QzeGpq4Gyfb7vErnugLcxswCQDWw8yH1bun4jkBM5RkuPJSIiIiIiIm0UzaLxE6B3ZFXTFJoXtpm4320mApdFvj4XeNs55yLXXxBZXbU70Bv4uKVjRu4zKXIMIsf8VxSfm4iIiIiISFKI2vDUyBzFG4DXAT/wkHNurpndCkxzzk0EHgQeNbMKYBPNRSCR2z0FzAMagOudc40ABzpm5CH/G3jCzH4FfBY59uFq1TDWGIiHHMqwVzzkiIcM7SVenks85IiHDBAfOeIhA8RPjvYQD88lHjJAfORQhr3iJUd7iJfnEg85lGGveMhxWBmsuZNORERERERE5P+K5vBUERERERER6QuHwvoAAAcHSURBVOBUNIqIiIiIiEiLVDS2wMxONrMFZlZhZjfF6DHLzGySmc0zs7lmdmPk+jwz+7eZLYr8mxuDLH4z+8zMXop8393MpkZejycjCxFFO0OOmT1jZp+b2XwzOzrWr4WZ/Vfk/2KOmf3TzEKxeC3M7CEzW2dmc/a57oDP3ZrdGckzy8yGt3eeaFE7UzuLZFA7iyIv2lnkcdXW9j6+5+0skkNtLYr0nqb3tEiGhGxnKhoPwMz8wD3AKcAA4EIzGxCDh24Avu+cGwCMBq6PPO5NwFvOud7AW5Hvo+1GYP4+3/8GuMM51wvYDFwZgwx/Al5zzvUDhkTyxOy1MLPOwHeAkc65QTQvvnQBsXktHgZO3u+6lp77KTSvMNwbuAa4Nwp52p3aGaB2pnYWZR62M1Bb25en7QzU1qJN72mA9+0M9J4WvXbmnNNlvwtwNPD6Pt/fDNzsQY5/AV8GFgCdItd1AhZE+XG7RH6xTgBeAgzYAAQO9PpEKUM2sJTIYk37XB+z1wLoDKwE8mheafgl4KRYvRZAN2DOoZ478BfgwgPdLp4vamdqZ5Hjq51F9/84LtpZ5LGTsq3FQzuLPIbaWnRf37hoa8naziKP4XlbS+R2pp7GA9v9H77bqsh1MWNm3YBhwFSg2DlXFfnRGqA4yg//R+BHQFPk+3xgi3OuIfJ9LF6P7sB64G+RoQ4PmFkGMXwtnHOrgd8BK4AqoBqYTuxfi91aeu6e/75+QZ7nVjtTOzsAtbMoSPK25nk7A7W1GPA8d5K3M4iDtpbI7UxFYxwys0zgWeC7zrmafX/mmk8HRG2fFDM7HVjnnJsercdopQAwHLjXOTcM2M5+wwli8FrkAmfR/EeoFMjg/3b7eyLazz0ZqJ0BamcHpXbWPtTWvG9noLaW6NTOgDhoa4nczlQ0HthqoGyf77tEros6MwvS3Ogfc849F7l6rZl1ivy8E7AuihHGAmea2TLgCZqHGfwJyDGzQOQ2sXg9VgGrnHNTI98/Q/Mfgli+Fl8Cljrn1jvndgHP0fz6xPq12K2l5+7Z7+thUjtTOwO1s2jzNLfaGhAf7QzU1qJN72l6T4MEbmcqGg/sE6B3ZKWjFJonsE6M9oOamQEPAvOdc3/Y50cTgcsiX19G83j1qHDO3eyc6+Kc60bz837bOXcxMAk4NxYZIjnWACvNrG/kqgnAPGL4WtA8tGC0maVH/m92Z4jpa7GPlp77RODSyEpYo4HqfYYixDO1M7UzUDuLNk/aGait7ZMhHtoZqK1Fm97T9J4GidzODjbhMZkvwKnAQmAx8JMYPeYxNHcbzwJmRC6n0jwu/C1gEfAmkBejPOOBlyJf9wA+BiqAp4HUGDz+UGBa5PV4AciN9WsB/BL4HJgDPAqkxuK1AP5J81j4XTSfObuypedO82TzeyK/q7NpXrEr6r8f7fQ81c7UztTOov9/HPN2FnlctbW9j+15O4vkUFuL7uur9zS9pyVsO7PIHUVERERERET+Dw1PFRERERERkRapaBQREREREZEWqWgUERERERGRFqloFBERERERkRapaBQREREREZEWqWiUL8TMfmJmc81slpnNMLOjzOy7ZpbudTaRRKF2JhJ9amcisaG21rFpyw1pMzM7GvgDMN45V29mBUAK8CHN+7xs8DSgSAJQOxOJPrUzkdhQW+v41NMoX0QnYINzrh4g0tDPBUqBSWY2CcDMTjSzj8zsUzN72swyI9cvM7PfmtlsM/vYzHpFrj/PzOaY2Uwze8+bpyYSN9TORKJP7UwkNtTWOjj1NEqbRRrwZCAdeBN40jn3rpktI3K2KHIG6TngFOfcdjP7byDVOXdr5Hb3O+duM7NLgfOdc6eb2WzgZOfcajPLcc5t8eQJisQBtTOR6FM7E4kNtbWOTz2N0mbOuW3ACOAaYD3wpJldvt/NRgMDgA/MbAZwGdB1n5//c59/j458/QHwsJldDfijk16kY1A7E4k+tTOR2FBb6/gCXgeQjsk51wi8A7wTOctz2X43MeDfzrkLWzrE/l875641s6OA04DpZjbCObexfZOLdBxqZyLRp3YmEhtqax2behqlzcysr5n13ueqocByYCuQFbluCjB2nzHnGWbWZ5/7fG2ffz+K3Kanc26qc+7nNJ+FKovi0xCJa2pnItGndiYSG2prHZ96GuWLyATuMrMcoAGooHm4wYXAa2ZW6Zw7PjLs4J9mlhq530+BhZGvc81sFlAfuR/A7ZE/KAa8BcyMybMRiU9qZyLRp3YmEhtqax2cFsKRmNt30rPXWUQSldqZSPSpnYnEhtqa9zQ8VURERERERFqknkYRERERERFpkXoaRUREREREpEUqGkVERERERKRFKhpFRERERESkRSoaRUREREREpEUqGkVERERERKRF/x8mOybhhWrqHwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1080x432 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBGQ-afRonh3"
      },
      "source": [
        ""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XoTViDRbLW-"
      },
      "source": [
        "class CustomLearningRateScheduler(keras.callbacks.Callback):\n",
        "    \"\"\"Learning rate scheduler which sets the learning rate according to schedule.\n",
        "\n",
        "  Arguments:\n",
        "      schedule: a function that takes an epoch index\n",
        "          (integer, indexed from 0) and current learning rate\n",
        "          as inputs and returns a new learning rate as output (float).\n",
        "  \"\"\"\n",
        "\n",
        "    def __init__(self, schedule):\n",
        "        super(CustomLearningRateScheduler, self).__init__()\n",
        "        self.schedule = schedule\n",
        "        self.current_epoch = 0\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.current_epoch += 1\n",
        "\n",
        "    def on_batch_begin(self, batch_step, logs=None):\n",
        "        if not hasattr(self.model.optimizer, \"lr\"):\n",
        "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
        "        # Get the current learning rate from model's optimizer.\n",
        "        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
        "        # Call schedule function to get the scheduled learning rate.\n",
        "        total_steps = batch_step + self.current_epoch * steps\n",
        "        scheduled_lr = self.schedule(total_steps, lr)\n",
        "        # Set the value back to the optimizer before this epoch starts\n",
        "        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n",
        "        # print(\"\\nEpoch %05d: Learning rate is %6.4f.\" % (total_steps, scheduled_lr))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsCNpoMEgIeV"
      },
      "source": [
        "start_profile_batch = steps+10\n",
        "stop_profile_batch = start_profile_batch + 100\n",
        "profile_range = f\"{start_profile_batch},{stop_profile_batch}\"\n",
        "\n",
        "log_path = log_dir + \"/\" + datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_path, histogram_freq=1,\n",
        "                                                     update_freq=20,profile_batch=profile_range)\n",
        "\n",
        "checkpoint_filepath = save_path + \"/\" + \"T5-{epoch:04d}-{val_loss:.4f}.ckpt\"\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=False,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True)\n",
        "\n",
        "callbacks = [tensorboard_callback, model_checkpoint_callback] \n",
        "if use_learning_schedule:\n",
        "    callbacks.append(CustomLearningRateScheduler(lr_schedule))\n",
        "metrics = [tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5,name='accuracy') ]#[drop_eval.get_metrics]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc5HnwUwgIeV"
      },
      "source": [
        "#### Compile and run model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wf6a-6OSSamp",
        "outputId": "ef9edc22-37fa-41c3-c1f1-660ed1032dee"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
        "                                     epsilon=1e-9)\n",
        "model.compile(optimizer=optimizer, metrics=metrics)\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"t5for_drop\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "shared (TFSharedEmbeddings)  multiple                  16449536  \n",
            "_________________________________________________________________\n",
            "encoder (TFT5MainLayer)      multiple                  18881280  \n",
            "_________________________________________________________________\n",
            "decoder (TFT5MainLayer)      multiple                  25175808  \n",
            "=================================================================\n",
            "Total params: 60,506,630\n",
            "Trainable params: 60,506,624\n",
            "Non-trainable params: 6\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        },
        "id": "aQmOhNO_hXOY",
        "outputId": "b066ff5f-e08b-4386-f410-29ad25e09f15"
      },
      "source": [
        "%tensorboard --logdir $log_dir"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 1106), started 1:02:40 ago. (Use '!kill 1106' to kill it.)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n",
              "            url.searchParams.set('tensorboardColab', 'true');\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjHtw6LogIeV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e2018076-42ff-453f-fffa-aa4c51efffde"
      },
      "source": [
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "if train_model:\n",
        "    model.fit(tf_train_ds, epochs=epochs, steps_per_epoch=steps, callbacks=callbacks, \n",
        "              validation_data=tf_valid_ds, validation_steps=valid_steps,verbose=1)\n",
        "    if(save_model):\n",
        "        model.save_pretrained(save_path)\n",
        "        print('Training complete, model saved')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "\n",
            "WARM UP: Using Increasing Linear Function at Training Step:0\n",
            "\n",
            "Epoch 00000: Learning rate is 0.0000.\n",
            "  1/125 [..............................] - ETA: 42:04 - accuracy: 0.0926 - loss: 13.4933 - F1: 0.1201 - EM: 0.0000e+00 - lr: 1.0000e-05\n",
            "WARM UP: Using Increasing Linear Function at Training Step:1\n",
            "\n",
            "Epoch 00001: Learning rate is 0.0000.\n",
            "  2/125 [..............................] - ETA: 1:27 - accuracy: 0.1019 - loss: 13.3390 - F1: 0.1329 - EM: 0.0000e+00 - lr: 2.3200e-05 \n",
            "WARM UP: Using Increasing Linear Function at Training Step:2\n",
            "\n",
            "Epoch 00002: Learning rate is 0.0001.\n",
            "  3/125 [..............................] - ETA: 56s - accuracy: 0.1073 - loss: 13.2678 - F1: 0.1370 - EM: 0.0000e+00 - lr: 3.6400e-05 \n",
            "WARM UP: Using Increasing Linear Function at Training Step:3\n",
            "\n",
            "Epoch 00003: Learning rate is 0.0001.\n",
            "  4/125 [..............................] - ETA: 46s - accuracy: 0.1128 - loss: 13.0179 - F1: 0.1216 - EM: 0.0000e+00 - lr: 4.9600e-05\n",
            "WARM UP: Using Increasing Linear Function at Training Step:4\n",
            "\n",
            "Epoch 00004: Learning rate is 0.0001.\n",
            "  5/125 [>.............................] - ETA: 40s - accuracy: 0.1282 - loss: 12.3972 - F1: 0.1232 - EM: 0.0000e+00 - lr: 6.2800e-05\n",
            "WARM UP: Using Increasing Linear Function at Training Step:5\n",
            "\n",
            "Epoch 00005: Learning rate is 0.0001.\n",
            "  6/125 [>.............................] - ETA: 37s - accuracy: 0.1640 - loss: 11.5617 - F1: 0.1242 - EM: 0.0000e+00 - lr: 7.6000e-05\n",
            "WARM UP: Using Increasing Linear Function at Training Step:6\n",
            "\n",
            "Epoch 00006: Learning rate is 0.0002.\n",
            "  7/125 [>.............................] - ETA: 35s - accuracy: 0.2371 - loss: 10.5070 - F1: 0.1323 - EM: 0.0000e+00 - lr: 8.9200e-05\n",
            "WARM UP: Using Increasing Linear Function at Training Step:7\n",
            "\n",
            "Epoch 00007: Learning rate is 0.0002.\n",
            "  8/125 [>.............................] - ETA: 33s - accuracy: 0.3015 - loss: 9.6633 - F1: 0.1359 - EM: 0.0000e+00 - lr: 1.0240e-04 \n",
            "WARM UP: Using Increasing Linear Function at Training Step:8\n",
            "\n",
            "Epoch 00008: Learning rate is 0.0002.\n",
            "  9/125 [=>............................] - ETA: 32s - accuracy: 0.3614 - loss: 8.8352 - F1: 0.1667 - EM: 0.0000e+00 - lr: 1.1560e-04\n",
            "WARM UP: Using Increasing Linear Function at Training Step:9\n",
            "\n",
            "Epoch 00009: Learning rate is 0.0002.\n",
            " 10/125 [=>............................] - ETA: 30s - accuracy: 0.4125 - loss: 8.1235 - F1: 0.1975 - EM: 0.0000e+00 - lr: 1.2880e-04\n",
            "WARM UP: Using Increasing Linear Function at Training Step:10\n",
            "\n",
            "Epoch 00010: Learning rate is 0.0003.\n",
            " 11/125 [=>............................] - ETA: 30s - accuracy: 0.4541 - loss: 7.5197 - F1: 0.2054 - EM: 0.0000e+00 - lr: 1.4200e-04\n",
            "WARM UP: Using Increasing Linear Function at Training Step:11\n",
            "\n",
            "Epoch 00011: Learning rate is 0.0003.\n",
            " 12/125 [=>............................] - ETA: 29s - accuracy: 0.4896 - loss: 7.0106 - F1: 0.2151 - EM: 0.0000e+00 - lr: 1.5520e-04\n",
            "WARM UP: Using Increasing Linear Function at Training Step:12\n",
            "\n",
            "Epoch 00012: Learning rate is 0.0003.\n",
            " 13/125 [==>...........................] - ETA: 28s - accuracy: 0.5185 - loss: 6.5863 - F1: 0.2236 - EM: 0.0000e+00 - lr: 1.6840e-04\n",
            "WARM UP: Using Increasing Linear Function at Training Step:13\n",
            "\n",
            "Epoch 00013: Learning rate is 0.0004.\n",
            " 14/125 [==>...........................] - ETA: 27s - accuracy: 0.5443 - loss: 6.2029 - F1: 0.2329 - EM: 0.0000e+00 - lr: 1.8160e-04\n",
            "WARM UP: Using Increasing Linear Function at Training Step:14\n",
            "\n",
            "Epoch 00014: Learning rate is 0.0004.\n",
            " 15/125 [==>...........................] - ETA: 27s - accuracy: 0.5670 - loss: 5.8666 - F1: 0.2369 - EM: 0.0000e+00 - lr: 1.9480e-04\n",
            "WARM UP: Using Increasing Linear Function at Training Step:15\n",
            "\n",
            "Epoch 00015: Learning rate is 0.0004.\n",
            " 16/125 [==>...........................] - ETA: 26s - accuracy: 0.5885 - loss: 5.5620 - F1: 0.2412 - EM: 0.0000e+00 - lr: 2.0800e-04\n",
            "WARM UP: Using Increasing Linear Function at Training Step:16\n",
            "\n",
            "Epoch 00016: Learning rate is 0.0004.\n",
            " 17/125 [===>..........................] - ETA: 26s - accuracy: 0.6093 - loss: 5.2789 - F1: 0.2580 - EM: 0.0000e+00 - lr: 2.2120e-04\n",
            "WARM UP: Using Increasing Linear Function at Training Step:17\n",
            "\n",
            "Epoch 00017: Learning rate is 0.0005.\n",
            " 18/125 [===>..........................] - ETA: 25s - accuracy: 0.6278 - loss: 5.0317 - F1: 0.2775 - EM: 0.0000e+00 - lr: 2.3440e-04\n",
            "WARM UP: Using Increasing Linear Function at Training Step:18\n",
            "\n",
            "Epoch 00018: Learning rate is 0.0005.\n",
            " 19/125 [===>..........................] - ETA: 25s - accuracy: 0.6435 - loss: 4.8136 - F1: 0.2791 - EM: 0.0000e+00 - lr: 2.4760e-04\n",
            "WARM UP: Using Increasing Linear Function at Training Step:19\n",
            "\n",
            "Epoch 00019: Learning rate is 0.0005.\n",
            " 20/125 [===>..........................] - ETA: 25s - accuracy: 0.6586 - loss: 4.6174 - F1: 0.2818 - EM: 0.0000e+00 - lr: 2.6080e-04\n",
            "WARM UP: Using Increasing Linear Function at Training Step:20\n",
            "\n",
            "Epoch 00020: Learning rate is 0.0005.\n",
            " 21/125 [====>.........................] - ETA: 24s - accuracy: 0.6721 - loss: 4.4378 - F1: 0.2945 - EM: 0.0000e+00 - lr: 2.7400e-04\n",
            "WARM UP: Using Increasing Linear Function at Training Step:21\n",
            "\n",
            "Epoch 00021: Learning rate is 0.0006.\n",
            " 22/125 [====>.........................] - ETA: 24s - accuracy: 0.6853 - loss: 4.2684 - F1: 0.2990 - EM: 0.0000e+00 - lr: 2.8720e-04\n",
            "WARM UP: Using Increasing Linear Function at Training Step:22\n",
            "\n",
            "Epoch 00022: Learning rate is 0.0006.\n",
            " 23/125 [====>.........................] - ETA: 24s - accuracy: 0.6964 - loss: 4.1262 - F1: 0.2987 - EM: 0.0000e+00 - lr: 3.0040e-04\n",
            "WARM UP: Using Increasing Linear Function at Training Step:23\n",
            "\n",
            "Epoch 00023: Learning rate is 0.0006.\n",
            " 24/125 [====>.........................] - ETA: 23s - accuracy: 0.7072 - loss: 3.9878 - F1: nan - EM: 0.0000e+00 - lr: 3.1360e-04   \n",
            "WARM UP: Using Increasing Linear Function at Training Step:24\n",
            "\n",
            "Epoch 00024: Learning rate is 0.0006.\n",
            " 25/125 [=====>........................] - ETA: 23s - accuracy: 0.7173 - loss: 3.8583 - F1: nan - EM: 0.0000e+00 - lr: 3.2680e-04\n",
            "WARM UP: Using Increasing Linear Function at Training Step:25\n",
            "\n",
            "Epoch 00025: Learning rate is 0.0007.\n",
            " 26/125 [=====>........................] - ETA: 23s - accuracy: 0.7273 - loss: 3.7358 - F1: nan - EM: 0.0000e+00 - lr: 3.4000e-04\n",
            "WARM UP: Using Increasing Linear Function at Training Step:26\n",
            "\n",
            "Epoch 00026: Learning rate is 0.0007.\n",
            " 27/125 [=====>........................] - ETA: 22s - accuracy: 0.7362 - loss: 3.6225 - F1: nan - EM: 0.0000e+00 - lr: 3.5320e-04\n",
            "WARM UP: Using Increasing Linear Function at Training Step:27\n",
            "\n",
            "Epoch 00027: Learning rate is 0.0007.\n",
            " 28/125 [=====>........................] - ETA: 22s - accuracy: 0.7444 - loss: 3.5216 - F1: nan - EM: 0.0000e+00 - lr: 3.6640e-04\n",
            "WARM UP: Using Increasing Linear Function at Training Step:28\n",
            "\n",
            "Epoch 00028: Learning rate is 0.0007.\n",
            " 29/125 [=====>........................] - ETA: 22s - accuracy: 0.7518 - loss: 3.4210 - F1: nan - EM: 0.0000e+00 - lr: 3.7960e-04\n",
            "WARM UP: Using Increasing Linear Function at Training Step:29\n",
            "\n",
            "Epoch 00029: Learning rate is 0.0008.\n",
            " 30/125 [======>.......................] - ETA: 21s - accuracy: 0.7592 - loss: 3.3255 - F1: nan - EM: 0.0000e+00 - lr: 3.9280e-04\n",
            "WARM UP: Using Increasing Linear Function at Training Step:30\n",
            "\n",
            "Epoch 00030: Learning rate is 0.0008.\n",
            " 31/125 [======>.......................] - ETA: 21s - accuracy: 0.7656 - loss: 3.2409 - F1: nan - EM: 0.0000e+00 - lr: 4.0600e-04\n",
            "WARM UP: Using Increasing Linear Function at Training Step:31\n",
            "\n",
            "Epoch 00031: Learning rate is 0.0008.\n",
            " 32/125 [======>.......................] - ETA: 21s - accuracy: 0.7721 - loss: 3.1530 - F1: nan - EM: 0.0000e+00 - lr: 4.1920e-04\n",
            "WARM UP: Using Increasing Linear Function at Training Step:32\n",
            "\n",
            "Epoch 00032: Learning rate is 0.0009.\n",
            " 33/125 [======>.......................] - ETA: 21s - accuracy: 0.7780 - loss: 3.0764 - F1: nan - EM: 0.0000e+00 - lr: 4.3240e-04\n",
            "WARM UP: Using Increasing Linear Function at Training Step:33\n",
            "\n",
            "Epoch 00033: Learning rate is 0.0009.\n",
            " 34/125 [=======>......................] - ETA: 20s - accuracy: 0.7838 - loss: 2.9987 - F1: nan - EM: 0.0000e+00 - lr: 4.4560e-04\n",
            "WARM UP: Using Increasing Linear Function at Training Step:34\n",
            "\n",
            "Epoch 00034: Learning rate is 0.0009.\n",
            " 35/125 [=======>......................] - ETA: 20s - accuracy: 0.7888 - loss: 2.9285 - F1: nan - EM: 0.0000e+00 - lr: 4.5880e-04\n",
            "WARM UP: Using Increasing Linear Function at Training Step:35\n",
            "\n",
            "Epoch 00035: Learning rate is 0.0009.\n",
            " 36/125 [=======>......................] - ETA: 20s - accuracy: 0.7937 - loss: 2.8614 - F1: nan - EM: 0.0000e+00 - lr: 4.7200e-04\n",
            "WARM UP: Using Increasing Linear Function at Training Step:36\n",
            "\n",
            "Epoch 00036: Learning rate is 0.0010.\n",
            " 37/125 [=======>......................] - ETA: 20s - accuracy: 0.7984 - loss: 2.7968 - F1: nan - EM: 0.0000e+00 - lr: 4.8520e-04\n",
            "WARM UP: Using Increasing Linear Function at Training Step:37\n",
            "\n",
            "Epoch 00037: Learning rate is 0.0010.\n",
            " 38/125 [========>.....................] - ETA: 19s - accuracy: 0.8029 - loss: 2.7325 - F1: nan - EM: 0.0000e+00 - lr: 4.9840e-04\n",
            "Epoch 00038: Learning rate is 0.0010.\n",
            " 39/125 [========>.....................] - ETA: 19s - accuracy: 0.8072 - loss: 2.6713 - F1: nan - EM: 0.0000e+00 - lr: 5.1126e-04\n",
            "Epoch 00039: Learning rate is 0.0010.\n",
            " 40/125 [========>.....................] - ETA: 19s - accuracy: 0.8112 - loss: 2.6128 - F1: nan - EM: 0.0000e+00 - lr: 5.2348e-04\n",
            "Epoch 00040: Learning rate is 0.0010.\n",
            " 41/125 [========>.....................] - ETA: 19s - accuracy: 0.8150 - loss: 2.5583 - F1: nan - EM: 0.0000e+00 - lr: 5.3510e-04\n",
            "Epoch 00041: Learning rate is 0.0010.\n",
            " 42/125 [=========>....................] - ETA: 18s - accuracy: 0.8188 - loss: 2.5053 - F1: nan - EM: 0.0000e+00 - lr: 5.4617e-04\n",
            "Epoch 00042: Learning rate is 0.0010.\n",
            " 43/125 [=========>....................] - ETA: 18s - accuracy: 0.8225 - loss: 2.4538 - F1: nan - EM: 0.0000e+00 - lr: 5.5673e-04\n",
            "Epoch 00043: Learning rate is 0.0010.\n",
            " 44/125 [=========>....................] - ETA: 18s - accuracy: 0.8259 - loss: 2.4056 - F1: nan - EM: 0.0000e+00 - lr: 5.6680e-04\n",
            "Epoch 00044: Learning rate is 0.0010.\n",
            " 45/125 [=========>....................] - ETA: 18s - accuracy: 0.8292 - loss: 2.3588 - F1: nan - EM: 0.0000e+00 - lr: 5.7643e-04\n",
            "Epoch 00045: Learning rate is 0.0010.\n",
            " 46/125 [==========>...................] - ETA: 17s - accuracy: 0.8321 - loss: 2.3153 - F1: nan - EM: 0.0000e+00 - lr: 5.8563e-04\n",
            "Epoch 00046: Learning rate is 0.0010.\n",
            " 47/125 [==========>...................] - ETA: 17s - accuracy: 0.8352 - loss: 2.2718 - F1: nan - EM: 0.0000e+00 - lr: 5.9445e-04\n",
            "Epoch 00047: Learning rate is 0.0010.\n",
            " 48/125 [==========>...................] - ETA: 17s - accuracy: 0.8380 - loss: 2.2312 - F1: nan - EM: 0.0000e+00 - lr: 6.0290e-04\n",
            "Epoch 00048: Learning rate is 0.0010.\n",
            " 49/125 [==========>...................] - ETA: 17s - accuracy: 0.8408 - loss: 2.1909 - F1: nan - EM: 0.0000e+00 - lr: 6.1100e-04\n",
            "Epoch 00049: Learning rate is 0.0010.\n",
            " 50/125 [===========>..................] - ETA: 16s - accuracy: 0.8437 - loss: 2.1519 - F1: nan - EM: 0.0000e+00 - lr: 6.1878e-04\n",
            "Epoch 00050: Learning rate is 0.0010.\n",
            " 51/125 [===========>..................] - ETA: 16s - accuracy: 0.8461 - loss: 2.1168 - F1: nan - EM: 0.0000e+00 - lr: 6.2626e-04\n",
            "Epoch 00051: Learning rate is 0.0010.\n",
            " 52/125 [===========>..................] - ETA: 16s - accuracy: 0.8486 - loss: 2.0811 - F1: nan - EM: 0.0000e+00 - lr: 6.3345e-04\n",
            "Epoch 00052: Learning rate is 0.0010.\n",
            " 53/125 [===========>..................] - ETA: 16s - accuracy: 0.8510 - loss: 2.0470 - F1: nan - EM: 0.0000e+00 - lr: 6.4036e-04\n",
            "Epoch 00053: Learning rate is 0.0010.\n",
            " 54/125 [===========>..................] - ETA: 15s - accuracy: 0.8530 - loss: 2.0154 - F1: nan - EM: 0.0000e+00 - lr: 6.4702e-04\n",
            "Epoch 00054: Learning rate is 0.0010.\n",
            " 55/125 [============>.................] - ETA: 15s - accuracy: 0.8551 - loss: 1.9854 - F1: nan - EM: 0.0000e+00 - lr: 6.5344e-04\n",
            "Epoch 00055: Learning rate is 0.0010.\n",
            " 56/125 [============>.................] - ETA: 15s - accuracy: 0.8573 - loss: 1.9537 - F1: nan - EM: 0.0000e+00 - lr: 6.5963e-04\n",
            "Epoch 00056: Learning rate is 0.0010.\n",
            " 57/125 [============>.................] - ETA: 15s - accuracy: 0.8594 - loss: 1.9233 - F1: nan - EM: 0.0000e+00 - lr: 6.6560e-04\n",
            "Epoch 00057: Learning rate is 0.0010.\n",
            " 58/125 [============>.................] - ETA: 15s - accuracy: 0.8613 - loss: 1.8960 - F1: nan - EM: 0.0000e+00 - lr: 6.7137e-04\n",
            "Epoch 00058: Learning rate is 0.0010.\n",
            " 59/125 [=============>................] - ETA: 14s - accuracy: 0.8633 - loss: 1.8676 - F1: nan - EM: 0.0000e+00 - lr: 6.7694e-04\n",
            "Epoch 00059: Learning rate is 0.0010.\n",
            " 60/125 [=============>................] - ETA: 14s - accuracy: 0.8652 - loss: 1.8401 - F1: nan - EM: 0.0000e+00 - lr: 6.8232e-04\n",
            "Epoch 00060: Learning rate is 0.0010.\n",
            " 61/125 [=============>................] - ETA: 14s - accuracy: 0.8671 - loss: 1.8136 - F1: nan - EM: 0.0000e+00 - lr: 6.8753e-04\n",
            "Epoch 00061: Learning rate is 0.0010.\n",
            " 62/125 [=============>................] - ETA: 14s - accuracy: 0.8689 - loss: 1.7873 - F1: nan - EM: 0.0000e+00 - lr: 6.9257e-04\n",
            "Epoch 00062: Learning rate is 0.0010.\n",
            " 63/125 [==============>...............] - ETA: 13s - accuracy: 0.8707 - loss: 1.7627 - F1: nan - EM: 0.0000e+00 - lr: 6.9745e-04\n",
            "Epoch 00063: Learning rate is 0.0010.\n",
            " 64/125 [==============>...............] - ETA: 13s - accuracy: 0.8723 - loss: 1.7389 - F1: nan - EM: 0.0000e+00 - lr: 7.0218e-04\n",
            "Epoch 00064: Learning rate is 0.0010.\n",
            " 65/125 [==============>...............] - ETA: 13s - accuracy: 0.8739 - loss: 1.7164 - F1: nan - EM: 0.0000e+00 - lr: 7.0676e-04\n",
            "Epoch 00065: Learning rate is 0.0010.\n",
            " 66/125 [==============>...............] - ETA: 13s - accuracy: 0.8755 - loss: 1.6931 - F1: nan - EM: 0.0000e+00 - lr: 7.1120e-04\n",
            "Epoch 00066: Learning rate is 0.0010.\n",
            " 67/125 [===============>..............] - ETA: 12s - accuracy: 0.8770 - loss: 1.6711 - F1: nan - EM: 0.0000e+00 - lr: 7.1551e-04\n",
            "Epoch 00067: Learning rate is 0.0010.\n",
            " 68/125 [===============>..............] - ETA: 12s - accuracy: 0.8785 - loss: 1.6504 - F1: nan - EM: 0.0000e+00 - lr: 7.1969e-04\n",
            "Epoch 00068: Learning rate is 0.0010.\n",
            " 69/125 [===============>..............] - ETA: 12s - accuracy: 0.8800 - loss: 1.6299 - F1: nan - EM: 0.0000e+00 - lr: 7.2376e-04\n",
            "Epoch 00069: Learning rate is 0.0010.\n",
            " 70/125 [===============>..............] - ETA: 12s - accuracy: 0.8814 - loss: 1.6100 - F1: nan - EM: 0.0000e+00 - lr: 7.2770e-04\n",
            "Epoch 00070: Learning rate is 0.0010.\n",
            " 71/125 [================>.............] - ETA: 12s - accuracy: 0.8827 - loss: 1.5911 - F1: nan - EM: 0.0000e+00 - lr: 7.3154e-04\n",
            "Epoch 00071: Learning rate is 0.0010.\n",
            " 72/125 [================>.............] - ETA: 11s - accuracy: 0.8840 - loss: 1.5716 - F1: nan - EM: 0.0000e+00 - lr: 7.3527e-04\n",
            "Epoch 00072: Learning rate is 0.0009.\n",
            " 73/125 [================>.............] - ETA: 11s - accuracy: 0.8853 - loss: 1.5527 - F1: nan - EM: 0.0000e+00 - lr: 7.3821e-04\n",
            "Epoch 00073: Learning rate is 0.0009.\n",
            " 74/125 [================>.............] - ETA: 11s - accuracy: 0.8867 - loss: 1.5347 - F1: nan - EM: 0.0000e+00 - lr: 7.4107e-04\n",
            "Epoch 00074: Learning rate is 0.0009.\n",
            " 75/125 [=================>............] - ETA: 11s - accuracy: 0.8879 - loss: 1.5167 - F1: nan - EM: 0.0000e+00 - lr: 7.4386e-04\n",
            "Epoch 00075: Learning rate is 0.0009.\n",
            " 76/125 [=================>............] - ETA: 10s - accuracy: 0.8891 - loss: 1.4991 - F1: nan - EM: 0.0000e+00 - lr: 7.4657e-04\n",
            "Epoch 00076: Learning rate is 0.0009.\n",
            " 77/125 [=================>............] - ETA: 10s - accuracy: 0.8903 - loss: 1.4821 - F1: nan - EM: 0.0000e+00 - lr: 7.4921e-04\n",
            "Epoch 00077: Learning rate is 0.0009.\n",
            " 78/125 [=================>............] - ETA: 10s - accuracy: 0.8914 - loss: 1.4666 - F1: nan - EM: 0.0000e+00 - lr: 7.5178e-04\n",
            "Epoch 00078: Learning rate is 0.0009.\n",
            " 79/125 [=================>............] - ETA: 10s - accuracy: 0.8926 - loss: 1.4500 - F1: nan - EM: 0.0000e+00 - lr: 7.5429e-04\n",
            "Epoch 00079: Learning rate is 0.0009.\n",
            " 80/125 [==================>...........] - ETA: 10s - accuracy: 0.8936 - loss: 1.4355 - F1: nan - EM: 0.0000e+00 - lr: 7.5674e-04\n",
            "Epoch 00080: Learning rate is 0.0009.\n",
            " 81/125 [==================>...........] - ETA: 9s - accuracy: 0.8947 - loss: 1.4204 - F1: nan - EM: 0.0000e+00 - lr: 7.5913e-04 \n",
            "Epoch 00081: Learning rate is 0.0009.\n",
            " 82/125 [==================>...........] - ETA: 9s - accuracy: 0.8958 - loss: 1.4054 - F1: nan - EM: 0.0000e+00 - lr: 7.6145e-04\n",
            "Epoch 00082: Learning rate is 0.0009.\n",
            " 83/125 [==================>...........] - ETA: 9s - accuracy: 0.8968 - loss: 1.3911 - F1: nan - EM: 0.0000e+00 - lr: 7.6373e-04\n",
            "Epoch 00083: Learning rate is 0.0009.\n",
            " 84/125 [===================>..........] - ETA: 9s - accuracy: 0.8978 - loss: 1.3768 - F1: nan - EM: 0.0000e+00 - lr: 7.6594e-04\n",
            "Epoch 00084: Learning rate is 0.0009.\n",
            " 85/125 [===================>..........] - ETA: 8s - accuracy: 0.8987 - loss: 1.3635 - F1: nan - EM: 0.0000e+00 - lr: 7.6811e-04\n",
            "Epoch 00085: Learning rate is 0.0009.\n",
            " 86/125 [===================>..........] - ETA: 8s - accuracy: 0.8997 - loss: 1.3498 - F1: nan - EM: 0.0000e+00 - lr: 7.7022e-04\n",
            "Epoch 00086: Learning rate is 0.0009.\n",
            " 87/125 [===================>..........] - ETA: 8s - accuracy: 0.9006 - loss: 1.3364 - F1: nan - EM: 0.0000e+00 - lr: 7.7229e-04\n",
            "Epoch 00087: Learning rate is 0.0009.\n",
            " 88/125 [====================>.........] - ETA: 8s - accuracy: 0.9015 - loss: 1.3233 - F1: nan - EM: 0.0000e+00 - lr: 7.7431e-04\n",
            "Epoch 00088: Learning rate is 0.0009.\n",
            " 89/125 [====================>.........] - ETA: 8s - accuracy: 0.9024 - loss: 1.3109 - F1: nan - EM: 0.0000e+00 - lr: 7.7628e-04\n",
            "Epoch 00089: Learning rate is 0.0009.\n",
            " 90/125 [====================>.........] - ETA: 7s - accuracy: 0.9033 - loss: 1.2984 - F1: nan - EM: 0.0000e+00 - lr: 7.7821e-04\n",
            "Epoch 00090: Learning rate is 0.0009.\n",
            " 91/125 [====================>.........] - ETA: 7s - accuracy: 0.9042 - loss: 1.2861 - F1: nan - EM: 0.0000e+00 - lr: 7.8010e-04\n",
            "Epoch 00091: Learning rate is 0.0009.\n",
            " 92/125 [=====================>........] - ETA: 7s - accuracy: 0.9050 - loss: 1.2742 - F1: nan - EM: 0.0000e+00 - lr: 7.8195e-04\n",
            "Epoch 00092: Learning rate is 0.0009.\n",
            " 93/125 [=====================>........] - ETA: 7s - accuracy: 0.9058 - loss: 1.2628 - F1: nan - EM: 0.0000e+00 - lr: 7.8375e-04\n",
            "Epoch 00093: Learning rate is 0.0009.\n",
            " 94/125 [=====================>........] - ETA: 6s - accuracy: 0.9066 - loss: 1.2512 - F1: nan - EM: 0.0000e+00 - lr: 7.8552e-04\n",
            "Epoch 00094: Learning rate is 0.0009.\n",
            " 95/125 [=====================>........] - ETA: 6s - accuracy: 0.9074 - loss: 1.2396 - F1: nan - EM: 0.0000e+00 - lr: 7.8725e-04\n",
            "Epoch 00095: Learning rate is 0.0009.\n",
            " 96/125 [======================>.......] - ETA: 6s - accuracy: 0.9082 - loss: 1.2288 - F1: nan - EM: 0.0000e+00 - lr: 7.8895e-04\n",
            "Epoch 00096: Learning rate is 0.0009.\n",
            " 97/125 [======================>.......] - ETA: 6s - accuracy: 0.9089 - loss: 1.2190 - F1: nan - EM: 0.0000e+00 - lr: 7.9061e-04\n",
            "Epoch 00097: Learning rate is 0.0009.\n",
            " 98/125 [======================>.......] - ETA: 5s - accuracy: 0.9096 - loss: 1.2084 - F1: nan - EM: 0.0000e+00 - lr: 7.9224e-04\n",
            "Epoch 00098: Learning rate is 0.0009.\n",
            " 99/125 [======================>.......] - ETA: 5s - accuracy: 0.9104 - loss: 1.1977 - F1: nan - EM: 0.0000e+00 - lr: 7.9383e-04\n",
            "Epoch 00099: Learning rate is 0.0009.\n",
            "100/125 [=======================>......] - ETA: 5s - accuracy: 0.9111 - loss: 1.1885 - F1: nan - EM: 0.0000e+00 - lr: 7.9539e-04\n",
            "Epoch 00100: Learning rate is 0.0009.\n",
            "101/125 [=======================>......] - ETA: 5s - accuracy: 0.9119 - loss: 1.1782 - F1: nan - EM: 0.0000e+00 - lr: 7.9692e-04\n",
            "Epoch 00101: Learning rate is 0.0009.\n",
            "102/125 [=======================>......] - ETA: 5s - accuracy: 0.9126 - loss: 1.1686 - F1: nan - EM: 0.0000e+00 - lr: 7.9842e-04\n",
            "Epoch 00102: Learning rate is 0.0009.\n",
            "103/125 [=======================>......] - ETA: 4s - accuracy: 0.9133 - loss: 1.1589 - F1: nan - EM: 0.0000e+00 - lr: 7.9990e-04\n",
            "Epoch 00103: Learning rate is 0.0009.\n",
            "104/125 [=======================>......] - ETA: 4s - accuracy: 0.9139 - loss: 1.1496 - F1: nan - EM: 0.0000e+00 - lr: 8.0134e-04\n",
            "Epoch 00104: Learning rate is 0.0009.\n",
            "105/125 [========================>.....] - ETA: 4s - accuracy: 0.9146 - loss: 1.1400 - F1: nan - EM: 0.0000e+00 - lr: 8.0275e-04\n",
            "Epoch 00105: Learning rate is 0.0009.\n",
            "106/125 [========================>.....] - ETA: 4s - accuracy: 0.9152 - loss: 1.1316 - F1: nan - EM: 0.0000e+00 - lr: 8.0414e-04\n",
            "Epoch 00106: Learning rate is 0.0009.\n",
            "107/125 [========================>.....] - ETA: 4s - accuracy: 0.9158 - loss: 1.1225 - F1: nan - EM: 0.0000e+00 - lr: 8.0504e-04\n",
            "Epoch 00107: Learning rate is 0.0009.\n",
            "108/125 [========================>.....] - ETA: 3s - accuracy: 0.9165 - loss: 1.1139 - F1: nan - EM: 0.0000e+00 - lr: 8.0592e-04\n",
            "Epoch 00108: Learning rate is 0.0009.\n",
            "109/125 [=========================>....] - ETA: 3s - accuracy: 0.9170 - loss: 1.1055 - F1: nan - EM: 0.0000e+00 - lr: 8.0678e-04\n",
            "Epoch 00109: Learning rate is 0.0009.\n",
            "110/125 [=========================>....] - ETA: 3s - accuracy: 0.9175 - loss: 1.0979 - F1: nan - EM: 0.0000e+00 - lr: 8.0763e-04\n",
            "Epoch 00110: Learning rate is 0.0009.\n",
            "111/125 [=========================>....] - ETA: 3s - accuracy: 0.9181 - loss: 1.0894 - F1: nan - EM: 0.0000e+00 - lr: 8.0846e-04\n",
            "Epoch 00111: Learning rate is 0.0009.\n",
            "112/125 [=========================>....] - ETA: 2s - accuracy: 0.9187 - loss: 1.0812 - F1: nan - EM: 0.0000e+00 - lr: 8.0928e-04\n",
            "Epoch 00112: Learning rate is 0.0009.\n",
            "113/125 [==========================>...] - ETA: 2s - accuracy: 0.9192 - loss: 1.0734 - F1: nan - EM: 0.0000e+00 - lr: 8.1008e-04\n",
            "Epoch 00113: Learning rate is 0.0009.\n",
            "114/125 [==========================>...] - ETA: 2s - accuracy: 0.9198 - loss: 1.0657 - F1: nan - EM: 0.0000e+00 - lr: 8.1087e-04\n",
            "Epoch 00114: Learning rate is 0.0009.\n",
            "115/125 [==========================>...] - ETA: 2s - accuracy: 0.9203 - loss: 1.0578 - F1: nan - EM: 0.0000e+00 - lr: 8.1165e-04\n",
            "Epoch 00115: Learning rate is 0.0009.\n",
            "116/125 [==========================>...] - ETA: 1s - accuracy: 0.9209 - loss: 1.0498 - F1: nan - EM: 0.0000e+00 - lr: 8.1241e-04\n",
            "Epoch 00116: Learning rate is 0.0009.\n",
            "117/125 [===========================>..] - ETA: 1s - accuracy: 0.9214 - loss: 1.0421 - F1: nan - EM: 0.0000e+00 - lr: 8.1316e-04\n",
            "Epoch 00117: Learning rate is 0.0009.\n",
            "118/125 [===========================>..] - ETA: 1s - accuracy: 0.9219 - loss: 1.0345 - F1: nan - EM: 0.0000e+00 - lr: 8.1389e-04\n",
            "Epoch 00118: Learning rate is 0.0009.\n",
            "119/125 [===========================>..] - ETA: 1s - accuracy: 0.9223 - loss: 1.0279 - F1: nan - EM: 0.0000e+00 - lr: 8.1462e-04\n",
            "Epoch 00119: Learning rate is 0.0009.\n",
            "120/125 [===========================>..] - ETA: 1s - accuracy: 0.9228 - loss: 1.0211 - F1: nan - EM: 0.0000e+00 - lr: 8.1533e-04\n",
            "Epoch 00120: Learning rate is 0.0009.\n",
            "121/125 [============================>.] - ETA: 0s - accuracy: 0.9232 - loss: 1.0143 - F1: nan - EM: 0.0000e+00 - lr: 8.1603e-04\n",
            "Epoch 00121: Learning rate is 0.0009.\n",
            "122/125 [============================>.] - ETA: 0s - accuracy: 0.9236 - loss: 1.0087 - F1: nan - EM: 0.0000e+00 - lr: 8.1671e-04\n",
            "Epoch 00122: Learning rate is 0.0009.\n",
            "123/125 [============================>.] - ETA: 0s - accuracy: 0.9241 - loss: 1.0020 - F1: nan - EM: 0.0000e+00 - lr: 8.1739e-04\n",
            "Epoch 00123: Learning rate is 0.0009.\n",
            "124/125 [============================>.] - ETA: 0s - accuracy: 0.9245 - loss: 0.9959 - F1: nan - EM: 0.0000e+00 - lr: 8.1806e-04\n",
            "Epoch 00124: Learning rate is 0.0009.\n",
            "125/125 [==============================] - 64s 352ms/step - accuracy: 0.9250 - loss: 0.9891 - F1: nan - EM: 0.0000e+00 - lr: 8.1936e-04 - val_accuracy: 0.9344 - val_loss: 0.7570 - val_F1: 0.4976 - val_EM: 0.0000e+00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as final_layer_norm_layer_call_fn, final_layer_norm_layer_call_and_return_conditional_losses, dropout_24_layer_call_fn, dropout_24_layer_call_and_return_conditional_losses, final_layer_norm_layer_call_fn while saving (showing 5 of 1310). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/3\n",
            "\n",
            "Epoch 00125: Learning rate is 0.0009.\n",
            "  1/125 [..............................] - ETA: 33s - accuracy: 0.9792 - loss: 0.1944 - F1: 0.3788 - EM: 0.0000e+00 - lr: 9.0000e-04\n",
            "Epoch 00126: Learning rate is 0.0009.\n",
            "  2/125 [..............................] - ETA: 26s - accuracy: 0.9826 - loss: 0.1699 - F1: 0.4598 - EM: 0.0000e+00 - lr: 9.0000e-04\n",
            "Epoch 00127: Learning rate is 0.0009.\n",
            "  3/125 [..............................] - ETA: 26s - accuracy: 0.9846 - loss: 0.1591 - F1: 0.5460 - EM: 0.0000e+00 - lr: 9.0000e-04\n",
            "Epoch 00128: Learning rate is 0.0009.\n",
            "  4/125 [..............................] - ETA: 25s - accuracy: 0.9844 - loss: 0.1596 - F1: 0.5840 - EM: 0.0000e+00 - lr: 9.0000e-04\n",
            "Epoch 00129: Learning rate is 0.0009.\n",
            "  5/125 [>.............................] - ETA: 25s - accuracy: 0.9838 - loss: 0.1553 - F1: 0.5980 - EM: 0.0000e+00 - lr: 9.0000e-04\n",
            "Epoch 00130: Learning rate is 0.0009.\n",
            "  6/125 [>.............................] - ETA: 24s - accuracy: 0.9838 - loss: 0.1563 - F1: 0.6019 - EM: 0.0000e+00 - lr: 9.0000e-04\n",
            "Epoch 00131: Learning rate is 0.0009.\n",
            "  7/125 [>.............................] - ETA: 24s - accuracy: 0.9841 - loss: 0.1547 - F1: 0.5895 - EM: 0.0000e+00 - lr: 9.0000e-04\n",
            "Epoch 00132: Learning rate is 0.0009.\n",
            "  8/125 [>.............................] - ETA: 24s - accuracy: 0.9835 - loss: 0.1590 - F1: 0.5923 - EM: 0.0000e+00 - lr: 9.0000e-04\n",
            "Epoch 00133: Learning rate is 0.0009.\n",
            "  9/125 [=>............................] - ETA: 24s - accuracy: 0.9835 - loss: 0.1571 - F1: 0.5900 - EM: 0.0000e+00 - lr: 9.0000e-04\n",
            "Epoch 00134: Learning rate is 0.0009.\n",
            " 10/125 [=>............................] - ETA: 26s - accuracy: 0.9840 - loss: 0.1556 - F1: 0.5861 - EM: 0.0000e+00 - lr: 9.0000e-04\n",
            "Epoch 00135: Learning rate is 0.0009.\n",
            " 11/125 [=>............................] - ETA: 26s - accuracy: 0.9842 - loss: 0.1535 - F1: 0.5835 - EM: 0.0000e+00 - lr: 9.0000e-04\n",
            "Epoch 00136: Learning rate is 0.0009.\n",
            " 12/125 [=>............................] - ETA: 26s - accuracy: 0.9840 - loss: 0.1556 - F1: 0.5877 - EM: 0.0000e+00 - lr: 9.0000e-04\n",
            "Epoch 00137: Learning rate is 0.0009.\n",
            " 13/125 [==>...........................] - ETA: 25s - accuracy: 0.9827 - loss: 0.1654 - F1: 0.5726 - EM: 0.0000e+00 - lr: 9.0000e-04\n",
            "Epoch 00138: Learning rate is 0.0009.\n",
            " 14/125 [==>...........................] - ETA: 25s - accuracy: 0.9830 - loss: 0.1644 - F1: 0.5842 - EM: 0.0000e+00 - lr: 9.0000e-04\n",
            "Epoch 00139: Learning rate is 0.0008.\n",
            " 15/125 [==>...........................] - ETA: 25s - accuracy: 0.9829 - loss: 0.1693 - F1: 0.5925 - EM: 0.0000e+00 - lr: 8.9667e-04\n",
            "Epoch 00140: Learning rate is 0.0008.\n",
            " 16/125 [==>...........................] - ETA: 25s - accuracy: 0.9829 - loss: 0.1706 - F1: 0.5952 - EM: 0.0000e+00 - lr: 8.9375e-04\n",
            "Epoch 00141: Learning rate is 0.0008.\n",
            " 17/125 [===>..........................] - ETA: 24s - accuracy: 0.9833 - loss: 0.1693 - F1: 0.5947 - EM: 0.0000e+00 - lr: 8.9118e-04\n",
            "Epoch 00142: Learning rate is 0.0008.\n",
            " 18/125 [===>..........................] - ETA: 24s - accuracy: 0.9835 - loss: 0.1670 - F1: 0.6053 - EM: 0.0000e+00 - lr: 8.8889e-04\n",
            "Epoch 00143: Learning rate is 0.0008.\n",
            " 19/125 [===>..........................] - ETA: 24s - accuracy: 0.9836 - loss: 0.1651 - F1: 0.6100 - EM: 0.0000e+00 - lr: 8.8684e-04\n",
            "Epoch 00144: Learning rate is 0.0008.\n",
            " 20/125 [===>..........................] - ETA: 24s - accuracy: 0.9837 - loss: 0.1638 - F1: 0.6169 - EM: 0.0000e+00 - lr: 8.8500e-04\n",
            "Epoch 00145: Learning rate is 0.0008.\n",
            " 21/125 [====>.........................] - ETA: 23s - accuracy: 0.9839 - loss: 0.1620 - F1: 0.6178 - EM: 0.0000e+00 - lr: 8.8333e-04\n",
            "Epoch 00146: Learning rate is 0.0008.\n",
            " 22/125 [====>.........................] - ETA: 23s - accuracy: 0.9834 - loss: 0.1648 - F1: 0.6126 - EM: 0.0000e+00 - lr: 8.8182e-04\n",
            "Epoch 00147: Learning rate is 0.0008.\n",
            " 23/125 [====>.........................] - ETA: 23s - accuracy: 0.9835 - loss: 0.1626 - F1: 0.6124 - EM: 0.0000e+00 - lr: 8.8043e-04\n",
            "Epoch 00148: Learning rate is 0.0008.\n",
            " 24/125 [====>.........................] - ETA: 23s - accuracy: 0.9836 - loss: 0.1628 - F1: 0.6130 - EM: 0.0000e+00 - lr: 8.7917e-04\n",
            "Epoch 00149: Learning rate is 0.0008.\n",
            " 25/125 [=====>........................] - ETA: 23s - accuracy: 0.9836 - loss: 0.1629 - F1: 0.6185 - EM: 0.0000e+00 - lr: 8.7800e-04\n",
            "Epoch 00150: Learning rate is 0.0008.\n",
            " 26/125 [=====>........................] - ETA: 22s - accuracy: 0.9834 - loss: 0.1650 - F1: 0.6214 - EM: 0.0000e+00 - lr: 8.7692e-04\n",
            "Epoch 00151: Learning rate is 0.0008.\n",
            " 27/125 [=====>........................] - ETA: 22s - accuracy: 0.9835 - loss: 0.1633 - F1: 0.6237 - EM: 0.0000e+00 - lr: 8.7593e-04\n",
            "Epoch 00152: Learning rate is 0.0008.\n",
            " 28/125 [=====>........................] - ETA: 22s - accuracy: 0.9836 - loss: 0.1645 - F1: 0.6250 - EM: 0.0000e+00 - lr: 8.7500e-04\n",
            "Epoch 00153: Learning rate is 0.0008.\n",
            " 29/125 [=====>........................] - ETA: 22s - accuracy: 0.9837 - loss: 0.1635 - F1: 0.6233 - EM: 0.0000e+00 - lr: 8.7414e-04\n",
            "Epoch 00154: Learning rate is 0.0008.\n",
            " 30/125 [======>.......................] - ETA: 21s - accuracy: 0.9838 - loss: 0.1621 - F1: 0.6267 - EM: 0.0000e+00 - lr: 8.7333e-04\n",
            "Epoch 00155: Learning rate is 0.0008.\n",
            " 31/125 [======>.......................] - ETA: 21s - accuracy: 0.9841 - loss: 0.1601 - F1: 0.6309 - EM: 0.0000e+00 - lr: 8.7258e-04\n",
            "Epoch 00156: Learning rate is 0.0008.\n",
            " 32/125 [======>.......................] - ETA: 21s - accuracy: 0.9841 - loss: 0.1591 - F1: 0.6302 - EM: 0.0000e+00 - lr: 8.7187e-04\n",
            "Epoch 00157: Learning rate is 0.0008.\n",
            " 33/125 [======>.......................] - ETA: 21s - accuracy: 0.9843 - loss: 0.1579 - F1: 0.6241 - EM: 0.0000e+00 - lr: 8.7121e-04\n",
            "Epoch 00158: Learning rate is 0.0008.\n",
            " 34/125 [=======>......................] - ETA: 20s - accuracy: 0.9843 - loss: 0.1582 - F1: 0.6247 - EM: 0.0000e+00 - lr: 8.7059e-04\n",
            "Epoch 00159: Learning rate is 0.0008.\n",
            " 35/125 [=======>......................] - ETA: 20s - accuracy: 0.9843 - loss: 0.1576 - F1: 0.6211 - EM: 0.0000e+00 - lr: 8.7000e-04\n",
            "Epoch 00160: Learning rate is 0.0008.\n",
            " 36/125 [=======>......................] - ETA: 20s - accuracy: 0.9843 - loss: 0.1577 - F1: 0.6213 - EM: 0.0000e+00 - lr: 8.6944e-04\n",
            "Epoch 00161: Learning rate is 0.0008.\n",
            " 37/125 [=======>......................] - ETA: 20s - accuracy: 0.9845 - loss: 0.1559 - F1: 0.6245 - EM: 0.0000e+00 - lr: 8.6892e-04\n",
            "Epoch 00162: Learning rate is 0.0008.\n",
            " 38/125 [========>.....................] - ETA: 20s - accuracy: 0.9843 - loss: 0.1561 - F1: 0.6272 - EM: 0.0000e+00 - lr: 8.6842e-04\n",
            "Epoch 00163: Learning rate is 0.0008.\n",
            " 39/125 [========>.....................] - ETA: 19s - accuracy: 0.9845 - loss: 0.1551 - F1: 0.6272 - EM: 0.0000e+00 - lr: 8.6795e-04\n",
            "Epoch 00164: Learning rate is 0.0008.\n",
            " 40/125 [========>.....................] - ETA: 19s - accuracy: 0.9847 - loss: 0.1541 - F1: nan - EM: 0.0000e+00 - lr: 8.6750e-04   \n",
            "Epoch 00165: Learning rate is 0.0008.\n",
            " 41/125 [========>.....................] - ETA: 19s - accuracy: 0.9842 - loss: 0.1559 - F1: nan - EM: 0.0000e+00 - lr: 8.6707e-04\n",
            "Epoch 00166: Learning rate is 0.0008.\n",
            " 42/125 [=========>....................] - ETA: 19s - accuracy: 0.9843 - loss: 0.1544 - F1: nan - EM: 0.0000e+00 - lr: 8.6667e-04\n",
            "Epoch 00167: Learning rate is 0.0008.\n",
            " 43/125 [=========>....................] - ETA: 18s - accuracy: 0.9844 - loss: 0.1539 - F1: nan - EM: 0.0000e+00 - lr: 8.6628e-04\n",
            "Epoch 00168: Learning rate is 0.0008.\n",
            " 44/125 [=========>....................] - ETA: 18s - accuracy: 0.9846 - loss: 0.1530 - F1: nan - EM: 0.0000e+00 - lr: 8.6591e-04\n",
            "Epoch 00169: Learning rate is 0.0008.\n",
            " 45/125 [=========>....................] - ETA: 18s - accuracy: 0.9847 - loss: 0.1528 - F1: nan - EM: 0.0000e+00 - lr: 8.6556e-04\n",
            "Epoch 00170: Learning rate is 0.0008.\n",
            " 46/125 [==========>...................] - ETA: 18s - accuracy: 0.9847 - loss: 0.1536 - F1: nan - EM: 0.0000e+00 - lr: 8.6522e-04\n",
            "Epoch 00171: Learning rate is 0.0008.\n",
            " 47/125 [==========>...................] - ETA: 17s - accuracy: 0.9849 - loss: 0.1518 - F1: nan - EM: 0.0000e+00 - lr: 8.6489e-04\n",
            "Epoch 00172: Learning rate is 0.0008.\n",
            " 48/125 [==========>...................] - ETA: 17s - accuracy: 0.9849 - loss: 0.1520 - F1: nan - EM: 0.0000e+00 - lr: 8.6458e-04\n",
            "Epoch 00173: Learning rate is 0.0008.\n",
            " 49/125 [==========>...................] - ETA: 17s - accuracy: 0.9849 - loss: 0.1516 - F1: nan - EM: 0.0000e+00 - lr: 8.6327e-04\n",
            "Epoch 00174: Learning rate is 0.0008.\n",
            " 50/125 [===========>..................] - ETA: 17s - accuracy: 0.9850 - loss: 0.1510 - F1: nan - EM: 0.0000e+00 - lr: 8.6200e-04\n",
            "Epoch 00175: Learning rate is 0.0008.\n",
            " 51/125 [===========>..................] - ETA: 16s - accuracy: 0.9851 - loss: 0.1498 - F1: nan - EM: 0.0000e+00 - lr: 8.6078e-04\n",
            "Epoch 00176: Learning rate is 0.0008.\n",
            " 52/125 [===========>..................] - ETA: 16s - accuracy: 0.9853 - loss: 0.1477 - F1: nan - EM: 0.0000e+00 - lr: 8.5962e-04\n",
            "Epoch 00177: Learning rate is 0.0008.\n",
            " 53/125 [===========>..................] - ETA: 16s - accuracy: 0.9853 - loss: 0.1487 - F1: nan - EM: 0.0000e+00 - lr: 8.5849e-04\n",
            "Epoch 00178: Learning rate is 0.0008.\n",
            " 54/125 [===========>..................] - ETA: 16s - accuracy: 0.9852 - loss: 0.1487 - F1: nan - EM: 0.0000e+00 - lr: 8.5741e-04\n",
            "Epoch 00179: Learning rate is 0.0008.\n",
            " 55/125 [============>.................] - ETA: 16s - accuracy: 0.9854 - loss: 0.1475 - F1: nan - EM: 0.0000e+00 - lr: 8.5636e-04\n",
            "Epoch 00180: Learning rate is 0.0008.\n",
            " 56/125 [============>.................] - ETA: 15s - accuracy: 0.9854 - loss: 0.1478 - F1: nan - EM: 0.0000e+00 - lr: 8.5536e-04\n",
            "Epoch 00181: Learning rate is 0.0008.\n",
            " 57/125 [============>.................] - ETA: 15s - accuracy: 0.9853 - loss: 0.1483 - F1: nan - EM: 0.0000e+00 - lr: 8.5439e-04\n",
            "Epoch 00182: Learning rate is 0.0008.\n",
            " 58/125 [============>.................] - ETA: 15s - accuracy: 0.9855 - loss: 0.1471 - F1: nan - EM: 0.0000e+00 - lr: 8.5345e-04\n",
            "Epoch 00183: Learning rate is 0.0008.\n",
            " 59/125 [=============>................] - ETA: 15s - accuracy: 0.9856 - loss: 0.1457 - F1: nan - EM: 0.0000e+00 - lr: 8.5254e-04\n",
            "Epoch 00184: Learning rate is 0.0008.\n",
            " 60/125 [=============>................] - ETA: 14s - accuracy: 0.9858 - loss: 0.1453 - F1: nan - EM: 0.0000e+00 - lr: 8.5167e-04\n",
            "Epoch 00185: Learning rate is 0.0008.\n",
            " 61/125 [=============>................] - ETA: 14s - accuracy: 0.9859 - loss: 0.1446 - F1: nan - EM: 0.0000e+00 - lr: 8.5082e-04\n",
            "Epoch 00186: Learning rate is 0.0008.\n",
            " 62/125 [=============>................] - ETA: 14s - accuracy: 0.9860 - loss: 0.1431 - F1: nan - EM: 0.0000e+00 - lr: 8.5000e-04\n",
            "Epoch 00187: Learning rate is 0.0008.\n",
            " 63/125 [==============>...............] - ETA: 14s - accuracy: 0.9860 - loss: 0.1438 - F1: nan - EM: 0.0000e+00 - lr: 8.4921e-04\n",
            "Epoch 00188: Learning rate is 0.0008.\n",
            " 64/125 [==============>...............] - ETA: 14s - accuracy: 0.9860 - loss: 0.1436 - F1: nan - EM: 0.0000e+00 - lr: 8.4844e-04\n",
            "Epoch 00189: Learning rate is 0.0008.\n",
            " 65/125 [==============>...............] - ETA: 13s - accuracy: 0.9862 - loss: 0.1426 - F1: nan - EM: 0.0000e+00 - lr: 8.4769e-04\n",
            "Epoch 00190: Learning rate is 0.0008.\n",
            " 66/125 [==============>...............] - ETA: 13s - accuracy: 0.9863 - loss: 0.1417 - F1: nan - EM: 0.0000e+00 - lr: 8.4697e-04\n",
            "Epoch 00191: Learning rate is 0.0008.\n",
            " 67/125 [===============>..............] - ETA: 13s - accuracy: 0.9863 - loss: 0.1411 - F1: nan - EM: 0.0000e+00 - lr: 8.4627e-04\n",
            "Epoch 00192: Learning rate is 0.0008.\n",
            " 68/125 [===============>..............] - ETA: 13s - accuracy: 0.9865 - loss: 0.1398 - F1: nan - EM: 0.0000e+00 - lr: 8.4559e-04\n",
            "Epoch 00193: Learning rate is 0.0008.\n",
            " 69/125 [===============>..............] - ETA: 12s - accuracy: 0.9867 - loss: 0.1388 - F1: nan - EM: 0.0000e+00 - lr: 8.4493e-04\n",
            "Epoch 00194: Learning rate is 0.0008.\n",
            " 70/125 [===============>..............] - ETA: 12s - accuracy: 0.9867 - loss: 0.1386 - F1: nan - EM: 0.0000e+00 - lr: 8.4429e-04\n",
            "Epoch 00195: Learning rate is 0.0008.\n",
            " 71/125 [================>.............] - ETA: 12s - accuracy: 0.9868 - loss: 0.1375 - F1: nan - EM: 0.0000e+00 - lr: 8.4366e-04\n",
            "Epoch 00196: Learning rate is 0.0008.\n",
            " 72/125 [================>.............] - ETA: 12s - accuracy: 0.9869 - loss: 0.1373 - F1: nan - EM: 0.0000e+00 - lr: 8.4306e-04\n",
            "Epoch 00197: Learning rate is 0.0008.\n",
            " 73/125 [================>.............] - ETA: 11s - accuracy: 0.9870 - loss: 0.1364 - F1: nan - EM: 0.0000e+00 - lr: 8.4247e-04\n",
            "Epoch 00198: Learning rate is 0.0008.\n",
            " 74/125 [================>.............] - ETA: 11s - accuracy: 0.9871 - loss: 0.1352 - F1: nan - EM: 0.0000e+00 - lr: 8.4189e-04\n",
            "Epoch 00199: Learning rate is 0.0008.\n",
            " 75/125 [=================>............] - ETA: 11s - accuracy: 0.9872 - loss: 0.1348 - F1: nan - EM: 0.0000e+00 - lr: 8.4133e-04\n",
            "Epoch 00200: Learning rate is 0.0008.\n",
            " 76/125 [=================>............] - ETA: 11s - accuracy: 0.9873 - loss: 0.1341 - F1: nan - EM: 0.0000e+00 - lr: 8.4079e-04\n",
            "Epoch 00201: Learning rate is 0.0008.\n",
            " 77/125 [=================>............] - ETA: 11s - accuracy: 0.9873 - loss: 0.1338 - F1: nan - EM: 0.0000e+00 - lr: 8.4026e-04\n",
            "Epoch 00202: Learning rate is 0.0008.\n",
            " 78/125 [=================>............] - ETA: 10s - accuracy: 0.9874 - loss: 0.1328 - F1: nan - EM: 0.0000e+00 - lr: 8.3974e-04\n",
            "Epoch 00203: Learning rate is 0.0008.\n",
            " 79/125 [=================>............] - ETA: 10s - accuracy: 0.9875 - loss: 0.1320 - F1: nan - EM: 0.0000e+00 - lr: 8.3924e-04\n",
            "Epoch 00204: Learning rate is 0.0008.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-e4ea83862d45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     model.fit(tf_train_ds, epochs=epochs, steps_per_epoch=steps, callbacks=callbacks, \n\u001b[0;32m----> 5\u001b[0;31m               validation_data=tf_valid_ds, validation_steps=valid_steps,verbose=1)\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqxSSq06bZct"
      },
      "source": [
        "if load_model:\n",
        "    model.load_weights(save_path+'/tf_model.h5')\n",
        "    print(f'Model loaded from {save_path}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjsHiiSldUFa"
      },
      "source": [
        "#### Predict & Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBfLJvUuSamq"
      },
      "source": [
        "def batch_predict(ds,model,tokenizer):\n",
        "    preds = []\n",
        "\n",
        "    with tqdm(total=batch_size*len(list(ds.as_numpy_iterator()))) as bar:\n",
        "        for batch in ds:\n",
        "            input_ids = batch['input_ids']\n",
        "            output = model.generate(input_ids)\n",
        "\n",
        "            for i in range(output.shape[0]):\n",
        "                single_pred = tokenizer.decode(output[i])\n",
        "                single_pred = single_pred.replace('<pad>','')\n",
        "                single_pred = single_pred.replace('</s>','')\n",
        "                single_pred = single_pred.strip()\n",
        "                single_pred = re.sub(r'(\\d)\\s+(\\d)', r'\\1\\2', single_pred)\n",
        "                preds.append(single_pred)\n",
        "                bar.update(1)\n",
        "    return preds\n",
        "\n",
        "def evaluate(df):\n",
        "    EM = []\n",
        "    F1 = []\n",
        "    \n",
        "    if dataset == 'drop': \n",
        "        col = 'answers_spans'\n",
        "        gold_col = 'spans'\n",
        "    else:\n",
        "        col = 'answers'\n",
        "        gold_col = 'text'\n",
        "    for predicted,gold in tqdm(zip(df['predicted'],df[col])):\n",
        "\n",
        "        best_EM = 0\n",
        "        best_F1 = 0\n",
        "\n",
        "        for potential_answer in gold[gold_col]:\n",
        "            metrics = drop_eval.get_metrics(predicted=predicted,gold=potential_answer)\n",
        "\n",
        "            if metrics[1] > best_F1:\n",
        "                best_EM = metrics[0]\n",
        "                best_F1 = metrics[1]\n",
        "\n",
        "        EM.append(best_EM)\n",
        "        F1.append(best_F1)\n",
        "        \n",
        "    df['EM'] = EM\n",
        "    df['F1'] = F1\n",
        "    \n",
        "    print('Exact Match: {:0.4f}, F1: {:0.4f}'.format(df.EM.mean(),df.F1.mean()))\n",
        "    return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RE5OUFOASamr"
      },
      "source": [
        "if predict_train:\n",
        "    print('Making Train Predictions...')\n",
        "    preds = batch_predict(ds=tf_train_ds,model=model,tokenizer=tokenizer)\n",
        "    train_df = train_dataset.to_pandas()\n",
        "    assert len(train_df) == len(preds), \"count mismatch, something went wrong\"\n",
        "    train_df['predicted'] = preds\n",
        "    print('Evaluating Train Predictions...')\n",
        "    train_df = evaluate(train_df)\n",
        "    if save_results:\n",
        "        train_df.to_pickle(results_dir+'{}_train'.format(dataset)+datetime.datetime.now().strftime('%H%M-%h%d')+'.pkl')\n",
        "        print('results for predictions on the training data saved to:\\n',results_dir)\n",
        "    \n",
        "if predict_dev:\n",
        "    print('Making Dev Predictions...')\n",
        "    preds = batch_predict(ds=tf_valid_ds,model=model,tokenizer=tokenizer)\n",
        "    valid_df = valid_dataset.to_pandas()\n",
        "    valid_df['predicted'] = preds\n",
        "    assert len(valid_df) == len(preds), \"count mismatch, something went wrong\"\n",
        "    print('Evaluating Dev Predictions...')\n",
        "    valid_df = evaluate(valid_df)\n",
        "    if save_results:\n",
        "        valid_df.to_pickle(results_dir+'{}_validation'.format(dataset)+datetime.datetime.now().strftime('%H%M-%h%d')+'.pkl')\n",
        "        print('results for predictions on the validation data saved to:\\n',results_dir)    \n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fVrboarSamr"
      },
      "source": [
        "valid_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxtJDZ5nSamr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKbjRaoeSamr"
      },
      "source": [
        "            __PIECEWISE_STEP = (MAX_LR - END_LR)/NUM_BOUNDARIES\n",
        "            __VALUES = list(np.arange(MAX_LR-__PIECEWISE_STEP,END_LR,__PIECEWISE_STEP))valid_df[['query_id','passage','question','answers_spans','predicted','EM','F1']].sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_BmeVI5Samr"
      },
      "source": [
        "def print_example(query_id,df):\n",
        "    print('question: ',df.loc[df.query_id == query_id,'question'].iloc[0])\n",
        "    print('passage: ',df.loc[df.query_id == query_id,'passage'].iloc[0])\n",
        "    print('\\npredicted answer: ',df.loc[df.query_id == query_id,'predicted'].iloc[0])\n",
        "    print('True answers: ',df.loc[df.query_id == query_id,'answers_spans'].iloc[0])\n",
        "    print('F1 score: ',df.loc[df.query_id == query_id,'F1'].iloc[0])\n",
        "    print('EM score: ',df.loc[df.query_id == query_id,'EM'].iloc[0])\n",
        "    \n",
        "    \n",
        "query_id = '0686d1f9-4a8e-4031-b665-49d425afb777'\n",
        "print_example(query_id,valid_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgoF5TXfSamr"
      },
      "source": [
        "query_id = '86dd1721-6bf4-45fa-b01e-de47e4f7301d'\n",
        "print_example(query_id,valid_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9Hytsj2Sams"
      },
      "source": [
        "query_id = 'ad19857f-cd76-4d01-ba29-1a589cfee053'\n",
        "print_example(query_id,valid_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ws2ChdQbSams"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAoN9D0gSams"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxwcC4XvSams"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaLAYAscSams"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTAHYYSdSams"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZtM8mIuSams"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GepQY8qgSams"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiMekPrISams"
      },
      "source": [
        "# Cells to explore the model a bit\n",
        "\n",
        "(make these raw cells into code cells to explore)"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "yMewpOnvSams"
      },
      "source": [
        "print('embeddings layer: ', model.layers[0].weights[0].name)\n",
        "print('shape: ',model.layers[0].weights[0].shape)"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "0CPfJD4QSamt"
      },
      "source": [
        "for x in range(len(model.layers[2].weights)):\n",
        "    weights = model.layers[2].weights[x]\n",
        "    print('layer: ',x)\n",
        "    print('   - '+weights.name)\n",
        "    print('shape: ',weights.shape)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "ueB23-hNSamt"
      },
      "source": [
        "for x in range(len(model.layers[1].weights)):\n",
        "    weights = model.layers[1].weights[x]\n",
        "    print('layer: ',x)\n",
        "    print('   - '+weights.name)\n",
        "    print('shape: ',weights.shape)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "HH9f3oA-Samt"
      },
      "source": [
        "len(model.layers[1].weights)"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "Uk3nFlB4Samt"
      },
      "source": [
        "model.layers[0].weights"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "hKhReDNxSamt"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "xjYIIzT8Samt"
      },
      "source": [
        "for item in tf_valid_ds:\n",
        "    inputs = item\n",
        "    outputs = model(inputs)\n",
        "    break"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "2BGvOBsrSamt"
      },
      "source": [
        "inputs.keys()"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "S6FKsphNSamt"
      },
      "source": [
        "type(outputs)"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "MwhDfHSZSamt"
      },
      "source": [
        "outputs.keys()"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "W422WXSvSamu"
      },
      "source": [
        "tokenizer.vocab_size"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "9NwWnVWvSamu"
      },
      "source": [
        "outputs['logits']"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "5xd1edNdSamu"
      },
      "source": [
        ""
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "T5_DROP_training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a983faae6abd456fb82c753729b5039a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_82cb020caa764e059e3741cbb2877906",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c562383304874cf4adb5cb7ca8571e2d",
              "IPY_MODEL_a7e723c7159d434f9f374cc0c1777e1f"
            ]
          }
        },
        "82cb020caa764e059e3741cbb2877906": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c562383304874cf4adb5cb7ca8571e2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f0403e48448e46159e97f09d87b2e713",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bac15fab3d5f41f8ad466f143d2c2617"
          }
        },
        "a7e723c7159d434f9f374cc0c1777e1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_750721cbc15347aa99c01fcb019c47c6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4/4 [00:00&lt;00:00, 14.57ex/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1303280f42534bc8823f3c9762941dcf"
          }
        },
        "f0403e48448e46159e97f09d87b2e713": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bac15fab3d5f41f8ad466f143d2c2617": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "750721cbc15347aa99c01fcb019c47c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1303280f42534bc8823f3c9762941dcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c632f46a7e624025a69069269ec681b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_df85c7a4e63f4b8ebf4d6918995e33f8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5ea0152d98814971863b748454d1182f",
              "IPY_MODEL_67852bf520714ddbb818b86ed241a367"
            ]
          }
        },
        "df85c7a4e63f4b8ebf4d6918995e33f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5ea0152d98814971863b748454d1182f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_564cb566fe59403c90b11046e01dc07e",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_263ce715956f4a2a817933ea2abe3437"
          }
        },
        "67852bf520714ddbb818b86ed241a367": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fb4273e0463549988bff4f6d46b16714",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4/4 [00:00&lt;00:00, 37.21ex/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6ac7308c6f3a40c0bde1117f92b07ae9"
          }
        },
        "564cb566fe59403c90b11046e01dc07e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "263ce715956f4a2a817933ea2abe3437": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fb4273e0463549988bff4f6d46b16714": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6ac7308c6f3a40c0bde1117f92b07ae9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omarkapur-mids/w266-project/blob/phillip/T5_DROP_F1_EM_metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47S35TVwgIeG"
      },
      "source": [
        "# Using T5 on DROP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUEAtP-egIeI"
      },
      "source": [
        "#### Package installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKtbsamsidKs"
      },
      "source": [
        "# !pip install --quiet transformers\n",
        "# !pip install --quiet sentencepiece\n",
        "# !pip install --quiet wget\n",
        "# !pip install --quiet datasets\n",
        "# #!pip install --quiet ipywidgets\n",
        "# #!pip install --quiet tensorflow\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAC1x24qgIeJ"
      },
      "source": [
        "#### check gpu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyexkfSDij_3"
      },
      "source": [
        "# !nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2EQCkymgIeK"
      },
      "source": [
        "#### Download allennlp drop_eval module\n",
        "\n",
        "https://github.com/allenai/allennlp-reading-comprehension/blob/master/allennlp_rc/eval/drop_eval.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xME0lIMVgIeK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8994971-c8ae-4ab2-d6d8-9327ac67edcb"
      },
      "source": [
        "!python -m wget https://raw.githubusercontent.com/allenai/allennlp-reading-comprehension/master/allennlp_rc/eval/drop_eval.py -o drop_eval.py"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Saved under drop_eval (58).py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFbTSPkUgIeK"
      },
      "source": [
        "#### set directories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01Kd-pE5gIeK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "281d22c9-c271-4cc6-f3f0-233dfc737a43"
      },
      "source": [
        "!mkdir data\n",
        "\n",
        "data_dir = \"./data\"\n",
        "log_dir = f\"{data_dir}/experiments/t5/logs\"\n",
        "save_path = f\"{data_dir}/experiments/t5/models\"\n",
        "cache_path_train = f\"{data_dir}/cache/t5.train\"\n",
        "cache_path_test = f\"{data_dir}/cache/t5.test\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-ADai1tgIeL"
      },
      "source": [
        "#### load packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4w1GhOTfgIeM"
      },
      "source": [
        "from transformers import T5Tokenizer, TFT5ForConditionalGeneration\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import drop_eval\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from datasets import Dataset, load_dataset\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "\n",
        "run_toy = True"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "II_jw-2amWe4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dc29352-80ba-4128-8658-9ae03b713dec"
      },
      "source": [
        "drop_eval.get_metrics(predicted=[\"1, 2 3 4?\", \"this\"],gold=[\"1, 2 3 4?\",\"0\"])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 0.5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxNvIw-x6wEp"
      },
      "source": [
        "# output_1 = model.predict(tf_train_ds.take(1))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1VZafTU7XZD"
      },
      "source": [
        "# for x in output_1:\n",
        "#   print(x)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mLD5Ri98vLa"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5goXuHR1gIeM"
      },
      "source": [
        "#### Define model class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nB2yXTd-t6sw"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdP6lcFegIeM"
      },
      "source": [
        "class T5forDrop(TFT5ForConditionalGeneration):\n",
        "    def __init__(self, *args, log_dir=None, cache_dir= None, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.loss_tracker= tf.keras.metrics.Mean(name='loss')\n",
        "        self.F1_tracker= tf.keras.metrics.Mean(name='F1')\n",
        "        self.EM_tracker= tf.keras.metrics.Mean(name='EM')        \n",
        "\n",
        "    \n",
        "    @tf.function\n",
        "    def train_step(self, data):\n",
        "        x = data\n",
        "        y_true = x[\"labels\"]\n",
        "        with tf.GradientTape() as tape:\n",
        "            outputs = self(x, training=True)\n",
        "            logits = outputs['logits']\n",
        "            y_pred = tf.math.argmax(tf.nn.softmax(logits,axis=2), axis = 2, output_type=tf.int32)\n",
        "            loss = tf.reduce_mean(outputs['loss'])            \n",
        "            grads = tape.gradient(loss, self.trainable_variables)\n",
        "\n",
        "        # Calculate F1 and EM Metrics\n",
        "        # Create a word mask to not count the padding/sentence tokens\n",
        "        recall_word_mask = tf.math.logical_and(\n",
        "                    tf.math.not_equal(y_true,0)\n",
        "                    ,tf.math.not_equal(y_true,1))\n",
        "\n",
        "        precision_word_mask = tf.math.logical_and(\n",
        "                    tf.math.not_equal(y_pred,0)\n",
        "                    ,tf.math.not_equal(y_pred,1))\n",
        "\n",
        "        # match the tokens\n",
        "        match_token = tf.math.equal(y_true,y_pred)\n",
        "        recall_match = tf.math.logical_and(match_token,recall_word_mask) \n",
        "        precision_match = tf.math.logical_and(match_token,precision_word_mask) \n",
        "\n",
        "        # calculate score\n",
        "        precision_array = tf.math.reduce_sum(tf.cast(precision_match, tf.int32) ,axis=1)/tf.math.reduce_sum(tf.cast(precision_word_mask, tf.int32) ,axis=1)\n",
        "        recall_array = tf.math.reduce_sum(tf.cast(recall_match, tf.int32) ,axis=1)/tf.math.reduce_sum(tf.cast(recall_word_mask, tf.int32) ,axis=1)\n",
        "\n",
        "        P = tf.math.reduce_mean(precision_array)\n",
        "        R = tf.math.reduce_mean(recall_array)\n",
        "\n",
        "        EM = tf.math.reduce_mean(\n",
        "            tf.cast(tf.math.logical_and(\n",
        "                    tf.math.equal(precision_array,1)\n",
        "                    ,tf.math.equal(recall_array,1)), tf.int32))\n",
        "        F1 = 2*(P*R)/(P+R)\n",
        "        \n",
        "        '''\n",
        "        Note, since FP and FN are the same, \n",
        "        the F1 score is the same as Precision, Recall, \n",
        "        which is the average \"match\" score\n",
        "        '''\n",
        "\n",
        "        y_true = tf.reshape(y_true, [-1, 1])\n",
        "\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
        "        lr = self.optimizer._decayed_lr(tf.float32)\n",
        "\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        self.F1_tracker.update_state(F1)   \n",
        "        self.EM_tracker.update_state(EM)           \n",
        "        self.compiled_metrics.update_state(y_true, logits)\n",
        "        metrics = {m.name: m.result() for m in self.metrics}\n",
        "        metrics.update({'lr': lr})\n",
        "        \n",
        "        return metrics\n",
        "\n",
        "    def test_step(self, data):\n",
        "        x = data\n",
        "        y_true = x[\"labels\"]\n",
        "        outputs = self(x, training=True)\n",
        "        logits = outputs['logits']\n",
        "        y_pred = tf.math.argmax(tf.nn.softmax(logits,axis=2), axis = 2, output_type=tf.int32)\n",
        "        loss = tf.reduce_mean(outputs['loss'])      \n",
        "        \n",
        "        # Calculate F1 and EM Metrics\n",
        "        # Create a word mask to not count the padding/sentence tokens\n",
        "        recall_word_mask = tf.math.logical_and(\n",
        "                    tf.math.not_equal(y_true,0)\n",
        "                    ,tf.math.not_equal(y_true,1))\n",
        "\n",
        "        precision_word_mask = tf.math.logical_and(\n",
        "                    tf.math.not_equal(y_pred,0)\n",
        "                    ,tf.math.not_equal(y_pred,1))\n",
        "\n",
        "        # match the tokens\n",
        "        match_token = tf.math.equal(y_true,y_pred)\n",
        "        recall_match = tf.math.logical_and(match_token,recall_word_mask) \n",
        "        precision_match = tf.math.logical_and(match_token,precision_word_mask) \n",
        "\n",
        "        # calculate score\n",
        "        precision_array = tf.math.reduce_sum(tf.cast(precision_match, tf.int32) ,axis=1)/tf.math.reduce_sum(tf.cast(precision_word_mask, tf.int32) ,axis=1)\n",
        "        recall_array = tf.math.reduce_sum(tf.cast(recall_match, tf.int32) ,axis=1)/tf.math.reduce_sum(tf.cast(recall_word_mask, tf.int32) ,axis=1)\n",
        "\n",
        "        P = tf.math.reduce_mean(precision_array)\n",
        "        R = tf.math.reduce_mean(recall_array)\n",
        "\n",
        "        EM = tf.math.reduce_mean(\n",
        "            tf.cast(tf.math.logical_and(\n",
        "                    tf.math.equal(precision_array,1)\n",
        "                    ,tf.math.equal(recall_array,1)), tf.int32))\n",
        "        F1 = 2*(P*R)/(P+R)\n",
        "\n",
        "        y_true = tf.reshape(y_true, [-1, 1])\n",
        "\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        self.F1_tracker.update_state(F1)   \n",
        "        self.EM_tracker.update_state(EM)           \n",
        "        self.compiled_metrics.update_state(y_true, logits)\n",
        "        metrics = {m.name: m.result() for m in self.metrics}        \n",
        "        return metrics"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKryMW9HMtra"
      },
      "source": [
        "\n",
        "        EM = tf.math.reduce_mean(\n",
        "            tf.cast(tf.math.logical_and(\n",
        "                    tf.math.equal(precision_array,1)\n",
        "                    ,tf.math.equal(recall_array,1)), tf.int32))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esxfDrXO66Bz"
      },
      "source": [
        "#### Import model and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r08M8aqE65_M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e73a191f-2168-498c-af26-946028b2c948"
      },
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "model = T5forDrop.from_pretrained('t5-small')#,return_dict=True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing T5forDrop.\n",
            "\n",
            "All the layers of T5forDrop were initialized from the model checkpoint at t5-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5forDrop for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIt7bbI8gIeN"
      },
      "source": [
        "#### Import data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaxF09BQgIeO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a298368-ae75-4169-c79a-df8a016cac17"
      },
      "source": [
        "train_dataset_full = load_dataset('drop', split='train')\n",
        "valid_dataset_full = load_dataset('drop', split='validation')\n",
        "\n",
        "train_dataset_full.features"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using custom data configuration default\n",
            "Reusing dataset drop (/root/.cache/huggingface/datasets/drop/default/0.1.0/393cc04823935c1302a6a7e380cdbe9f452d37858ea276409787c983748eae25)\n",
            "Using custom data configuration default\n",
            "Reusing dataset drop (/root/.cache/huggingface/datasets/drop/default/0.1.0/393cc04823935c1302a6a7e380cdbe9f452d37858ea276409787c983748eae25)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answers_spans': Sequence(feature={'spans': Value(dtype='string', id=None), 'types': Value(dtype='string', id=None)}, length=-1, id=None),\n",
              " 'passage': Value(dtype='string', id=None),\n",
              " 'query_id': Value(dtype='string', id=None),\n",
              " 'question': Value(dtype='string', id=None),\n",
              " 'section_id': Value(dtype='string', id=None)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6lzFuWQgIeR"
      },
      "source": [
        "#### Reduce data to toy size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8G5S7GbgIeR"
      },
      "source": [
        "\n",
        "if run_toy:\n",
        "    toy_train_df = train_dataset_full.to_pandas()\n",
        "    toy_train_df = toy_train_df.head(4)\n",
        "\n",
        "    toy_valid_df = valid_dataset_full.to_pandas()\n",
        "    toy_valid_df = toy_valid_df.head(4)\n",
        "\n",
        "    train_dataset = Dataset.from_pandas(toy_train_df)\n",
        "    valid_dataset = Dataset.from_pandas(toy_valid_df)\n",
        "else:\n",
        "    train_dataset = train_dataset_full\n",
        "    valid_dataset = valid_dataset_full"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQ-KxBp4gIeR"
      },
      "source": [
        "#### check out one record"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZLH55jogIeS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a2297f4-e446-4bf4-bad3-b7c0eca842ec"
      },
      "source": [
        "data = next(iter(valid_dataset))\n",
        "print(\"Example data from the dataset: \\n\", data)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example data from the dataset: \n",
            " {'section_id': 'nfl_1184', 'query_id': 'f37e81fa-ef7b-4583-b671-762fc433faa9', 'passage': \" Hoping to rebound from their loss to the Patriots, the Raiders stayed at home for a Week 16 duel with the Houston Texans.  Oakland would get the early lead in the first quarter as quarterback JaMarcus Russell completed a 20-yard touchdown pass to rookie wide receiver Chaz Schilens.  The Texans would respond with fullback Vonta Leach getting a 1-yard touchdown run, yet the Raiders would answer with kicker Sebastian Janikowski getting a 33-yard and a 30-yard field goal.  Houston would tie the game in the second quarter with kicker Kris Brown getting a 53-yard and a 24-yard field goal. Oakland would take the lead in the third quarter with wide receiver Johnnie Lee Higgins catching a 29-yard touchdown pass from Russell, followed up by an 80-yard punt return for a touchdown.  The Texans tried to rally in the fourth quarter as Brown nailed a 40-yard field goal, yet the Raiders' defense would shut down any possible attempt.\", 'question': 'Who scored the first touchdown of the game?', 'answers_spans': {'spans': ['Chaz Schilens', 'JaMarcus Russell'], 'types': ['span', 'span']}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jo9sZK0bgIeS"
      },
      "source": [
        "#### set parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsgfPOqogIeS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0e58237-d6db-4bbe-8c6e-428494d97edf"
      },
      "source": [
        "warmup_steps = 10 #1e4\n",
        "batch_size = 4\n",
        "encoder_max_len = 250\n",
        "decoder_max_len = 54\n",
        "buffer_size = 1000\n",
        "ntrain = len(train_dataset)\n",
        "nvalid = len(valid_dataset)\n",
        "steps = int(np.ceil(ntrain/batch_size))\n",
        "valid_steps = int(np.ceil(nvalid/batch_size))\n",
        "print(\"Total Steps: \", steps)\n",
        "print(\"Total Validation Steps: \", valid_steps)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Steps:  1\n",
            "Total Validation Steps:  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZCcTPA0gIeS"
      },
      "source": [
        "#### Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnCSjtGwgIeT"
      },
      "source": [
        "def encode(example,\n",
        "           encoder_max_len=encoder_max_len, decoder_max_len=decoder_max_len):\n",
        "  \n",
        "    context = example['passage']\n",
        "    question = example['question']\n",
        "    \n",
        "    answer = example['answers_spans']['spans']\n",
        "#     answer_type = example['answers_spans']['types']\n",
        "    \n",
        "    question_plus = f\"answer_me: {str(question)}\"\n",
        "    question_plus += f\" context: {str(context)}\"\n",
        "    \n",
        "    answer_plus = ', '.join([i for i in list(answer)])\n",
        "    answer_plus = f\"{answer_plus}\"\n",
        "    \n",
        "    encoder_inputs = tokenizer(question_plus, truncation=True, \n",
        "                               return_tensors='tf', max_length=encoder_max_len,\n",
        "                              padding=True)\n",
        "    \n",
        "    decoder_inputs = tokenizer(answer_plus, truncation=True, \n",
        "                               return_tensors='tf', max_length=decoder_max_len,\n",
        "                              padding=True)\n",
        "    \n",
        "    input_ids = encoder_inputs['input_ids'][0]\n",
        "    input_attention = encoder_inputs['attention_mask'][0]\n",
        "    target_ids = decoder_inputs['input_ids'][0]\n",
        "    target_attention = decoder_inputs['attention_mask'][0]\n",
        "    \n",
        "    outputs = {'input_ids':input_ids, 'attention_mask': input_attention, \n",
        "               'labels':target_ids, 'decoder_attention_mask':target_attention}\n",
        "    return outputs\n",
        "    \n",
        "    "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xiO1_gwgIeT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115,
          "referenced_widgets": [
            "a983faae6abd456fb82c753729b5039a",
            "82cb020caa764e059e3741cbb2877906",
            "c562383304874cf4adb5cb7ca8571e2d",
            "a7e723c7159d434f9f374cc0c1777e1f",
            "f0403e48448e46159e97f09d87b2e713",
            "bac15fab3d5f41f8ad466f143d2c2617",
            "750721cbc15347aa99c01fcb019c47c6",
            "1303280f42534bc8823f3c9762941dcf",
            "c632f46a7e624025a69069269ec681b7",
            "df85c7a4e63f4b8ebf4d6918995e33f8",
            "5ea0152d98814971863b748454d1182f",
            "67852bf520714ddbb818b86ed241a367",
            "564cb566fe59403c90b11046e01dc07e",
            "263ce715956f4a2a817933ea2abe3437",
            "fb4273e0463549988bff4f6d46b16714",
            "6ac7308c6f3a40c0bde1117f92b07ae9"
          ]
        },
        "outputId": "2084b165-dc34-4b8b-9375-078e6268ef36"
      },
      "source": [
        "train_ds = train_dataset.map(encode)\n",
        "valid_ds = valid_dataset.map(encode)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a983faae6abd456fb82c753729b5039a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c632f46a7e624025a69069269ec681b7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pm4iB3-mgIeT"
      },
      "source": [
        "def to_tf_dataset(dataset):  \n",
        "    columns = ['input_ids', 'attention_mask', 'labels', 'decoder_attention_mask']\n",
        "    dataset.set_format(type='tensorflow', columns=columns)\n",
        "    return_types = {'input_ids':tf.int32, 'attention_mask':tf.int32, \n",
        "                'labels':tf.int32, 'decoder_attention_mask':tf.int32,  }\n",
        "    return_shapes = {'input_ids': tf.TensorShape([None]), 'attention_mask': tf.TensorShape([None]), \n",
        "                  'labels': tf.TensorShape([None]), 'decoder_attention_mask':tf.TensorShape([None])}\n",
        "    ds = tf.data.Dataset.from_generator(lambda : dataset, return_types, return_shapes)\n",
        "    return ds"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc74EwMGgIeU"
      },
      "source": [
        "tf_train_ds = to_tf_dataset(train_ds)\n",
        "tf_valid_ds = to_tf_dataset(valid_ds)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pz-T3qSMgIeU"
      },
      "source": [
        "def create_dataset(dataset, cache_path=None, batch_size=4, \n",
        "                   buffer_size= 1000, shuffling=True):    \n",
        "    if cache_path is not None:\n",
        "        dataset = dataset.cache(cache_path)        \n",
        "    if shuffling:\n",
        "        dataset = dataset.shuffle(buffer_size)\n",
        "    dataset = dataset.padded_batch(batch_size)\n",
        "#     dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return dataset"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMXhVHJLgIeU"
      },
      "source": [
        "tf_train_ds= create_dataset(tf_train_ds, batch_size=batch_size, \n",
        "                         shuffling=True, cache_path = None)\n",
        "tf_valid_ds = create_dataset(tf_valid_ds, batch_size=batch_size, \n",
        "                         shuffling=False, cache_path = None)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHAi1i2BgIeV"
      },
      "source": [
        "#### Callbacks and checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsCNpoMEgIeV"
      },
      "source": [
        "start_profile_batch = steps+10\n",
        "stop_profile_batch = start_profile_batch + 100\n",
        "profile_range = f\"{start_profile_batch},{stop_profile_batch}\"\n",
        "\n",
        "log_path = log_dir + \"/\" + datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_path, histogram_freq=1,\n",
        "                                                     update_freq=20,profile_batch=profile_range)\n",
        "\n",
        "checkpoint_filepath = save_path + \"/\" + \"T5-{epoch:04d}-{val_loss:.4f}.ckpt\"\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=False,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True)\n",
        "\n",
        "callbacks = [tensorboard_callback, model_checkpoint_callback] \n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc5HnwUwgIeV"
      },
      "source": [
        "#### Compile and run model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjHtw6LogIeV"
      },
      "source": [
        "# learning_rate = CustomSchedule()\n",
        "# learning_rate = 0.001  # Instead set a static learning rate\n",
        "optimizer = tf.keras.optimizers.Adam()#learning_rate)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jUQP4a4gIeV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42c8b350-54c1-4ba4-f04b-ae554a751fe8"
      },
      "source": [
        "model.compile(optimizer=optimizer)\n",
        "model.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"t5for_drop\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "shared (TFSharedEmbeddings)  multiple                  16449536  \n",
            "_________________________________________________________________\n",
            "encoder (TFT5MainLayer)      multiple                  18881280  \n",
            "_________________________________________________________________\n",
            "decoder (TFT5MainLayer)      multiple                  25175808  \n",
            "=================================================================\n",
            "Total params: 60,506,630\n",
            "Trainable params: 60,506,624\n",
            "Non-trainable params: 6\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-o4fqo83gIeW",
        "scrolled": true
      },
      "source": [
        "# %tensorboard --logdir ./data/experiments/t5/logs"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "-5CQJG5JZ-3C",
        "outputId": "0f21af62-43dc-445f-832e-356a02ef2e79"
      },
      "source": [
        "tokenizer.decode(range(100))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<pad> </s> <unk> X.,s thea: and to of fille int- is de for’i that youd I withn on\\'o are iten be The as yourl ( or have at from an was thiser lamring can! will by? notre) wey und has all die but our their A more un dercuin so they one about myul whichà In/hef le out also des It up \" timeăif'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jM9lkfKHgIeW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5ce598e-05f9-4ab6-f9d6-79cbb1d849b8"
      },
      "source": [
        "model.fit(tf_train_ds, epochs=3, steps_per_epoch=steps, callbacks=callbacks, \n",
        "          validation_data=tf_valid_ds, validation_steps=valid_steps)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fbfe880cf30>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fbfe880cf30>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7fc0040b9dd0> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function wrap at 0x7fc0040b9dd0> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "1/1 [==============================] - ETA: 0s - loss: 10.5015 - F1: 0.3409 - EM: 0.0000e+00 - lr: 0.0010WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "1/1 [==============================] - 70s 70s/step - loss: 10.5015 - F1: 0.3409 - EM: 0.0000e+00 - lr: 0.0010 - val_loss: 3.2598 - val_F1: 0.3127 - val_EM: 0.0000e+00\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as final_layer_norm_layer_call_and_return_conditional_losses, final_layer_norm_layer_call_fn, dropout_24_layer_call_and_return_conditional_losses, dropout_24_layer_call_fn, final_layer_norm_layer_call_and_return_conditional_losses while saving (showing 5 of 1310). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./data/experiments/t5/models/T5-0001-3.2598.ckpt/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./data/experiments/t5/models/T5-0001-3.2598.ckpt/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/3\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3 batches). You may need to use the repeat() function when building your dataset.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3 batches). You may need to use the repeat() function when building your dataset.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r1/1 [==============================] - 3s 3s/step - loss: 10.5015 - F1: 0.3409 - EM: 0.0000e+00 - lr: 0.0010 - val_loss: 4.5770 - val_F1: 0.2812 - val_EM: 0.0000e+00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fbf26dc6fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wPFXiTKgIeW"
      },
      "source": [
        "model.save_pretrained(save_path)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqxSSq06bZct"
      },
      "source": [
        "model.load_weights('./data/experiments/t5/models/tf_model.h5')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijiJJ9LMgIeW"
      },
      "source": [
        "def generate_answer(question,passage,model,tokenizer):\n",
        "\n",
        "    input_text = f\"question: {question} context: {passage}\"\n",
        "\n",
        "    input_ids = tokenizer.encode(input_text,return_tensors=\"tf\")  \n",
        "    outputs = model.generate(input_ids)\n",
        "    tokenizer.decode(outputs[0])\n",
        "\n",
        "    return tokenizer.decode(outputs[0])\n",
        "\n",
        "\n",
        "def predict(df):\n",
        "    df['pred_answer'] = df.apply(lambda row: generate_answer(row['question'],row['passage'],model,tokenizer),axis=1)\n",
        "    df['pred_answer'] = df['pred_answer'].str.replace('<pad> ','')\n",
        "    df['pred_answer'] = df['pred_answer'].str.replace('</s>','')\n",
        "    return df\n",
        "\n",
        "\n",
        "def evaluate(df):\n",
        "    EM = []\n",
        "    F1 = []\n",
        "    for predicted,gold in zip(df['pred_answer'],df['answer']):\n",
        "\n",
        "        metrics = drop_eval.get_metrics(predicted=predicted,gold=gold)\n",
        "\n",
        "        EM.append(metrics[0])\n",
        "        F1.append(metrics[1])\n",
        "\n",
        "    df['EM'] = EM\n",
        "    df['F1'] = F1\n",
        "    \n",
        "    print('Exact Match: {:0.4f}, F1: {:0.4f}'.format(df.EM.mean(),df.F1.mean()))\n",
        "    return df"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWqYrjoBgIeW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "75281e88-dbd3-4cba-da45-ce492c69d375"
      },
      "source": [
        "train_df = train_ds.to_pandas()\n",
        "valid_df = valid_ds.to_pandas()\n",
        "train_df.head()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>section_id</th>\n",
              "      <th>query_id</th>\n",
              "      <th>passage</th>\n",
              "      <th>question</th>\n",
              "      <th>answers_spans</th>\n",
              "      <th>input_ids</th>\n",
              "      <th>attention_mask</th>\n",
              "      <th>labels</th>\n",
              "      <th>decoder_attention_mask</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>nfl_2201</td>\n",
              "      <td>f16c0ee7-f131-4a8b-a6ac-4d275ea68066</td>\n",
              "      <td>To start the season, the Lions traveled south ...</td>\n",
              "      <td>How many points did the buccaneers need to tie...</td>\n",
              "      <td>{'spans': ['3'], 'types': ['number']}</td>\n",
              "      <td>[1525, 834, 526, 10, 571, 186, 979, 410, 8, 80...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[220, 1]</td>\n",
              "      <td>[1, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>nfl_2201</td>\n",
              "      <td>c9582e03-b01b-42ed-83e0-b90a5334aefa</td>\n",
              "      <td>To start the season, the Lions traveled south ...</td>\n",
              "      <td>How many field goals did the Lions score?</td>\n",
              "      <td>{'spans': ['2'], 'types': ['number']}</td>\n",
              "      <td>[1525, 834, 526, 10, 571, 186, 1057, 1766, 410...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[204, 1]</td>\n",
              "      <td>[1, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>nfl_2201</td>\n",
              "      <td>f703d43d-73fa-4fda-8913-d81bd5569700</td>\n",
              "      <td>To start the season, the Lions traveled south ...</td>\n",
              "      <td>How long was the Lion's longest field goal?</td>\n",
              "      <td>{'spans': ['28-yard'], 'types': ['span']}</td>\n",
              "      <td>[1525, 834, 526, 10, 571, 307, 47, 8, 10371, 3...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[2059, 18, 6636, 1]</td>\n",
              "      <td>[1, 1, 1, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>nfl_2201</td>\n",
              "      <td>2fd4f473-af2b-44ce-929a-20c82fa6be2c</td>\n",
              "      <td>To start the season, the Lions traveled south ...</td>\n",
              "      <td>Who caught the touchdown for the fewest yard?</td>\n",
              "      <td>{'spans': ['Mike Williams'], 'types': ['span']}</td>\n",
              "      <td>[1525, 834, 526, 10, 2645, 4682, 8, 19396, 21,...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[4794, 6060, 1]</td>\n",
              "      <td>[1, 1, 1]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  section_id  ... decoder_attention_mask\n",
              "0   nfl_2201  ...                 [1, 1]\n",
              "1   nfl_2201  ...                 [1, 1]\n",
              "2   nfl_2201  ...           [1, 1, 1, 1]\n",
              "3   nfl_2201  ...              [1, 1, 1]\n",
              "\n",
              "[4 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjsHiiSldUFa"
      },
      "source": [
        "#### Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0g8AC4LTgIeW"
      },
      "source": [
        "train_df = predict(train_df)\n",
        "valid_df = predict(valid_df)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKTCJS8NdWwV"
      },
      "source": [
        "#### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NARIuGX8gIeX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "outputId": "51b66633-ac08-4312-caa4-d6147c996589"
      },
      "source": [
        "train_df = evaluate(train_df)\n",
        "valid_df = evaluate(valid_df)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'answer'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-bc8801a1b040>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvalid_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-cb73640061e1>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mEM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mF1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pred_answer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'answer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrop_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'answer'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G06JDNLngIeX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TtAfZaB94JJ"
   },
   "source": [
    "# Fine-tuning DistilBert on SQUAD v2\n",
    "\n",
    "using example from here: https://huggingface.co/transformers/custom_datasets.html#qa-squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2686,
     "status": "ok",
     "timestamp": 1625110036357,
     "user": {
      "displayName": "Omar Kapur",
      "photoUrl": "",
      "userId": "17558778258581519354"
     },
     "user_tz": 240
    },
    "id": "BuNFhxJK85iq",
    "outputId": "819d89a0-978e-4755-b0de-8361374b6d78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.8.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19y_MDi6-LC9"
   },
   "source": [
    "Start by downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 903,
     "status": "ok",
     "timestamp": 1625110037254,
     "user": {
      "displayName": "Omar Kapur",
      "photoUrl": "",
      "userId": "17558778258581519354"
     },
     "user_tz": 240
    },
    "id": "0Hjqtpwl9Vhj",
    "outputId": "53001a56-9654-4af2-a699-6a81d1cd7bf7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file squad already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved under squad/train-v2.0.json\n",
      "\n",
      "Saved under squad/dev-v2.0.json\n"
     ]
    }
   ],
   "source": [
    "!mkdir squad\n",
    "!python -m wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json -o squad/train-v2.0.json\n",
    "!python -m wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json -o squad/dev-v2.0.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tU8SeidL9cPB"
   },
   "source": [
    "Each split is in a structured json file with anumber of questions and answers for each passage (context). Let's take this apart into parallel lists of contexts, questions, and answers (contexts are repeated because there are multiple questions per context)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1137,
     "status": "ok",
     "timestamp": 1625110038388,
     "user": {
      "displayName": "Omar Kapur",
      "photoUrl": "",
      "userId": "17558778258581519354"
     },
     "user_tz": 240
    },
    "id": "ncDwP8NE9VfP"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def read_squad(path):\n",
    "    path = Path(path)\n",
    "    with open(path, 'rb') as f:\n",
    "        squad_dict = json.load(f)\n",
    "\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for group in squad_dict['data']:\n",
    "        for passage in group['paragraphs']:\n",
    "            context = passage['context']\n",
    "            for qa in passage['qas']:\n",
    "                question = qa['question']\n",
    "                for answer in qa['answers']:\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "\n",
    "    return contexts, questions, answers\n",
    "\n",
    "train_contexts, train_questions, train_answers = read_squad('squad/train-v2.0.json')\n",
    "val_contexts, val_questions, val_answers = read_squad('squad/dev-v2.0.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1625110038395,
     "user": {
      "displayName": "Omar Kapur",
      "photoUrl": "",
      "userId": "17558778258581519354"
     },
     "user_tz": 240
    },
    "id": "VPdvxxcAAz_o",
    "outputId": "8da03c61-9a9e-4ad2-d097-3eba6be44c50"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_contexts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1625110038396,
     "user": {
      "displayName": "Omar Kapur",
      "photoUrl": "",
      "userId": "17558778258581519354"
     },
     "user_tz": 240
    },
    "id": "T4_PFYiVBm1e",
    "outputId": "0c0eaa51-2ad6-486c-ac47-dd2007b7f9c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When did Beyonce start becoming popular?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1625110038396,
     "user": {
      "displayName": "Omar Kapur",
      "photoUrl": "",
      "userId": "17558778258581519354"
     },
     "user_tz": 240
    },
    "id": "DI-hQey5BmtV",
    "outputId": "aa9b3593-81d5-4a91-a545-ffa2bc0fca9c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'in the late 1990s', 'answer_start': 269}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_answers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8WVDEnjBLn5"
   },
   "source": [
    "Shorten the training dataset (!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1625110038397,
     "user": {
      "displayName": "Omar Kapur",
      "photoUrl": "",
      "userId": "17558778258581519354"
     },
     "user_tz": 240
    },
    "id": "ZBuDMKqDAz9Q",
    "outputId": "3ad2cd24-99f7-455b-b9c6-fa1f19f5e561"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num train_contexts: 10000\n",
      "num train_questions: 10000\n",
      "num train_answers: 10000\n",
      "\n",
      "num val_contexts: 2000\n",
      "num val_questions: 2000\n",
      "num val_answers: 2000\n"
     ]
    }
   ],
   "source": [
    "num_train = 10000\n",
    "num_val = 2000\n",
    "\n",
    "train_contexts = train_contexts[:num_train]\n",
    "train_questions = train_questions[:num_train]\n",
    "train_answers = train_answers[:num_train]\n",
    "\n",
    "val_contexts = val_contexts[:num_val]\n",
    "val_questions = val_questions[:num_val]\n",
    "val_answers = val_answers[:num_val]\n",
    "\n",
    "print(f'num train_contexts: {len(train_contexts)}')\n",
    "print(f'num train_questions: {len(train_questions)}')\n",
    "print(f'num train_answers: {len(train_answers)}')\n",
    "\n",
    "print(f'\\nnum val_contexts: {len(val_contexts)}')\n",
    "print(f'num val_questions: {len(val_questions)}')\n",
    "print(f'num val_answers: {len(val_answers)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QxTaST-B-c7i"
   },
   "source": [
    "The contexts and questions are just strings. The answers are dicts containing the subsequence of hte passage with the correct answer as well as an integer indicating hte character at which the answer begins. In order to train a model on this data we need (1) the tokenized context/question pairs, and (2) integers indicating at which __token__ positions the answer begins and ends\n",
    "\n",
    "First, let's get the _character_ position at which the answer ends in the passage (we are given the starting position). Sometimes SQuAD answers are off by one or two characters, so we will adjust for that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1625110038397,
     "user": {
      "displayName": "Omar Kapur",
      "photoUrl": "",
      "userId": "17558778258581519354"
     },
     "user_tz": 240
    },
    "id": "K8XAHbo99Vco"
   },
   "outputs": [],
   "source": [
    "def add_end_idx(answers, contexts):\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        gold_text = answer['text']\n",
    "        start_idx = answer['answer_start']\n",
    "        end_idx = start_idx + len(gold_text)\n",
    "\n",
    "        # sometimes squad answers are off by a character or two – fix this\n",
    "        if context[start_idx:end_idx] == gold_text:\n",
    "            answer['answer_end'] = end_idx\n",
    "        elif context[start_idx-1:end_idx-1] == gold_text:\n",
    "            answer['answer_start'] = start_idx - 1\n",
    "            answer['answer_end'] = end_idx - 1     # When the gold label is off by one character\n",
    "        elif context[start_idx-2:end_idx-2] == gold_text:\n",
    "            answer['answer_start'] = start_idx - 2\n",
    "            answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters\n",
    "\n",
    "add_end_idx(train_answers, train_contexts)\n",
    "add_end_idx(val_answers, val_contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkIaNOr7--Km"
   },
   "source": [
    "Now `train_answers` and `val_answers` include the character end positions and the corrected start positions. Next, let's tokenize our context/question pairs. HF Tokenizers can accept parallel lists of sequences and encode them together as sequence pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 6518,
     "status": "ok",
     "timestamp": 1625110044905,
     "user": {
      "displayName": "Omar Kapur",
      "photoUrl": "",
      "userId": "17558778258581519354"
     },
     "user_tz": 240
    },
    "id": "3oMHBARS9VZ_"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3E0YNEUc_Kd3"
   },
   "source": [
    "Next we need to convert our character start/end positions to token start/end positions. When using HF Fast Tokenizers, we can use the built in `char_to_token()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1625110044906,
     "user": {
      "displayName": "Omar Kapur",
      "photoUrl": "",
      "userId": "17558778258581519354"
     },
     "user_tz": 240
    },
    "id": "I44BMJbh9VXg"
   },
   "outputs": [],
   "source": [
    "def add_token_positions(encodings, answers):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i in range(len(answers)):\n",
    "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
    "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n",
    "\n",
    "        # if start position is None, the answer passage has been truncated\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "        if end_positions[-1] is None:\n",
    "            end_positions[-1] = tokenizer.model_max_length\n",
    "\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "\n",
    "add_token_positions(train_encodings, train_answers)\n",
    "add_token_positions(val_encodings, val_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lrMpdeft_Z9f"
   },
   "source": [
    "Our data is ready. Let's just put it in a PyTorch/Tensorflow dataset so that we can easily use it for training.\n",
    "\n",
    "Let's do both for good measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJAutesW_h8_"
   },
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 165,
     "status": "ok",
     "timestamp": 1625110056423,
     "user": {
      "displayName": "Omar Kapur",
      "photoUrl": "",
      "userId": "17558778258581519354"
     },
     "user_tz": 240
    },
    "id": "R9_joxiQ9VUw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SquadDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "train_dataset = SquadDataset(train_encodings)\n",
    "val_dataset = SquadDataset(val_encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9dziiif_mTt"
   },
   "source": [
    "Now we can use a DistilBert model with a QA head for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1408,
     "status": "ok",
     "timestamp": 1625110059930,
     "user": {
      "displayName": "Omar Kapur",
      "photoUrl": "",
      "userId": "17558778258581519354"
     },
     "user_tz": 240
    },
    "id": "n4YPoqEv9VST",
    "outputId": "498df517-4224-4409-a9d5-f486e67ed624"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForQuestionAnswering\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYA-b7t0_2d9"
   },
   "source": [
    "The data and model are both ready to go. We can train the model with `Trainer`/`TFTrainer`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "9zknq-iJ9VP9"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal instruction was encountered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-410269766053>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0minput_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'input_ids'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'attention_mask'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mstart_positions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'start_positions'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: an illegal instruction was encountered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch in range(3):\n",
    "    for batch in train_loader:\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ya4UkMMFATLN"
   },
   "source": [
    "## TensorFlow"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "adaB63P19VNp"
   },
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {key: train_encodings[key] for key in ['input_ids', 'attention_mask']},\n",
    "    {key: train_encodings[key] for key in ['start_positions', 'end_positions']}\n",
    "))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {key: val_encodings[key] for key in ['input_ids', 'attention_mask']},\n",
    "    {key: val_encodings[key] for key in ['start_positions', 'end_positions']}\n",
    "))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "yAUhAZ3r9VK5"
   },
   "source": [
    "from transformers import TFDistilBertForQuestionAnswering\n",
    "model = TFDistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "ndTdMq5K9VH_"
   },
   "source": [
    "# Keras will expect a tuple when dealing with labels\n",
    "train_dataset = train_dataset.map(lambda x, y: (x, (y['start_positions'], y['end_positions'])))\n",
    "\n",
    "# Keras will assign a separate loss for each output and add them together. So we'll just use the standard CE loss\n",
    "# instead of using the built-in model.compute_loss, which expects a dict of outputs and averages the two terms.\n",
    "# Note that this means the loss will be 2x of when using TFTrainer since we're adding instead of averaging them.\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.distilbert.return_dict = False # if using 🤗 Transformers >3.02, make sure outputs are tuples\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(optimizer=optimizer, loss=loss) # can also use any keras loss fn\n",
    "model.fit(train_dataset.shuffle(1000).batch(8), epochs=3, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0pmw-Gjl9VFR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "okjCcBhs9VCc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZzmJC2AS9VAM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e-oAjg9M9U9i"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TXCQNN_P9U4l"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TTccZMIP9U12"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOmQT5qgrZ9VvpPih6OXWyE",
   "name": "squad-trial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
